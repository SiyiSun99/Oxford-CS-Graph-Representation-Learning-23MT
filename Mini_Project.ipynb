{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "WtiZordtZIen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pirMyBUYcV2",
        "outputId": "8915909a-b424-415a-e5fc-113470861110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch version installed on this system\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the corresponding PyTorch Geometric module\n",
        "%%capture\n",
        "\"\"\"\n",
        "Assign to TORCH with what you get from the cell above. E.g., export TORCH=1.12.1+cu113\n",
        "\"\"\"\n",
        "%env TORCH=2.1.0+cu121\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "r3bZw1KfYhlW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first import all the things we are gonna need for this task\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.auto import trange\n",
        "\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import torch_geometric.utils as U\n",
        "\n",
        "# torch_geometric\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data.data import Data\n",
        "import torch_geometric.utils as U\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.logging import init_wandb, log\n",
        "\n",
        "# models ready to compare\n",
        "from torch_geometric.nn import GCNConv,GraphConv,GINConv,MLP,global_mean_pool,global_add_pool\n",
        "from torch_geometric.nn.models import GCN\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.loader import DataLoader\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "oi_5mCX_Ys8a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "CDsI0u-khVBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# please **DO NOT** modify  any part of the following code  in this cell\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')"
      ],
      "metadata": {
        "id": "xCTdKEvIobt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c53f85b-6e89-4db5-a9fc-f40a0fbef284"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize one\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize_graph(G, color=data.y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "YRvaDwziCCI0",
        "outputId": "db128379-5240-4dd9-80e2-36eafcd525ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py:437: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "  node_collection = ax.scatter(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Setting parameters"
      ],
      "metadata": {
        "id": "O3U1CX-mhgUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all models' parameters\n",
        "params = {\n",
        "    \"input_features\": 7,\n",
        "    \"hidden_features_gin\": 64,\n",
        "    \"hidden_features_gcn\": 64,\n",
        "    \"hidden_features_kgnn\": 64,\n",
        "    \"num_gin_layers\": 3,\n",
        "    \"num_gcn_layers\": 2,\n",
        "    \"num_mlp_layers\": 2,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0,\n",
        "    \"num_epochs\": 256,\n",
        "    \"num_classes\": 2,\n",
        "    \"batch_size\": 32,\n",
        "}\n",
        "\n",
        "# device to use\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "\n",
        "# data loader\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()"
      ],
      "metadata": {
        "id": "Z4kR87iLg1wT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "MIiiEyoJyf16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN"
      ],
      "metadata": {
        "id": "LYsGBfZmI2U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model1: GCN\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sm_5DCWWzAEJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GIN"
      ],
      "metadata": {
        "id": "nN-wpUI1ND0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
        "            in_channels = hidden_channels\n",
        "\n",
        "        self.mlp = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "N0GraqytNGD2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-GNN"
      ],
      "metadata": {
        "id": "HX25n7ZTI9et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3: k-GNN\n",
        "class kGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(kGNN, self).__init__()\n",
        "\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "H_zSnspgJCKg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Compare Different Models"
      ],
      "metadata": {
        "id": "LY70vmIzNQpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model1: GCN\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     loss_ = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         data = data.to(device)\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         loss = criterion(out, data.y)\n",
        "         loss_ += loss.item()\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset), loss_ / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset[:start] + dataset[end:]\n",
        "    test_dataset = dataset[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model = GCN(params[\"hidden_features_gcn\"]).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr=params[\"learning_rate\"],\n",
        "                                weight_decay=params[\"weight_decay\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, _ = test(train_loader)\n",
        "        test_acc, _ = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2upZwUv3T2Jv",
        "outputId": "e6e82bfc-51d7-4bf9-91dd-14b8047d45f3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.6931, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 002, Loss: 0.6926, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 003, Loss: 0.6882, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 004, Loss: 0.6864, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 005, Loss: 0.6864, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 006, Loss: 0.6854, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 007, Loss: 0.6824, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 008, Loss: 0.6803, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 009, Loss: 0.6776, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 010, Loss: 0.6788, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 011, Loss: 0.6760, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 012, Loss: 0.6753, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.6709, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 014, Loss: 0.6710, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.6689, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 016, Loss: 0.6686, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 017, Loss: 0.6689, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 018, Loss: 0.6651, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 019, Loss: 0.6668, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 020, Loss: 0.6599, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 021, Loss: 0.6594, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 022, Loss: 0.6567, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 023, Loss: 0.6560, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 024, Loss: 0.6565, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 025, Loss: 0.6589, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 026, Loss: 0.6596, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 027, Loss: 0.6567, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 028, Loss: 0.6514, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 029, Loss: 0.6527, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 030, Loss: 0.6515, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 031, Loss: 0.6449, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 032, Loss: 0.6451, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 033, Loss: 0.6509, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 034, Loss: 0.6479, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 035, Loss: 0.6483, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 036, Loss: 0.6471, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 037, Loss: 0.6437, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 038, Loss: 0.6479, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 039, Loss: 0.6448, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 040, Loss: 0.6376, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 041, Loss: 0.6425, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 042, Loss: 0.6403, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 043, Loss: 0.6388, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 044, Loss: 0.6349, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 045, Loss: 0.6363, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 046, Loss: 0.6386, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 047, Loss: 0.6360, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 048, Loss: 0.6397, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 049, Loss: 0.6342, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 050, Loss: 0.6332, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 051, Loss: 0.6384, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 052, Loss: 0.6371, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 053, Loss: 0.6313, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 054, Loss: 0.6327, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 055, Loss: 0.6282, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 056, Loss: 0.6339, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 057, Loss: 0.6374, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 058, Loss: 0.6311, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 059, Loss: 0.6281, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 060, Loss: 0.6238, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 061, Loss: 0.6299, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 062, Loss: 0.6264, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 063, Loss: 0.6311, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 064, Loss: 0.6341, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 065, Loss: 0.6295, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 066, Loss: 0.6287, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 067, Loss: 0.6247, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 068, Loss: 0.6327, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 069, Loss: 0.6295, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 070, Loss: 0.6268, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 071, Loss: 0.6265, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 072, Loss: 0.6285, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 073, Loss: 0.6294, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 074, Loss: 0.6198, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 075, Loss: 0.6326, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 076, Loss: 0.6185, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 077, Loss: 0.6205, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 078, Loss: 0.6244, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 079, Loss: 0.6277, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 080, Loss: 0.6274, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 081, Loss: 0.6281, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 082, Loss: 0.6231, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 083, Loss: 0.6220, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 084, Loss: 0.6200, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 085, Loss: 0.6199, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 086, Loss: 0.6227, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 087, Loss: 0.6258, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 088, Loss: 0.6260, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 089, Loss: 0.6194, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 090, Loss: 0.6162, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 091, Loss: 0.6156, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 092, Loss: 0.6178, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 093, Loss: 0.6271, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 094, Loss: 0.6257, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 095, Loss: 0.6206, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 096, Loss: 0.6154, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 097, Loss: 0.6203, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 098, Loss: 0.6272, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 099, Loss: 0.6143, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 100, Loss: 0.6139, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 101, Loss: 0.6115, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 102, Loss: 0.6173, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 103, Loss: 0.6203, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 104, Loss: 0.6239, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 105, Loss: 0.6162, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 106, Loss: 0.6100, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 107, Loss: 0.6161, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 108, Loss: 0.6152, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 109, Loss: 0.6172, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 110, Loss: 0.6146, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 111, Loss: 0.6129, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 112, Loss: 0.6156, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 113, Loss: 0.6135, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 114, Loss: 0.6153, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 115, Loss: 0.6106, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 116, Loss: 0.6152, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 117, Loss: 0.6116, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 118, Loss: 0.6071, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 119, Loss: 0.6153, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 120, Loss: 0.6132, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 121, Loss: 0.6086, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 122, Loss: 0.6073, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 123, Loss: 0.6089, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 124, Loss: 0.6119, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 125, Loss: 0.5999, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 126, Loss: 0.6144, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 127, Loss: 0.6038, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 128, Loss: 0.5974, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 129, Loss: 0.6053, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 130, Loss: 0.6049, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 131, Loss: 0.6111, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 132, Loss: 0.6096, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 133, Loss: 0.6048, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 134, Loss: 0.5988, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 135, Loss: 0.5977, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 136, Loss: 0.6009, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 137, Loss: 0.6029, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 138, Loss: 0.6005, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 139, Loss: 0.6008, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 140, Loss: 0.6058, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 141, Loss: 0.6041, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 142, Loss: 0.6090, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 143, Loss: 0.5972, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 144, Loss: 0.5914, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 145, Loss: 0.6012, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 146, Loss: 0.5886, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 147, Loss: 0.6044, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 148, Loss: 0.5912, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 149, Loss: 0.5989, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 150, Loss: 0.5960, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 151, Loss: 0.5953, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 152, Loss: 0.5944, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 153, Loss: 0.5870, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 154, Loss: 0.5936, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 155, Loss: 0.5974, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 156, Loss: 0.5949, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 157, Loss: 0.5949, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 158, Loss: 0.5861, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 159, Loss: 0.5952, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 160, Loss: 0.6000, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 161, Loss: 0.5860, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 162, Loss: 0.5884, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 163, Loss: 0.5918, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 164, Loss: 0.5859, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 165, Loss: 0.5867, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 166, Loss: 0.5879, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 167, Loss: 0.5851, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 168, Loss: 0.5742, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 169, Loss: 0.5790, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 170, Loss: 0.5928, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 171, Loss: 0.5843, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 172, Loss: 0.5844, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 173, Loss: 0.5770, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 174, Loss: 0.5745, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 175, Loss: 0.5843, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 176, Loss: 0.5888, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 177, Loss: 0.5815, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 178, Loss: 0.5772, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 179, Loss: 0.5767, Train: 0.6824, Test: 0.7222\n",
            "Epoch: 180, Loss: 0.5761, Train: 0.6824, Test: 0.7222\n",
            "Epoch: 181, Loss: 0.5784, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 182, Loss: 0.5777, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 183, Loss: 0.5764, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 184, Loss: 0.5704, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 185, Loss: 0.5751, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 186, Loss: 0.5717, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 187, Loss: 0.5729, Train: 0.6824, Test: 0.7222\n",
            "Epoch: 188, Loss: 0.5775, Train: 0.6824, Test: 0.7222\n",
            "Epoch: 189, Loss: 0.5719, Train: 0.6882, Test: 0.7222\n",
            "Epoch: 190, Loss: 0.5635, Train: 0.6882, Test: 0.7222\n",
            "Epoch: 191, Loss: 0.5656, Train: 0.6882, Test: 0.7222\n",
            "Epoch: 192, Loss: 0.5689, Train: 0.6882, Test: 0.7222\n",
            "Epoch: 193, Loss: 0.5684, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 194, Loss: 0.5596, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 195, Loss: 0.5715, Train: 0.7059, Test: 0.7222\n",
            "Epoch: 196, Loss: 0.5672, Train: 0.7059, Test: 0.7222\n",
            "Epoch: 197, Loss: 0.5586, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 198, Loss: 0.5678, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 199, Loss: 0.5642, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 200, Loss: 0.5582, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 201, Loss: 0.5692, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 202, Loss: 0.5603, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 203, Loss: 0.5690, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 204, Loss: 0.5582, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 205, Loss: 0.5666, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.5713, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.5570, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 208, Loss: 0.5563, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 209, Loss: 0.5572, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 210, Loss: 0.5541, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 211, Loss: 0.5592, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 212, Loss: 0.5635, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 213, Loss: 0.5622, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 214, Loss: 0.5508, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 215, Loss: 0.5530, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.5539, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 217, Loss: 0.5607, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 218, Loss: 0.5487, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 219, Loss: 0.5478, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 220, Loss: 0.5494, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 221, Loss: 0.5552, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 222, Loss: 0.5474, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 223, Loss: 0.5490, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 224, Loss: 0.5527, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 225, Loss: 0.5501, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.5555, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 227, Loss: 0.5442, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 228, Loss: 0.5431, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 229, Loss: 0.5460, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 230, Loss: 0.5526, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 231, Loss: 0.5485, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 232, Loss: 0.5426, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 233, Loss: 0.5484, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 234, Loss: 0.5456, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 235, Loss: 0.5425, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 236, Loss: 0.5455, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 237, Loss: 0.5456, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 238, Loss: 0.5447, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 239, Loss: 0.5429, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 240, Loss: 0.5430, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 241, Loss: 0.5543, Train: 0.7118, Test: 0.8889\n",
            "Epoch: 242, Loss: 0.5334, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 243, Loss: 0.5434, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 244, Loss: 0.5450, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 245, Loss: 0.5457, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 246, Loss: 0.5403, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 247, Loss: 0.5385, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 248, Loss: 0.5357, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 249, Loss: 0.5483, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 250, Loss: 0.5342, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 251, Loss: 0.5415, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 252, Loss: 0.5431, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 253, Loss: 0.5385, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 254, Loss: 0.5393, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 255, Loss: 0.5346, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 256, Loss: 0.5511, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 001, Loss: 0.6935, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 002, Loss: 0.6929, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 003, Loss: 0.6879, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 004, Loss: 0.6875, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 005, Loss: 0.6872, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 006, Loss: 0.6855, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 007, Loss: 0.6832, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 008, Loss: 0.6818, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 009, Loss: 0.6781, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 010, Loss: 0.6789, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 011, Loss: 0.6763, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 012, Loss: 0.6750, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 013, Loss: 0.6719, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 014, Loss: 0.6719, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 015, Loss: 0.6698, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 016, Loss: 0.6703, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 017, Loss: 0.6695, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 018, Loss: 0.6673, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 019, Loss: 0.6680, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 020, Loss: 0.6610, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 021, Loss: 0.6624, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 022, Loss: 0.6582, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 023, Loss: 0.6594, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 024, Loss: 0.6585, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 025, Loss: 0.6623, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 026, Loss: 0.6627, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 027, Loss: 0.6564, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 028, Loss: 0.6554, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 029, Loss: 0.6575, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 030, Loss: 0.6536, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 031, Loss: 0.6490, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 032, Loss: 0.6481, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 033, Loss: 0.6536, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 034, Loss: 0.6488, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 035, Loss: 0.6508, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 036, Loss: 0.6485, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 037, Loss: 0.6450, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 038, Loss: 0.6506, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 039, Loss: 0.6482, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 040, Loss: 0.6411, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 041, Loss: 0.6480, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 042, Loss: 0.6454, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 043, Loss: 0.6463, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 044, Loss: 0.6404, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 045, Loss: 0.6435, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 046, Loss: 0.6401, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 047, Loss: 0.6410, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 048, Loss: 0.6469, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 049, Loss: 0.6391, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.6395, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.6414, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.6407, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.6374, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.6364, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.6335, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.6396, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.6429, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.6355, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.6341, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.6301, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.6340, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.6299, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.6319, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.6376, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.6338, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.6345, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.6280, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 068, Loss: 0.6348, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 069, Loss: 0.6346, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 070, Loss: 0.6322, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 071, Loss: 0.6332, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.6333, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.6343, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.6258, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 075, Loss: 0.6353, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 076, Loss: 0.6218, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 077, Loss: 0.6261, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 078, Loss: 0.6282, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 079, Loss: 0.6314, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 080, Loss: 0.6352, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 081, Loss: 0.6332, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 082, Loss: 0.6291, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 083, Loss: 0.6273, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 084, Loss: 0.6236, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 085, Loss: 0.6274, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 086, Loss: 0.6280, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 087, Loss: 0.6280, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 088, Loss: 0.6317, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 089, Loss: 0.6260, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 090, Loss: 0.6238, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 091, Loss: 0.6203, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 092, Loss: 0.6224, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 093, Loss: 0.6336, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 094, Loss: 0.6331, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 095, Loss: 0.6238, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 096, Loss: 0.6190, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 097, Loss: 0.6261, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 098, Loss: 0.6293, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 099, Loss: 0.6195, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 100, Loss: 0.6179, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 101, Loss: 0.6168, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 102, Loss: 0.6227, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 103, Loss: 0.6285, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 104, Loss: 0.6285, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 105, Loss: 0.6205, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 106, Loss: 0.6168, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 107, Loss: 0.6228, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 108, Loss: 0.6228, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 109, Loss: 0.6231, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 110, Loss: 0.6212, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 111, Loss: 0.6223, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 112, Loss: 0.6211, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 113, Loss: 0.6202, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.6181, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 115, Loss: 0.6179, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 116, Loss: 0.6225, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 117, Loss: 0.6162, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 118, Loss: 0.6105, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 119, Loss: 0.6182, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 120, Loss: 0.6194, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 121, Loss: 0.6206, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 122, Loss: 0.6149, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 123, Loss: 0.6147, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 124, Loss: 0.6183, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 125, Loss: 0.6097, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 126, Loss: 0.6173, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 127, Loss: 0.6136, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 128, Loss: 0.6095, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 129, Loss: 0.6126, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 130, Loss: 0.6128, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 131, Loss: 0.6175, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 132, Loss: 0.6162, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 133, Loss: 0.6135, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 134, Loss: 0.6063, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 135, Loss: 0.6065, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 136, Loss: 0.6117, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 137, Loss: 0.6072, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 138, Loss: 0.6100, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 139, Loss: 0.6084, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 140, Loss: 0.6148, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 141, Loss: 0.6144, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 142, Loss: 0.6146, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 143, Loss: 0.6073, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 144, Loss: 0.6027, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 145, Loss: 0.6103, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 146, Loss: 0.5966, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 147, Loss: 0.6142, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 148, Loss: 0.5993, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 149, Loss: 0.6075, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 150, Loss: 0.6056, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 151, Loss: 0.6037, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 152, Loss: 0.6049, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 153, Loss: 0.5987, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 154, Loss: 0.6016, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 155, Loss: 0.6045, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 156, Loss: 0.6006, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 157, Loss: 0.6035, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 158, Loss: 0.5990, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 159, Loss: 0.6024, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 160, Loss: 0.6075, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 161, Loss: 0.5979, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 162, Loss: 0.6004, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 163, Loss: 0.6023, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 164, Loss: 0.5998, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 165, Loss: 0.5962, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 166, Loss: 0.5970, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 167, Loss: 0.5981, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 168, Loss: 0.5827, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 169, Loss: 0.5885, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 170, Loss: 0.6044, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 171, Loss: 0.5934, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 172, Loss: 0.5930, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 173, Loss: 0.5894, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 174, Loss: 0.5863, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 175, Loss: 0.5940, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 176, Loss: 0.6016, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 177, Loss: 0.5940, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 178, Loss: 0.5886, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 179, Loss: 0.5874, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 180, Loss: 0.5861, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 181, Loss: 0.5918, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 182, Loss: 0.5879, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 183, Loss: 0.5879, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 184, Loss: 0.5791, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 185, Loss: 0.5857, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 186, Loss: 0.5813, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 187, Loss: 0.5804, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 188, Loss: 0.5861, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 189, Loss: 0.5865, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 190, Loss: 0.5746, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 191, Loss: 0.5766, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 192, Loss: 0.5813, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 193, Loss: 0.5795, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 194, Loss: 0.5680, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 195, Loss: 0.5797, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 196, Loss: 0.5799, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 197, Loss: 0.5728, Train: 0.6765, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.5800, Train: 0.6765, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.5717, Train: 0.6824, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.5739, Train: 0.6824, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.5830, Train: 0.6882, Test: 0.8333\n",
            "Epoch: 202, Loss: 0.5745, Train: 0.6882, Test: 0.8333\n",
            "Epoch: 203, Loss: 0.5812, Train: 0.6941, Test: 0.8333\n",
            "Epoch: 204, Loss: 0.5720, Train: 0.6941, Test: 0.8333\n",
            "Epoch: 205, Loss: 0.5773, Train: 0.6941, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.5805, Train: 0.6941, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.5711, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.5668, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.5695, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.5656, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.5737, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.5742, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.5754, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.5647, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.5706, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 216, Loss: 0.5645, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.5697, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.5621, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.5602, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.5639, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.5676, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.5577, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.5591, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.5637, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.5619, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.5679, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.5589, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.5556, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.5553, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.5631, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.5607, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.5564, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.5575, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.5587, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 235, Loss: 0.5584, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.5551, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.5601, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.5558, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.5557, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.5553, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.5656, Train: 0.7118, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.5452, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.5517, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.5569, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.5594, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.5531, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.5489, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.5502, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.5563, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.5493, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.5515, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.5509, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.5514, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.5484, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.5484, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.5599, Train: 0.7176, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.6932, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 002, Loss: 0.6907, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 003, Loss: 0.6871, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 004, Loss: 0.6878, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 005, Loss: 0.6868, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 006, Loss: 0.6843, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 007, Loss: 0.6820, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 008, Loss: 0.6799, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 009, Loss: 0.6771, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 010, Loss: 0.6753, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 011, Loss: 0.6747, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 012, Loss: 0.6732, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 013, Loss: 0.6715, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 014, Loss: 0.6696, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 015, Loss: 0.6679, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 016, Loss: 0.6674, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 017, Loss: 0.6665, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 018, Loss: 0.6626, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 019, Loss: 0.6653, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 020, Loss: 0.6590, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 021, Loss: 0.6580, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 022, Loss: 0.6548, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 023, Loss: 0.6542, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 024, Loss: 0.6543, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 025, Loss: 0.6570, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 026, Loss: 0.6553, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 027, Loss: 0.6507, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 028, Loss: 0.6491, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 029, Loss: 0.6491, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 030, Loss: 0.6455, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 031, Loss: 0.6451, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 032, Loss: 0.6401, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 033, Loss: 0.6455, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 034, Loss: 0.6410, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 035, Loss: 0.6417, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 036, Loss: 0.6407, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 037, Loss: 0.6363, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 038, Loss: 0.6354, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 039, Loss: 0.6379, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 040, Loss: 0.6285, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 041, Loss: 0.6389, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 042, Loss: 0.6357, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 043, Loss: 0.6365, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 044, Loss: 0.6278, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 045, Loss: 0.6313, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 046, Loss: 0.6300, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 047, Loss: 0.6300, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 048, Loss: 0.6316, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 049, Loss: 0.6264, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 050, Loss: 0.6280, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 051, Loss: 0.6288, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 052, Loss: 0.6274, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 053, Loss: 0.6209, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 054, Loss: 0.6254, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 055, Loss: 0.6231, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 056, Loss: 0.6295, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 057, Loss: 0.6248, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 058, Loss: 0.6208, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 059, Loss: 0.6225, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 060, Loss: 0.6192, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 061, Loss: 0.6192, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 062, Loss: 0.6151, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 063, Loss: 0.6174, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 064, Loss: 0.6267, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 065, Loss: 0.6178, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 066, Loss: 0.6210, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 067, Loss: 0.6132, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 068, Loss: 0.6213, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 069, Loss: 0.6228, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 070, Loss: 0.6191, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 071, Loss: 0.6206, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 072, Loss: 0.6204, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 073, Loss: 0.6256, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 074, Loss: 0.6096, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 075, Loss: 0.6193, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 076, Loss: 0.6074, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 077, Loss: 0.6119, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 078, Loss: 0.6172, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 079, Loss: 0.6120, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 080, Loss: 0.6196, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 081, Loss: 0.6238, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 082, Loss: 0.6167, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 083, Loss: 0.6156, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 084, Loss: 0.6107, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 085, Loss: 0.6119, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 086, Loss: 0.6138, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 087, Loss: 0.6159, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 088, Loss: 0.6122, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 089, Loss: 0.6079, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 090, Loss: 0.6111, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 091, Loss: 0.6054, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 092, Loss: 0.6039, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 093, Loss: 0.6184, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 094, Loss: 0.6121, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 095, Loss: 0.6081, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 096, Loss: 0.6034, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 097, Loss: 0.6106, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 098, Loss: 0.6122, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 099, Loss: 0.6058, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 100, Loss: 0.6021, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 101, Loss: 0.6013, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 102, Loss: 0.6067, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 103, Loss: 0.6141, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 104, Loss: 0.6121, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 105, Loss: 0.6063, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 106, Loss: 0.6002, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 107, Loss: 0.6085, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 108, Loss: 0.6079, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 109, Loss: 0.6076, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 110, Loss: 0.6043, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 111, Loss: 0.6046, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 112, Loss: 0.6042, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 113, Loss: 0.6047, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 114, Loss: 0.5961, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 115, Loss: 0.5963, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 116, Loss: 0.6031, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 117, Loss: 0.6042, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 118, Loss: 0.5950, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 119, Loss: 0.6010, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 120, Loss: 0.6027, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 121, Loss: 0.6052, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 122, Loss: 0.5955, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 123, Loss: 0.6000, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 124, Loss: 0.5987, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 125, Loss: 0.5911, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 126, Loss: 0.5980, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 127, Loss: 0.5970, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 128, Loss: 0.5932, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 129, Loss: 0.5925, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 130, Loss: 0.5963, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 131, Loss: 0.5973, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 132, Loss: 0.5970, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 133, Loss: 0.5988, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 134, Loss: 0.5862, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 135, Loss: 0.5888, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 136, Loss: 0.5952, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 137, Loss: 0.5888, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 138, Loss: 0.5892, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 139, Loss: 0.5886, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 140, Loss: 0.5963, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 141, Loss: 0.5956, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 142, Loss: 0.5917, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 143, Loss: 0.5905, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 144, Loss: 0.5838, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 145, Loss: 0.5894, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 146, Loss: 0.5794, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 147, Loss: 0.5942, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 148, Loss: 0.5792, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 149, Loss: 0.5833, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 150, Loss: 0.5864, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 151, Loss: 0.5846, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 152, Loss: 0.5856, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 153, Loss: 0.5769, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 154, Loss: 0.5771, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 155, Loss: 0.5834, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 156, Loss: 0.5800, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 157, Loss: 0.5853, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 158, Loss: 0.5786, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 159, Loss: 0.5835, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 160, Loss: 0.5879, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 161, Loss: 0.5762, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 162, Loss: 0.5787, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 163, Loss: 0.5800, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 164, Loss: 0.5778, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 165, Loss: 0.5704, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 166, Loss: 0.5735, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 167, Loss: 0.5728, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 168, Loss: 0.5608, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 169, Loss: 0.5683, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 170, Loss: 0.5801, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 171, Loss: 0.5697, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 172, Loss: 0.5670, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 173, Loss: 0.5665, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 174, Loss: 0.5597, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 175, Loss: 0.5680, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 176, Loss: 0.5781, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 177, Loss: 0.5680, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 178, Loss: 0.5649, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 179, Loss: 0.5631, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 180, Loss: 0.5649, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 181, Loss: 0.5637, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 182, Loss: 0.5615, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 183, Loss: 0.5618, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 184, Loss: 0.5539, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 185, Loss: 0.5592, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 186, Loss: 0.5549, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 187, Loss: 0.5536, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 188, Loss: 0.5612, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 189, Loss: 0.5601, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 190, Loss: 0.5502, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 191, Loss: 0.5502, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 192, Loss: 0.5534, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 193, Loss: 0.5526, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 194, Loss: 0.5412, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 195, Loss: 0.5544, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 196, Loss: 0.5527, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 197, Loss: 0.5458, Train: 0.7176, Test: 0.6111\n",
            "Epoch: 198, Loss: 0.5535, Train: 0.7176, Test: 0.6111\n",
            "Epoch: 199, Loss: 0.5466, Train: 0.7176, Test: 0.6111\n",
            "Epoch: 200, Loss: 0.5459, Train: 0.7235, Test: 0.6111\n",
            "Epoch: 201, Loss: 0.5540, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 202, Loss: 0.5466, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 203, Loss: 0.5548, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 204, Loss: 0.5411, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 205, Loss: 0.5506, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 206, Loss: 0.5545, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 207, Loss: 0.5449, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 208, Loss: 0.5392, Train: 0.7529, Test: 0.6667\n",
            "Epoch: 209, Loss: 0.5438, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 210, Loss: 0.5351, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 211, Loss: 0.5462, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 212, Loss: 0.5448, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 213, Loss: 0.5464, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 214, Loss: 0.5353, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 215, Loss: 0.5422, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 216, Loss: 0.5373, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 217, Loss: 0.5373, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 218, Loss: 0.5341, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 219, Loss: 0.5328, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 220, Loss: 0.5345, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 221, Loss: 0.5406, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 222, Loss: 0.5298, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 223, Loss: 0.5311, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 224, Loss: 0.5359, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 225, Loss: 0.5320, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 226, Loss: 0.5386, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 227, Loss: 0.5348, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 228, Loss: 0.5308, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 229, Loss: 0.5256, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 230, Loss: 0.5352, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 231, Loss: 0.5286, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 232, Loss: 0.5297, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 233, Loss: 0.5314, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 234, Loss: 0.5330, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 235, Loss: 0.5302, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 236, Loss: 0.5274, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 237, Loss: 0.5319, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 238, Loss: 0.5307, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 239, Loss: 0.5291, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 240, Loss: 0.5281, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 241, Loss: 0.5338, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 242, Loss: 0.5153, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 243, Loss: 0.5241, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 244, Loss: 0.5252, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 245, Loss: 0.5321, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 246, Loss: 0.5263, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 247, Loss: 0.5205, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 248, Loss: 0.5230, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 249, Loss: 0.5267, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 250, Loss: 0.5242, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 251, Loss: 0.5208, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 252, Loss: 0.5204, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 253, Loss: 0.5210, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 254, Loss: 0.5201, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 255, Loss: 0.5192, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 256, Loss: 0.5307, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 001, Loss: 0.6931, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 002, Loss: 0.6906, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 003, Loss: 0.6875, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 004, Loss: 0.6871, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 005, Loss: 0.6855, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 006, Loss: 0.6849, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 007, Loss: 0.6820, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 008, Loss: 0.6787, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 009, Loss: 0.6756, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 010, Loss: 0.6748, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 011, Loss: 0.6736, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 012, Loss: 0.6715, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 013, Loss: 0.6702, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 014, Loss: 0.6682, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 015, Loss: 0.6659, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 016, Loss: 0.6673, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 017, Loss: 0.6640, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 018, Loss: 0.6611, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 019, Loss: 0.6645, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 020, Loss: 0.6583, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 021, Loss: 0.6577, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 022, Loss: 0.6529, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 023, Loss: 0.6523, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 024, Loss: 0.6548, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 025, Loss: 0.6560, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 026, Loss: 0.6539, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 027, Loss: 0.6486, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 028, Loss: 0.6473, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 029, Loss: 0.6465, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 030, Loss: 0.6418, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 031, Loss: 0.6425, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 032, Loss: 0.6389, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 033, Loss: 0.6428, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 034, Loss: 0.6360, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 035, Loss: 0.6393, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 036, Loss: 0.6395, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 037, Loss: 0.6321, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 038, Loss: 0.6334, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 039, Loss: 0.6329, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 040, Loss: 0.6272, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 041, Loss: 0.6353, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 042, Loss: 0.6362, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 043, Loss: 0.6343, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 044, Loss: 0.6295, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 045, Loss: 0.6279, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 046, Loss: 0.6296, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 047, Loss: 0.6288, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 048, Loss: 0.6293, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 049, Loss: 0.6226, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 050, Loss: 0.6265, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 051, Loss: 0.6235, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 052, Loss: 0.6205, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 053, Loss: 0.6168, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 054, Loss: 0.6218, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 055, Loss: 0.6199, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 056, Loss: 0.6226, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 057, Loss: 0.6186, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 058, Loss: 0.6159, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 059, Loss: 0.6177, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 060, Loss: 0.6175, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 061, Loss: 0.6167, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 062, Loss: 0.6122, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 063, Loss: 0.6175, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 064, Loss: 0.6289, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 065, Loss: 0.6150, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 066, Loss: 0.6182, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 067, Loss: 0.6129, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 068, Loss: 0.6169, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 069, Loss: 0.6247, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 070, Loss: 0.6185, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 071, Loss: 0.6215, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 072, Loss: 0.6136, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 073, Loss: 0.6231, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 074, Loss: 0.6082, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 075, Loss: 0.6172, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 076, Loss: 0.6039, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 077, Loss: 0.6065, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 078, Loss: 0.6130, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 079, Loss: 0.6092, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 080, Loss: 0.6141, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 081, Loss: 0.6216, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 082, Loss: 0.6124, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 083, Loss: 0.6130, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 084, Loss: 0.6095, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 085, Loss: 0.6063, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 086, Loss: 0.6103, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 087, Loss: 0.6108, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 088, Loss: 0.6133, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 089, Loss: 0.6085, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 090, Loss: 0.6081, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 091, Loss: 0.6007, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 092, Loss: 0.6043, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 093, Loss: 0.6185, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 094, Loss: 0.6115, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 095, Loss: 0.6061, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 096, Loss: 0.6049, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 097, Loss: 0.6108, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 098, Loss: 0.6114, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 099, Loss: 0.6041, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 100, Loss: 0.5990, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 101, Loss: 0.6035, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 102, Loss: 0.6031, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 103, Loss: 0.6082, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 104, Loss: 0.6105, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 105, Loss: 0.6009, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 106, Loss: 0.6014, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 107, Loss: 0.6064, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 108, Loss: 0.6073, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 109, Loss: 0.6095, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 110, Loss: 0.6040, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 111, Loss: 0.5985, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 112, Loss: 0.6020, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 113, Loss: 0.6061, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 114, Loss: 0.5919, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 115, Loss: 0.5992, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 116, Loss: 0.6027, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 117, Loss: 0.6026, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 118, Loss: 0.5956, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 119, Loss: 0.5982, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 120, Loss: 0.6015, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 121, Loss: 0.6036, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 122, Loss: 0.5946, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 123, Loss: 0.5970, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 124, Loss: 0.6008, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 125, Loss: 0.5915, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 126, Loss: 0.5995, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 127, Loss: 0.5972, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 128, Loss: 0.5952, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 129, Loss: 0.5936, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 130, Loss: 0.5940, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 131, Loss: 0.5961, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 132, Loss: 0.5958, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 133, Loss: 0.6003, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 134, Loss: 0.5904, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 135, Loss: 0.5900, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 136, Loss: 0.5979, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 137, Loss: 0.5896, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 138, Loss: 0.5909, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 139, Loss: 0.5890, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 140, Loss: 0.5956, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 141, Loss: 0.5914, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 142, Loss: 0.5928, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 143, Loss: 0.5902, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 144, Loss: 0.5867, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 145, Loss: 0.5920, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 146, Loss: 0.5817, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 147, Loss: 0.5959, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 148, Loss: 0.5797, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 149, Loss: 0.5876, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 150, Loss: 0.5846, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 151, Loss: 0.5860, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 152, Loss: 0.5856, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 153, Loss: 0.5800, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 154, Loss: 0.5809, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 155, Loss: 0.5852, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 156, Loss: 0.5829, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 157, Loss: 0.5927, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 158, Loss: 0.5808, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 159, Loss: 0.5854, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 160, Loss: 0.5886, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 161, Loss: 0.5767, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 162, Loss: 0.5789, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 163, Loss: 0.5776, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 164, Loss: 0.5810, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 165, Loss: 0.5729, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 166, Loss: 0.5754, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 167, Loss: 0.5728, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 168, Loss: 0.5663, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 169, Loss: 0.5753, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 170, Loss: 0.5797, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 171, Loss: 0.5731, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 172, Loss: 0.5706, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 173, Loss: 0.5700, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 174, Loss: 0.5631, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 175, Loss: 0.5722, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 176, Loss: 0.5799, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 177, Loss: 0.5711, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 178, Loss: 0.5708, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 179, Loss: 0.5686, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 180, Loss: 0.5656, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 181, Loss: 0.5673, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 182, Loss: 0.5666, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 183, Loss: 0.5621, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 184, Loss: 0.5593, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 185, Loss: 0.5620, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 186, Loss: 0.5618, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 187, Loss: 0.5589, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 188, Loss: 0.5673, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 189, Loss: 0.5645, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 190, Loss: 0.5557, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 191, Loss: 0.5565, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 192, Loss: 0.5541, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 193, Loss: 0.5572, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 194, Loss: 0.5505, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 195, Loss: 0.5561, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 196, Loss: 0.5578, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 197, Loss: 0.5521, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 198, Loss: 0.5591, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 199, Loss: 0.5506, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 200, Loss: 0.5533, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 201, Loss: 0.5629, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 202, Loss: 0.5505, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 203, Loss: 0.5585, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 204, Loss: 0.5488, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 205, Loss: 0.5533, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 206, Loss: 0.5574, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 207, Loss: 0.5497, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 208, Loss: 0.5453, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 209, Loss: 0.5508, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 210, Loss: 0.5411, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 211, Loss: 0.5466, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 212, Loss: 0.5471, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 213, Loss: 0.5491, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 214, Loss: 0.5398, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 215, Loss: 0.5463, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 216, Loss: 0.5395, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 217, Loss: 0.5400, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 218, Loss: 0.5424, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 219, Loss: 0.5329, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 220, Loss: 0.5390, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 221, Loss: 0.5417, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 222, Loss: 0.5337, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 223, Loss: 0.5358, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 224, Loss: 0.5368, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 225, Loss: 0.5370, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 226, Loss: 0.5412, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 227, Loss: 0.5399, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 228, Loss: 0.5354, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 229, Loss: 0.5289, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 230, Loss: 0.5413, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 231, Loss: 0.5311, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 232, Loss: 0.5356, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 233, Loss: 0.5358, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 234, Loss: 0.5377, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 235, Loss: 0.5316, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 236, Loss: 0.5311, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 237, Loss: 0.5395, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 238, Loss: 0.5355, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 239, Loss: 0.5320, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 240, Loss: 0.5355, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 241, Loss: 0.5373, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 242, Loss: 0.5237, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 243, Loss: 0.5297, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 244, Loss: 0.5305, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 245, Loss: 0.5345, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 246, Loss: 0.5295, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 247, Loss: 0.5201, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 248, Loss: 0.5275, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 249, Loss: 0.5256, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 250, Loss: 0.5292, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 251, Loss: 0.5255, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 252, Loss: 0.5245, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 253, Loss: 0.5234, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 254, Loss: 0.5284, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 255, Loss: 0.5244, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 256, Loss: 0.5352, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 001, Loss: 0.6931, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 002, Loss: 0.6907, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 003, Loss: 0.6873, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 004, Loss: 0.6877, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 005, Loss: 0.6858, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 006, Loss: 0.6841, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 007, Loss: 0.6818, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 008, Loss: 0.6786, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 009, Loss: 0.6759, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 010, Loss: 0.6736, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 011, Loss: 0.6737, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 012, Loss: 0.6693, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 013, Loss: 0.6704, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 014, Loss: 0.6682, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 015, Loss: 0.6651, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 016, Loss: 0.6668, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 017, Loss: 0.6632, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 018, Loss: 0.6599, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 019, Loss: 0.6613, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 020, Loss: 0.6579, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 021, Loss: 0.6563, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 022, Loss: 0.6520, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 023, Loss: 0.6498, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 024, Loss: 0.6545, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 025, Loss: 0.6532, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 026, Loss: 0.6495, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 027, Loss: 0.6478, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 028, Loss: 0.6455, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 029, Loss: 0.6445, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 030, Loss: 0.6408, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 031, Loss: 0.6396, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 032, Loss: 0.6369, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 033, Loss: 0.6371, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 034, Loss: 0.6313, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 035, Loss: 0.6346, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 036, Loss: 0.6371, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 037, Loss: 0.6290, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 038, Loss: 0.6324, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 039, Loss: 0.6304, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 040, Loss: 0.6240, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 041, Loss: 0.6291, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 042, Loss: 0.6353, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 043, Loss: 0.6299, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 044, Loss: 0.6251, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 045, Loss: 0.6237, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 046, Loss: 0.6271, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 047, Loss: 0.6250, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 048, Loss: 0.6197, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 049, Loss: 0.6175, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 050, Loss: 0.6221, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 051, Loss: 0.6190, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 052, Loss: 0.6153, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 053, Loss: 0.6136, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 054, Loss: 0.6172, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 055, Loss: 0.6168, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 056, Loss: 0.6162, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 057, Loss: 0.6161, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 058, Loss: 0.6091, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 059, Loss: 0.6119, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 060, Loss: 0.6144, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 061, Loss: 0.6162, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 062, Loss: 0.6096, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 063, Loss: 0.6127, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 064, Loss: 0.6233, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 065, Loss: 0.6129, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 066, Loss: 0.6135, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 067, Loss: 0.6109, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 068, Loss: 0.6158, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 069, Loss: 0.6211, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 070, Loss: 0.6153, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 071, Loss: 0.6194, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 072, Loss: 0.6097, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 073, Loss: 0.6195, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 074, Loss: 0.6053, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 075, Loss: 0.6112, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 076, Loss: 0.6001, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 077, Loss: 0.6022, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 078, Loss: 0.6059, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 079, Loss: 0.6032, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 080, Loss: 0.6128, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 081, Loss: 0.6180, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 082, Loss: 0.6085, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 083, Loss: 0.6155, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 084, Loss: 0.6035, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 085, Loss: 0.6059, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 086, Loss: 0.6120, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 087, Loss: 0.6083, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 088, Loss: 0.6101, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 089, Loss: 0.6074, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 090, Loss: 0.6086, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 091, Loss: 0.6006, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 092, Loss: 0.6032, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 093, Loss: 0.6187, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 094, Loss: 0.6108, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 095, Loss: 0.6024, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 096, Loss: 0.6043, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 097, Loss: 0.6034, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 098, Loss: 0.6087, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 099, Loss: 0.6026, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 100, Loss: 0.5952, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 101, Loss: 0.6006, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 102, Loss: 0.5993, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 103, Loss: 0.6066, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 104, Loss: 0.6054, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 105, Loss: 0.6005, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 106, Loss: 0.5997, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 107, Loss: 0.6042, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 108, Loss: 0.6040, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 109, Loss: 0.6004, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 110, Loss: 0.6022, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 111, Loss: 0.5955, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 112, Loss: 0.6002, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 113, Loss: 0.6038, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 114, Loss: 0.5921, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 115, Loss: 0.5972, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 116, Loss: 0.6036, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 117, Loss: 0.6010, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 118, Loss: 0.5989, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 119, Loss: 0.5999, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 120, Loss: 0.6017, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 121, Loss: 0.6075, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 122, Loss: 0.5946, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 123, Loss: 0.5967, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 124, Loss: 0.5940, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 125, Loss: 0.5914, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 126, Loss: 0.6000, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 127, Loss: 0.5980, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 128, Loss: 0.5936, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 129, Loss: 0.5923, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 130, Loss: 0.5926, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 131, Loss: 0.5953, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 132, Loss: 0.5927, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 133, Loss: 0.5992, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 134, Loss: 0.5894, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 135, Loss: 0.5893, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 136, Loss: 0.5995, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 137, Loss: 0.5927, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 138, Loss: 0.5881, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 139, Loss: 0.5910, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 140, Loss: 0.5883, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 141, Loss: 0.5885, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 142, Loss: 0.5911, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 143, Loss: 0.5909, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 144, Loss: 0.5880, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 145, Loss: 0.5924, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 146, Loss: 0.5810, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 147, Loss: 0.5959, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 148, Loss: 0.5811, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 149, Loss: 0.5898, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 150, Loss: 0.5871, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 151, Loss: 0.5849, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 152, Loss: 0.5839, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 153, Loss: 0.5833, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 154, Loss: 0.5838, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 155, Loss: 0.5822, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 156, Loss: 0.5853, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 157, Loss: 0.5899, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 158, Loss: 0.5785, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 159, Loss: 0.5821, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 160, Loss: 0.5903, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 161, Loss: 0.5795, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 162, Loss: 0.5791, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 163, Loss: 0.5789, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 164, Loss: 0.5811, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 165, Loss: 0.5745, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 166, Loss: 0.5746, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 167, Loss: 0.5756, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 168, Loss: 0.5703, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 169, Loss: 0.5763, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 170, Loss: 0.5809, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 171, Loss: 0.5735, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 172, Loss: 0.5732, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 173, Loss: 0.5709, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 174, Loss: 0.5610, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 175, Loss: 0.5727, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 176, Loss: 0.5807, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 177, Loss: 0.5763, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 178, Loss: 0.5749, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 179, Loss: 0.5702, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 180, Loss: 0.5713, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 181, Loss: 0.5740, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 182, Loss: 0.5707, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 183, Loss: 0.5690, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 184, Loss: 0.5652, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 185, Loss: 0.5650, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 186, Loss: 0.5678, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 187, Loss: 0.5655, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 188, Loss: 0.5688, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 189, Loss: 0.5710, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 190, Loss: 0.5601, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 191, Loss: 0.5623, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 192, Loss: 0.5592, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 193, Loss: 0.5595, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 194, Loss: 0.5520, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 195, Loss: 0.5584, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 196, Loss: 0.5590, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 197, Loss: 0.5611, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 198, Loss: 0.5658, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 199, Loss: 0.5565, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 200, Loss: 0.5569, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 201, Loss: 0.5654, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 202, Loss: 0.5559, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 203, Loss: 0.5658, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 204, Loss: 0.5538, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 205, Loss: 0.5584, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 206, Loss: 0.5577, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 207, Loss: 0.5571, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 208, Loss: 0.5513, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 209, Loss: 0.5561, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 210, Loss: 0.5464, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 211, Loss: 0.5527, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 212, Loss: 0.5498, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 213, Loss: 0.5547, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 214, Loss: 0.5471, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 215, Loss: 0.5525, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 216, Loss: 0.5424, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 217, Loss: 0.5439, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 218, Loss: 0.5458, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 219, Loss: 0.5417, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 220, Loss: 0.5506, Train: 0.7176, Test: 0.6667\n",
            "Epoch: 221, Loss: 0.5470, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 222, Loss: 0.5382, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 223, Loss: 0.5398, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 224, Loss: 0.5432, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 225, Loss: 0.5410, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 226, Loss: 0.5452, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 227, Loss: 0.5451, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 228, Loss: 0.5425, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 229, Loss: 0.5376, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 230, Loss: 0.5439, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 231, Loss: 0.5369, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 232, Loss: 0.5380, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 233, Loss: 0.5374, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 234, Loss: 0.5454, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 235, Loss: 0.5385, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 236, Loss: 0.5374, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 237, Loss: 0.5484, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 238, Loss: 0.5418, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 239, Loss: 0.5389, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 240, Loss: 0.5391, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 241, Loss: 0.5438, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 242, Loss: 0.5317, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 243, Loss: 0.5392, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 244, Loss: 0.5345, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 245, Loss: 0.5441, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 246, Loss: 0.5347, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 247, Loss: 0.5230, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 248, Loss: 0.5369, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 249, Loss: 0.5317, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 250, Loss: 0.5362, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 251, Loss: 0.5275, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.5278, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 253, Loss: 0.5275, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 254, Loss: 0.5287, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 255, Loss: 0.5271, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.5404, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 001, Loss: 0.6937, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 002, Loss: 0.6912, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.6886, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6902, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6890, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6866, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6847, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.6825, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.6813, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.6780, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.6788, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.6742, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 013, Loss: 0.6774, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.6753, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 015, Loss: 0.6724, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 016, Loss: 0.6727, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 017, Loss: 0.6700, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 018, Loss: 0.6690, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 019, Loss: 0.6697, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 020, Loss: 0.6690, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 021, Loss: 0.6672, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 022, Loss: 0.6628, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 023, Loss: 0.6604, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 024, Loss: 0.6665, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 025, Loss: 0.6640, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 026, Loss: 0.6626, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 027, Loss: 0.6586, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 028, Loss: 0.6558, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.6591, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.6566, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.6560, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.6525, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.6526, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.6473, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.6517, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.6531, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.6484, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.6520, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.6500, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.6471, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.6497, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.6540, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.6503, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.6472, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.6461, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.6477, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.6467, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.6429, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.6426, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.6450, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.6412, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.6379, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 053, Loss: 0.6387, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 054, Loss: 0.6417, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 055, Loss: 0.6423, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 056, Loss: 0.6416, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 057, Loss: 0.6455, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 058, Loss: 0.6370, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 059, Loss: 0.6376, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 060, Loss: 0.6410, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 061, Loss: 0.6407, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 062, Loss: 0.6354, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 063, Loss: 0.6385, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 064, Loss: 0.6445, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 065, Loss: 0.6360, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 066, Loss: 0.6382, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 067, Loss: 0.6372, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 068, Loss: 0.6394, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 069, Loss: 0.6425, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 070, Loss: 0.6391, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 071, Loss: 0.6390, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 072, Loss: 0.6344, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 073, Loss: 0.6436, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 074, Loss: 0.6328, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 075, Loss: 0.6385, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 076, Loss: 0.6306, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 077, Loss: 0.6282, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 078, Loss: 0.6325, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 079, Loss: 0.6271, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 080, Loss: 0.6379, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 081, Loss: 0.6413, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 082, Loss: 0.6333, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 083, Loss: 0.6356, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 084, Loss: 0.6285, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 085, Loss: 0.6351, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 086, Loss: 0.6302, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 087, Loss: 0.6345, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 088, Loss: 0.6353, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 089, Loss: 0.6319, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 090, Loss: 0.6336, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 091, Loss: 0.6255, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 092, Loss: 0.6304, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 093, Loss: 0.6405, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 094, Loss: 0.6344, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 095, Loss: 0.6273, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 096, Loss: 0.6320, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 097, Loss: 0.6266, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 098, Loss: 0.6330, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 099, Loss: 0.6266, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 100, Loss: 0.6204, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 101, Loss: 0.6239, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 102, Loss: 0.6226, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 103, Loss: 0.6281, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 104, Loss: 0.6275, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 105, Loss: 0.6241, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 106, Loss: 0.6227, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 107, Loss: 0.6267, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 108, Loss: 0.6261, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 109, Loss: 0.6217, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 110, Loss: 0.6228, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 111, Loss: 0.6192, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 112, Loss: 0.6244, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 113, Loss: 0.6264, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 114, Loss: 0.6194, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 115, Loss: 0.6191, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 116, Loss: 0.6268, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 117, Loss: 0.6228, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 118, Loss: 0.6217, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 119, Loss: 0.6168, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.6212, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 121, Loss: 0.6230, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 122, Loss: 0.6179, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.6119, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.6157, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.6105, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 126, Loss: 0.6210, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 127, Loss: 0.6160, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 128, Loss: 0.6144, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 129, Loss: 0.6135, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 130, Loss: 0.6120, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 131, Loss: 0.6116, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 132, Loss: 0.6135, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 133, Loss: 0.6150, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 134, Loss: 0.6071, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 135, Loss: 0.6039, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 136, Loss: 0.6133, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 137, Loss: 0.6055, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 138, Loss: 0.6060, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 139, Loss: 0.6060, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 140, Loss: 0.6027, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 141, Loss: 0.6039, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 142, Loss: 0.6049, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 143, Loss: 0.6049, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 144, Loss: 0.6054, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 145, Loss: 0.6064, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 146, Loss: 0.5980, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 147, Loss: 0.6108, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 148, Loss: 0.6026, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 149, Loss: 0.6039, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 150, Loss: 0.5999, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 151, Loss: 0.5962, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 152, Loss: 0.5924, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 153, Loss: 0.5972, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 154, Loss: 0.5965, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 155, Loss: 0.5961, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 156, Loss: 0.5962, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 157, Loss: 0.6029, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 158, Loss: 0.5889, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 159, Loss: 0.5911, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 160, Loss: 0.6003, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 161, Loss: 0.5889, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 162, Loss: 0.5892, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 163, Loss: 0.5871, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 164, Loss: 0.5874, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 165, Loss: 0.5858, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 166, Loss: 0.5829, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 167, Loss: 0.5880, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 168, Loss: 0.5790, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 169, Loss: 0.5860, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 170, Loss: 0.5877, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 171, Loss: 0.5824, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 172, Loss: 0.5823, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 173, Loss: 0.5796, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 174, Loss: 0.5732, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 175, Loss: 0.5791, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 176, Loss: 0.5857, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 177, Loss: 0.5837, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 178, Loss: 0.5828, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 179, Loss: 0.5784, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 180, Loss: 0.5766, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 181, Loss: 0.5787, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 182, Loss: 0.5772, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 183, Loss: 0.5751, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.5687, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 185, Loss: 0.5726, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 186, Loss: 0.5731, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 187, Loss: 0.5692, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 188, Loss: 0.5750, Train: 0.7059, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.5727, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 190, Loss: 0.5649, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 191, Loss: 0.5681, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 192, Loss: 0.5643, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 193, Loss: 0.5637, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 194, Loss: 0.5570, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 195, Loss: 0.5650, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 196, Loss: 0.5668, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 197, Loss: 0.5647, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 198, Loss: 0.5701, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 199, Loss: 0.5588, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 200, Loss: 0.5621, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 201, Loss: 0.5691, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 202, Loss: 0.5585, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 203, Loss: 0.5673, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 204, Loss: 0.5596, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 205, Loss: 0.5621, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 206, Loss: 0.5595, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 207, Loss: 0.5586, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 208, Loss: 0.5565, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 209, Loss: 0.5542, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 210, Loss: 0.5509, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 211, Loss: 0.5566, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 212, Loss: 0.5519, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 213, Loss: 0.5558, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 214, Loss: 0.5508, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 215, Loss: 0.5580, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 216, Loss: 0.5513, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 217, Loss: 0.5474, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 218, Loss: 0.5500, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 219, Loss: 0.5428, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 220, Loss: 0.5544, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 221, Loss: 0.5528, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 222, Loss: 0.5374, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 223, Loss: 0.5454, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 224, Loss: 0.5443, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 225, Loss: 0.5437, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 226, Loss: 0.5501, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 227, Loss: 0.5488, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 228, Loss: 0.5480, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 229, Loss: 0.5495, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 230, Loss: 0.5480, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 231, Loss: 0.5414, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 232, Loss: 0.5468, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 233, Loss: 0.5415, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 234, Loss: 0.5451, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 235, Loss: 0.5425, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 236, Loss: 0.5433, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 237, Loss: 0.5521, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 238, Loss: 0.5451, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 239, Loss: 0.5427, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 240, Loss: 0.5465, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 241, Loss: 0.5457, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 242, Loss: 0.5411, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 243, Loss: 0.5435, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 244, Loss: 0.5423, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 245, Loss: 0.5467, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 246, Loss: 0.5408, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 247, Loss: 0.5320, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 248, Loss: 0.5418, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 249, Loss: 0.5445, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 250, Loss: 0.5424, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 251, Loss: 0.5340, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 252, Loss: 0.5353, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 253, Loss: 0.5330, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 254, Loss: 0.5347, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 255, Loss: 0.5347, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 256, Loss: 0.5490, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 001, Loss: 0.6937, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 002, Loss: 0.6910, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.6889, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6895, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6896, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6869, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6851, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.6816, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.6809, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.6780, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.6780, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.6748, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 013, Loss: 0.6778, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.6760, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 015, Loss: 0.6729, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 016, Loss: 0.6720, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 017, Loss: 0.6690, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 018, Loss: 0.6693, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 019, Loss: 0.6708, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 020, Loss: 0.6694, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 021, Loss: 0.6678, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 022, Loss: 0.6645, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 023, Loss: 0.6611, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 024, Loss: 0.6658, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 025, Loss: 0.6637, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 026, Loss: 0.6633, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 027, Loss: 0.6585, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 028, Loss: 0.6568, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.6596, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.6575, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.6574, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.6529, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.6526, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.6492, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.6517, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.6531, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.6484, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.6531, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.6509, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.6489, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.6510, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.6553, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.6513, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.6491, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.6481, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.6477, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.6482, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.6441, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.6431, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.6473, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.6426, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.6407, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 053, Loss: 0.6408, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 054, Loss: 0.6444, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 055, Loss: 0.6445, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 056, Loss: 0.6420, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 057, Loss: 0.6448, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 058, Loss: 0.6387, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 059, Loss: 0.6404, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 060, Loss: 0.6442, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 061, Loss: 0.6420, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 062, Loss: 0.6376, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 063, Loss: 0.6432, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 064, Loss: 0.6452, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 065, Loss: 0.6389, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 066, Loss: 0.6403, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 067, Loss: 0.6375, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 068, Loss: 0.6402, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 069, Loss: 0.6476, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 070, Loss: 0.6416, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 071, Loss: 0.6422, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 072, Loss: 0.6369, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 073, Loss: 0.6448, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 074, Loss: 0.6348, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 075, Loss: 0.6421, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 076, Loss: 0.6346, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 077, Loss: 0.6344, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 078, Loss: 0.6372, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 079, Loss: 0.6281, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 080, Loss: 0.6423, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 081, Loss: 0.6438, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 082, Loss: 0.6386, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 083, Loss: 0.6417, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 084, Loss: 0.6328, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 085, Loss: 0.6393, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 086, Loss: 0.6330, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 087, Loss: 0.6393, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 088, Loss: 0.6409, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 089, Loss: 0.6375, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 090, Loss: 0.6412, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 091, Loss: 0.6296, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 092, Loss: 0.6347, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 093, Loss: 0.6430, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 094, Loss: 0.6386, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 095, Loss: 0.6315, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 096, Loss: 0.6357, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 097, Loss: 0.6298, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 098, Loss: 0.6378, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 099, Loss: 0.6328, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 100, Loss: 0.6261, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 101, Loss: 0.6329, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 102, Loss: 0.6290, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 103, Loss: 0.6342, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 104, Loss: 0.6340, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 105, Loss: 0.6316, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 106, Loss: 0.6305, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 107, Loss: 0.6330, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 108, Loss: 0.6321, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 109, Loss: 0.6279, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 110, Loss: 0.6279, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 111, Loss: 0.6275, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 112, Loss: 0.6312, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 113, Loss: 0.6335, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 114, Loss: 0.6276, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 115, Loss: 0.6271, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 116, Loss: 0.6348, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 117, Loss: 0.6288, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 118, Loss: 0.6319, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 119, Loss: 0.6271, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.6298, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 121, Loss: 0.6313, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 122, Loss: 0.6261, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.6209, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.6258, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.6186, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 126, Loss: 0.6306, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 127, Loss: 0.6253, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 128, Loss: 0.6251, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 129, Loss: 0.6247, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 130, Loss: 0.6228, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 131, Loss: 0.6232, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 132, Loss: 0.6250, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 133, Loss: 0.6276, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 134, Loss: 0.6191, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 135, Loss: 0.6149, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 136, Loss: 0.6232, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 137, Loss: 0.6195, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 138, Loss: 0.6163, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 139, Loss: 0.6184, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 140, Loss: 0.6166, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 141, Loss: 0.6166, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 142, Loss: 0.6194, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 143, Loss: 0.6159, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 144, Loss: 0.6211, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 145, Loss: 0.6205, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 146, Loss: 0.6110, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 147, Loss: 0.6272, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 148, Loss: 0.6148, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 149, Loss: 0.6178, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 150, Loss: 0.6153, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 151, Loss: 0.6097, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 152, Loss: 0.6111, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 153, Loss: 0.6162, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 154, Loss: 0.6120, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 155, Loss: 0.6113, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 156, Loss: 0.6121, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 157, Loss: 0.6184, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 158, Loss: 0.6089, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 159, Loss: 0.6112, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 160, Loss: 0.6147, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 161, Loss: 0.6068, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 162, Loss: 0.6099, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 163, Loss: 0.6045, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 164, Loss: 0.6076, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 165, Loss: 0.6055, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 166, Loss: 0.6033, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 167, Loss: 0.6065, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 168, Loss: 0.5995, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 169, Loss: 0.6051, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 170, Loss: 0.6082, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 171, Loss: 0.6043, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 172, Loss: 0.6051, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 173, Loss: 0.6006, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 174, Loss: 0.5962, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 175, Loss: 0.5979, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 176, Loss: 0.6068, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 177, Loss: 0.6050, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 178, Loss: 0.6057, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 179, Loss: 0.6039, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 180, Loss: 0.5990, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 181, Loss: 0.6020, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 182, Loss: 0.5996, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 183, Loss: 0.5977, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.5927, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 185, Loss: 0.5960, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 186, Loss: 0.5940, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 187, Loss: 0.5957, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 188, Loss: 0.5965, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 189, Loss: 0.5975, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 190, Loss: 0.5901, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 191, Loss: 0.5930, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 192, Loss: 0.5883, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 193, Loss: 0.5885, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 194, Loss: 0.5843, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 195, Loss: 0.5877, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 196, Loss: 0.5918, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 197, Loss: 0.5917, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 198, Loss: 0.5940, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 199, Loss: 0.5854, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 200, Loss: 0.5878, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 201, Loss: 0.5928, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 202, Loss: 0.5844, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 203, Loss: 0.5921, Train: 0.7000, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.5868, Train: 0.7000, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.5874, Train: 0.6941, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.5849, Train: 0.6941, Test: 0.9444\n",
            "Epoch: 207, Loss: 0.5850, Train: 0.7000, Test: 0.9444\n",
            "Epoch: 208, Loss: 0.5806, Train: 0.7118, Test: 0.9444\n",
            "Epoch: 209, Loss: 0.5806, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 210, Loss: 0.5770, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 211, Loss: 0.5806, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 212, Loss: 0.5756, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 213, Loss: 0.5819, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 214, Loss: 0.5790, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 215, Loss: 0.5823, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 216, Loss: 0.5751, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 217, Loss: 0.5744, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 218, Loss: 0.5762, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 219, Loss: 0.5711, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 220, Loss: 0.5811, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 221, Loss: 0.5760, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 222, Loss: 0.5637, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 223, Loss: 0.5716, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 224, Loss: 0.5699, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 225, Loss: 0.5696, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 226, Loss: 0.5725, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 227, Loss: 0.5754, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 228, Loss: 0.5741, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 229, Loss: 0.5709, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 230, Loss: 0.5734, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 231, Loss: 0.5660, Train: 0.7118, Test: 0.9444\n",
            "Epoch: 232, Loss: 0.5705, Train: 0.7118, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.5666, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.5720, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.5675, Train: 0.7000, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.5682, Train: 0.7000, Test: 0.9444\n",
            "Epoch: 237, Loss: 0.5746, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 238, Loss: 0.5707, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 239, Loss: 0.5665, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 240, Loss: 0.5696, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 241, Loss: 0.5702, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 242, Loss: 0.5680, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 243, Loss: 0.5674, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 244, Loss: 0.5675, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 245, Loss: 0.5706, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 246, Loss: 0.5671, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 247, Loss: 0.5608, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 248, Loss: 0.5645, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 249, Loss: 0.5641, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 250, Loss: 0.5645, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 251, Loss: 0.5590, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.5598, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 253, Loss: 0.5559, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 254, Loss: 0.5576, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 255, Loss: 0.5570, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 256, Loss: 0.5697, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.6937, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 002, Loss: 0.6901, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 003, Loss: 0.6886, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 004, Loss: 0.6892, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 005, Loss: 0.6876, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 006, Loss: 0.6843, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.6844, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 008, Loss: 0.6789, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 009, Loss: 0.6794, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.6755, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 011, Loss: 0.6748, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.6721, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 013, Loss: 0.6747, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 014, Loss: 0.6719, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 015, Loss: 0.6687, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 016, Loss: 0.6670, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.6640, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 018, Loss: 0.6631, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 019, Loss: 0.6645, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 020, Loss: 0.6642, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.6618, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.6613, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.6536, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.6574, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.6542, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.6559, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.6505, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.6471, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.6521, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 030, Loss: 0.6477, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 031, Loss: 0.6487, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 032, Loss: 0.6423, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 033, Loss: 0.6419, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 034, Loss: 0.6365, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 035, Loss: 0.6393, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 036, Loss: 0.6431, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 037, Loss: 0.6380, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.6395, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.6385, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.6340, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.6402, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.6442, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.6378, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.6355, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.6353, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 046, Loss: 0.6345, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 047, Loss: 0.6331, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 048, Loss: 0.6289, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 049, Loss: 0.6293, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 050, Loss: 0.6349, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 051, Loss: 0.6270, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 052, Loss: 0.6235, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 053, Loss: 0.6215, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 054, Loss: 0.6294, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 055, Loss: 0.6294, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 056, Loss: 0.6266, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 057, Loss: 0.6254, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 058, Loss: 0.6205, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 059, Loss: 0.6210, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 060, Loss: 0.6270, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 061, Loss: 0.6216, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 062, Loss: 0.6227, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 063, Loss: 0.6242, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 064, Loss: 0.6291, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 065, Loss: 0.6218, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 066, Loss: 0.6207, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 067, Loss: 0.6188, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 068, Loss: 0.6206, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 069, Loss: 0.6311, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 070, Loss: 0.6237, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 071, Loss: 0.6236, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 072, Loss: 0.6170, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 073, Loss: 0.6236, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 074, Loss: 0.6168, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 075, Loss: 0.6223, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 076, Loss: 0.6142, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 077, Loss: 0.6165, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 078, Loss: 0.6183, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 079, Loss: 0.6093, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 080, Loss: 0.6253, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 081, Loss: 0.6230, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 082, Loss: 0.6146, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 083, Loss: 0.6217, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 084, Loss: 0.6118, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 085, Loss: 0.6187, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 086, Loss: 0.6101, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 087, Loss: 0.6184, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 088, Loss: 0.6194, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 089, Loss: 0.6144, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 090, Loss: 0.6210, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 091, Loss: 0.6134, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 092, Loss: 0.6151, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 093, Loss: 0.6197, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 094, Loss: 0.6218, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 095, Loss: 0.6065, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 096, Loss: 0.6169, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 097, Loss: 0.6059, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 098, Loss: 0.6160, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 099, Loss: 0.6101, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 100, Loss: 0.6049, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 101, Loss: 0.6128, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 102, Loss: 0.6090, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 103, Loss: 0.6149, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 104, Loss: 0.6097, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 105, Loss: 0.6085, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 106, Loss: 0.6096, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 107, Loss: 0.6083, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 108, Loss: 0.6093, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 109, Loss: 0.6035, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 110, Loss: 0.6047, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 111, Loss: 0.6036, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 112, Loss: 0.6067, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 113, Loss: 0.6122, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 114, Loss: 0.6060, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 115, Loss: 0.6061, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 116, Loss: 0.6113, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 117, Loss: 0.6082, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 118, Loss: 0.6088, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 119, Loss: 0.6021, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 120, Loss: 0.6063, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 121, Loss: 0.6092, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 122, Loss: 0.6043, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 123, Loss: 0.5951, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 124, Loss: 0.6019, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 125, Loss: 0.5960, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 126, Loss: 0.6034, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 127, Loss: 0.6008, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 128, Loss: 0.5969, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 129, Loss: 0.5998, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 130, Loss: 0.5939, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 131, Loss: 0.5932, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 132, Loss: 0.5979, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 133, Loss: 0.6007, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 134, Loss: 0.5899, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 135, Loss: 0.5871, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 136, Loss: 0.5963, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 137, Loss: 0.5937, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 138, Loss: 0.5880, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 139, Loss: 0.5882, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 140, Loss: 0.5865, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 141, Loss: 0.5887, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 142, Loss: 0.5836, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 143, Loss: 0.5840, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 144, Loss: 0.5912, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 145, Loss: 0.5888, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 146, Loss: 0.5827, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 147, Loss: 0.5965, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 148, Loss: 0.5843, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 149, Loss: 0.5852, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 150, Loss: 0.5822, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 151, Loss: 0.5774, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 152, Loss: 0.5798, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 153, Loss: 0.5823, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 154, Loss: 0.5774, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 155, Loss: 0.5811, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 156, Loss: 0.5790, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 157, Loss: 0.5865, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 158, Loss: 0.5731, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 159, Loss: 0.5766, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 160, Loss: 0.5822, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 161, Loss: 0.5694, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 162, Loss: 0.5767, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 163, Loss: 0.5659, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 164, Loss: 0.5758, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 165, Loss: 0.5680, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 166, Loss: 0.5655, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 167, Loss: 0.5749, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 168, Loss: 0.5649, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 169, Loss: 0.5670, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 170, Loss: 0.5682, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 171, Loss: 0.5664, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 172, Loss: 0.5682, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 173, Loss: 0.5623, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 174, Loss: 0.5582, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 175, Loss: 0.5598, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 176, Loss: 0.5660, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 177, Loss: 0.5644, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 178, Loss: 0.5674, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 179, Loss: 0.5614, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 180, Loss: 0.5606, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 181, Loss: 0.5580, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 182, Loss: 0.5589, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 183, Loss: 0.5591, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 184, Loss: 0.5502, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 185, Loss: 0.5521, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 186, Loss: 0.5545, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 187, Loss: 0.5544, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 188, Loss: 0.5539, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 189, Loss: 0.5547, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 190, Loss: 0.5471, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 191, Loss: 0.5520, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 192, Loss: 0.5463, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 193, Loss: 0.5435, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 194, Loss: 0.5423, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 195, Loss: 0.5458, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 196, Loss: 0.5457, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 197, Loss: 0.5460, Train: 0.7529, Test: 0.4444\n",
            "Epoch: 198, Loss: 0.5517, Train: 0.7529, Test: 0.3889\n",
            "Epoch: 199, Loss: 0.5414, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 200, Loss: 0.5448, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 201, Loss: 0.5515, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 202, Loss: 0.5417, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 203, Loss: 0.5461, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 204, Loss: 0.5416, Train: 0.7588, Test: 0.3889\n",
            "Epoch: 205, Loss: 0.5425, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 206, Loss: 0.5364, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 207, Loss: 0.5378, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 208, Loss: 0.5354, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 209, Loss: 0.5363, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 210, Loss: 0.5277, Train: 0.7647, Test: 0.3889\n",
            "Epoch: 211, Loss: 0.5349, Train: 0.7529, Test: 0.3889\n",
            "Epoch: 212, Loss: 0.5281, Train: 0.7529, Test: 0.4444\n",
            "Epoch: 213, Loss: 0.5337, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 214, Loss: 0.5338, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 215, Loss: 0.5384, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 216, Loss: 0.5332, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 217, Loss: 0.5283, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 218, Loss: 0.5282, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 219, Loss: 0.5210, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 220, Loss: 0.5383, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 221, Loss: 0.5306, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 222, Loss: 0.5157, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 223, Loss: 0.5257, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 224, Loss: 0.5211, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 225, Loss: 0.5189, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 226, Loss: 0.5240, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 227, Loss: 0.5296, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 228, Loss: 0.5291, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 229, Loss: 0.5273, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 230, Loss: 0.5264, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 231, Loss: 0.5182, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 232, Loss: 0.5291, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 233, Loss: 0.5138, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 234, Loss: 0.5253, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 235, Loss: 0.5226, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 236, Loss: 0.5198, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 237, Loss: 0.5286, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 238, Loss: 0.5228, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 239, Loss: 0.5195, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 240, Loss: 0.5220, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 241, Loss: 0.5258, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 242, Loss: 0.5218, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 243, Loss: 0.5173, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 244, Loss: 0.5229, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 245, Loss: 0.5205, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 246, Loss: 0.5195, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 247, Loss: 0.5184, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 248, Loss: 0.5173, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 249, Loss: 0.5151, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 250, Loss: 0.5204, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 251, Loss: 0.5092, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 252, Loss: 0.5098, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 253, Loss: 0.5116, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 254, Loss: 0.5117, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 255, Loss: 0.5130, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 256, Loss: 0.5208, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 001, Loss: 0.6938, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 002, Loss: 0.6907, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 003, Loss: 0.6885, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 004, Loss: 0.6890, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 005, Loss: 0.6871, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 006, Loss: 0.6839, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.6846, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 008, Loss: 0.6792, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 009, Loss: 0.6794, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.6756, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 011, Loss: 0.6748, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.6716, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 013, Loss: 0.6740, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 014, Loss: 0.6724, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 015, Loss: 0.6684, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 016, Loss: 0.6679, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.6654, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 018, Loss: 0.6641, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 019, Loss: 0.6639, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 020, Loss: 0.6634, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.6612, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.6618, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.6558, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.6568, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.6547, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.6566, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.6496, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.6474, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.6511, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 030, Loss: 0.6490, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 031, Loss: 0.6485, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 032, Loss: 0.6418, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 033, Loss: 0.6435, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 034, Loss: 0.6374, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 035, Loss: 0.6389, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 036, Loss: 0.6407, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 037, Loss: 0.6380, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.6391, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.6389, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.6312, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.6380, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.6453, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.6352, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.6375, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.6365, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 046, Loss: 0.6338, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 047, Loss: 0.6376, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 048, Loss: 0.6284, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 049, Loss: 0.6291, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 050, Loss: 0.6354, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 051, Loss: 0.6246, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 052, Loss: 0.6244, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 053, Loss: 0.6226, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 054, Loss: 0.6300, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 055, Loss: 0.6310, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 056, Loss: 0.6283, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 057, Loss: 0.6244, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 058, Loss: 0.6238, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 059, Loss: 0.6218, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 060, Loss: 0.6312, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 061, Loss: 0.6213, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 062, Loss: 0.6274, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 063, Loss: 0.6249, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 064, Loss: 0.6275, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 065, Loss: 0.6241, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 066, Loss: 0.6235, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 067, Loss: 0.6195, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 068, Loss: 0.6240, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 069, Loss: 0.6303, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 070, Loss: 0.6237, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 071, Loss: 0.6248, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 072, Loss: 0.6204, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 073, Loss: 0.6224, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 074, Loss: 0.6186, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 075, Loss: 0.6261, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 076, Loss: 0.6179, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 077, Loss: 0.6189, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 078, Loss: 0.6181, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 079, Loss: 0.6132, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 080, Loss: 0.6244, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 081, Loss: 0.6229, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 082, Loss: 0.6190, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 083, Loss: 0.6236, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 084, Loss: 0.6146, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 085, Loss: 0.6232, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 086, Loss: 0.6151, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 087, Loss: 0.6208, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 088, Loss: 0.6241, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 089, Loss: 0.6196, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 090, Loss: 0.6225, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 091, Loss: 0.6154, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 092, Loss: 0.6151, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 093, Loss: 0.6193, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 094, Loss: 0.6230, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 095, Loss: 0.6099, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 096, Loss: 0.6210, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 097, Loss: 0.6092, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 098, Loss: 0.6164, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 099, Loss: 0.6136, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 100, Loss: 0.6121, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 101, Loss: 0.6163, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 102, Loss: 0.6114, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 103, Loss: 0.6141, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 104, Loss: 0.6128, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 105, Loss: 0.6095, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 106, Loss: 0.6160, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 107, Loss: 0.6115, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 108, Loss: 0.6104, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 109, Loss: 0.6082, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 110, Loss: 0.6104, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 111, Loss: 0.6067, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 112, Loss: 0.6126, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 113, Loss: 0.6171, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 114, Loss: 0.6114, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 115, Loss: 0.6098, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 116, Loss: 0.6159, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 117, Loss: 0.6119, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 118, Loss: 0.6146, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 119, Loss: 0.6044, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 120, Loss: 0.6095, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 121, Loss: 0.6127, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 122, Loss: 0.6102, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 123, Loss: 0.5997, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 124, Loss: 0.6065, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 125, Loss: 0.6039, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 126, Loss: 0.6063, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 127, Loss: 0.6045, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 128, Loss: 0.6051, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 129, Loss: 0.6060, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 130, Loss: 0.6009, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 131, Loss: 0.5973, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 132, Loss: 0.6034, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 133, Loss: 0.6072, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 134, Loss: 0.5977, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 135, Loss: 0.5956, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 136, Loss: 0.6027, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 137, Loss: 0.6023, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 138, Loss: 0.5963, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 139, Loss: 0.5966, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 140, Loss: 0.5917, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 141, Loss: 0.6020, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 142, Loss: 0.5898, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 143, Loss: 0.5954, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 144, Loss: 0.5995, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 145, Loss: 0.5956, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 146, Loss: 0.5902, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 147, Loss: 0.6033, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 148, Loss: 0.5955, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 149, Loss: 0.5927, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 150, Loss: 0.5892, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 151, Loss: 0.5827, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 152, Loss: 0.5874, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 153, Loss: 0.5931, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 154, Loss: 0.5878, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 155, Loss: 0.5908, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 156, Loss: 0.5865, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 157, Loss: 0.5949, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 158, Loss: 0.5864, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 159, Loss: 0.5876, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 160, Loss: 0.5922, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 161, Loss: 0.5822, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 162, Loss: 0.5862, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 163, Loss: 0.5785, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 164, Loss: 0.5885, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 165, Loss: 0.5784, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 166, Loss: 0.5772, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 167, Loss: 0.5876, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 168, Loss: 0.5753, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 169, Loss: 0.5781, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 170, Loss: 0.5783, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 171, Loss: 0.5755, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 172, Loss: 0.5792, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 173, Loss: 0.5746, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 174, Loss: 0.5723, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 175, Loss: 0.5700, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 176, Loss: 0.5777, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 177, Loss: 0.5781, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 178, Loss: 0.5788, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 179, Loss: 0.5737, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 180, Loss: 0.5772, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 181, Loss: 0.5708, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 182, Loss: 0.5710, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 183, Loss: 0.5713, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 184, Loss: 0.5636, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 185, Loss: 0.5662, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 186, Loss: 0.5645, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 187, Loss: 0.5695, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 188, Loss: 0.5643, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 189, Loss: 0.5692, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 190, Loss: 0.5599, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 191, Loss: 0.5671, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 192, Loss: 0.5622, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 193, Loss: 0.5576, Train: 0.6882, Test: 0.6667\n",
            "Epoch: 194, Loss: 0.5577, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 195, Loss: 0.5577, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 196, Loss: 0.5601, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 197, Loss: 0.5628, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 198, Loss: 0.5655, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 199, Loss: 0.5550, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 200, Loss: 0.5621, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 201, Loss: 0.5621, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 202, Loss: 0.5559, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 203, Loss: 0.5609, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 204, Loss: 0.5568, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 205, Loss: 0.5556, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 206, Loss: 0.5493, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 207, Loss: 0.5546, Train: 0.7176, Test: 0.6667\n",
            "Epoch: 208, Loss: 0.5512, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 209, Loss: 0.5523, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 210, Loss: 0.5464, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 211, Loss: 0.5506, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 212, Loss: 0.5389, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 213, Loss: 0.5471, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 214, Loss: 0.5467, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 215, Loss: 0.5548, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 216, Loss: 0.5456, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 217, Loss: 0.5483, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 218, Loss: 0.5448, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 219, Loss: 0.5361, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 220, Loss: 0.5528, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 221, Loss: 0.5437, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 222, Loss: 0.5322, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 223, Loss: 0.5412, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 224, Loss: 0.5370, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 225, Loss: 0.5365, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 226, Loss: 0.5402, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.5475, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.5427, Train: 0.7235, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.5433, Train: 0.7235, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.5393, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 231, Loss: 0.5350, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 232, Loss: 0.5444, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 233, Loss: 0.5330, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 234, Loss: 0.5408, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 235, Loss: 0.5385, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 236, Loss: 0.5308, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 237, Loss: 0.5422, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 238, Loss: 0.5376, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 239, Loss: 0.5373, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 240, Loss: 0.5345, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 241, Loss: 0.5356, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 242, Loss: 0.5362, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 243, Loss: 0.5306, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 244, Loss: 0.5347, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 245, Loss: 0.5301, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 246, Loss: 0.5338, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 247, Loss: 0.5334, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 248, Loss: 0.5343, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 249, Loss: 0.5278, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 250, Loss: 0.5325, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 251, Loss: 0.5218, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 252, Loss: 0.5261, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 253, Loss: 0.5258, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 254, Loss: 0.5232, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 255, Loss: 0.5245, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 256, Loss: 0.5344, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 001, Loss: 0.6936, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 002, Loss: 0.6901, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 003, Loss: 0.6878, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 004, Loss: 0.6881, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 005, Loss: 0.6860, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 006, Loss: 0.6824, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 007, Loss: 0.6811, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 008, Loss: 0.6758, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 009, Loss: 0.6785, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 010, Loss: 0.6721, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 011, Loss: 0.6712, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 012, Loss: 0.6684, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 013, Loss: 0.6695, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 014, Loss: 0.6679, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 015, Loss: 0.6640, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 016, Loss: 0.6624, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 017, Loss: 0.6592, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 018, Loss: 0.6600, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 019, Loss: 0.6588, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 020, Loss: 0.6561, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 021, Loss: 0.6542, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 022, Loss: 0.6557, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 023, Loss: 0.6491, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 024, Loss: 0.6489, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 025, Loss: 0.6453, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 026, Loss: 0.6475, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 027, Loss: 0.6404, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 028, Loss: 0.6369, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 029, Loss: 0.6419, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 030, Loss: 0.6370, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 031, Loss: 0.6365, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 032, Loss: 0.6327, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 033, Loss: 0.6318, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 034, Loss: 0.6264, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 035, Loss: 0.6258, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 036, Loss: 0.6284, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 037, Loss: 0.6255, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 038, Loss: 0.6262, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 039, Loss: 0.6261, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 040, Loss: 0.6182, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 041, Loss: 0.6225, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 042, Loss: 0.6311, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 043, Loss: 0.6181, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 044, Loss: 0.6221, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 045, Loss: 0.6225, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 046, Loss: 0.6200, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 047, Loss: 0.6210, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 048, Loss: 0.6133, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 049, Loss: 0.6094, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 050, Loss: 0.6174, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 051, Loss: 0.6102, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 052, Loss: 0.6045, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 053, Loss: 0.6038, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 054, Loss: 0.6107, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 055, Loss: 0.6175, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 056, Loss: 0.6078, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 057, Loss: 0.6055, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 058, Loss: 0.6056, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 059, Loss: 0.6024, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 060, Loss: 0.6135, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 061, Loss: 0.6008, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 062, Loss: 0.6084, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 063, Loss: 0.6066, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 064, Loss: 0.6105, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 065, Loss: 0.6032, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 066, Loss: 0.6033, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 067, Loss: 0.6001, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 068, Loss: 0.6055, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 069, Loss: 0.6084, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 070, Loss: 0.6065, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 071, Loss: 0.6065, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 072, Loss: 0.6007, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 073, Loss: 0.6061, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 074, Loss: 0.6025, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 075, Loss: 0.6062, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 076, Loss: 0.5951, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 077, Loss: 0.5996, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 078, Loss: 0.5992, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 079, Loss: 0.5908, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 080, Loss: 0.6056, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 081, Loss: 0.6071, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 082, Loss: 0.5992, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 083, Loss: 0.6063, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 084, Loss: 0.5940, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 085, Loss: 0.6050, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 086, Loss: 0.5989, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 087, Loss: 0.6037, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 088, Loss: 0.6074, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 089, Loss: 0.6022, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 090, Loss: 0.6035, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 091, Loss: 0.6025, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 092, Loss: 0.5969, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 093, Loss: 0.6014, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 094, Loss: 0.6061, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 095, Loss: 0.5936, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 096, Loss: 0.6017, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 097, Loss: 0.5884, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 098, Loss: 0.5980, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 099, Loss: 0.5957, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 100, Loss: 0.5934, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 101, Loss: 0.6005, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 102, Loss: 0.5935, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 103, Loss: 0.5960, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 104, Loss: 0.5976, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 105, Loss: 0.5966, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 106, Loss: 0.5998, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 107, Loss: 0.5961, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 108, Loss: 0.5943, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 109, Loss: 0.5959, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 110, Loss: 0.5946, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 111, Loss: 0.5935, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 112, Loss: 0.6007, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 113, Loss: 0.6012, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 114, Loss: 0.5951, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 115, Loss: 0.5947, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 116, Loss: 0.5994, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 117, Loss: 0.5956, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 118, Loss: 0.5992, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 119, Loss: 0.5888, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 120, Loss: 0.5942, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 121, Loss: 0.6008, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 122, Loss: 0.5967, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 123, Loss: 0.5860, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 124, Loss: 0.5930, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 125, Loss: 0.5913, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 126, Loss: 0.5968, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 127, Loss: 0.5898, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 128, Loss: 0.5950, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 129, Loss: 0.5956, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 130, Loss: 0.5923, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 131, Loss: 0.5854, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 132, Loss: 0.5913, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 133, Loss: 0.5942, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 134, Loss: 0.5851, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 135, Loss: 0.5849, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 136, Loss: 0.5897, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 137, Loss: 0.5922, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 138, Loss: 0.5865, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 139, Loss: 0.5852, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 140, Loss: 0.5816, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 141, Loss: 0.5886, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 142, Loss: 0.5793, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 143, Loss: 0.5846, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 144, Loss: 0.5879, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 145, Loss: 0.5845, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 146, Loss: 0.5809, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 147, Loss: 0.5917, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 148, Loss: 0.5880, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 149, Loss: 0.5823, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 150, Loss: 0.5784, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 151, Loss: 0.5771, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 152, Loss: 0.5758, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 153, Loss: 0.5852, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 154, Loss: 0.5796, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 155, Loss: 0.5839, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 156, Loss: 0.5773, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 157, Loss: 0.5880, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 158, Loss: 0.5807, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 159, Loss: 0.5808, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 160, Loss: 0.5844, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 161, Loss: 0.5762, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 162, Loss: 0.5768, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 163, Loss: 0.5691, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 164, Loss: 0.5850, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 165, Loss: 0.5737, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 166, Loss: 0.5734, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 167, Loss: 0.5847, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 168, Loss: 0.5730, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 169, Loss: 0.5721, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 170, Loss: 0.5731, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 171, Loss: 0.5702, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 172, Loss: 0.5760, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 173, Loss: 0.5744, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 174, Loss: 0.5713, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 175, Loss: 0.5672, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 176, Loss: 0.5750, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 177, Loss: 0.5772, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 178, Loss: 0.5767, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 179, Loss: 0.5711, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 180, Loss: 0.5757, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 181, Loss: 0.5741, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 182, Loss: 0.5689, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 183, Loss: 0.5700, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 184, Loss: 0.5668, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 185, Loss: 0.5677, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 186, Loss: 0.5638, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 187, Loss: 0.5717, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 188, Loss: 0.5630, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 189, Loss: 0.5709, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 190, Loss: 0.5627, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 191, Loss: 0.5664, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 192, Loss: 0.5643, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 193, Loss: 0.5608, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 194, Loss: 0.5587, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 195, Loss: 0.5613, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 196, Loss: 0.5619, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 197, Loss: 0.5645, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 198, Loss: 0.5663, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 199, Loss: 0.5590, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 200, Loss: 0.5693, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 201, Loss: 0.5654, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 202, Loss: 0.5618, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 203, Loss: 0.5661, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 204, Loss: 0.5622, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 205, Loss: 0.5641, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 206, Loss: 0.5490, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 207, Loss: 0.5606, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 208, Loss: 0.5575, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 209, Loss: 0.5584, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 210, Loss: 0.5544, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 211, Loss: 0.5570, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 212, Loss: 0.5455, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 213, Loss: 0.5508, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 214, Loss: 0.5501, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 215, Loss: 0.5598, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 216, Loss: 0.5529, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 217, Loss: 0.5532, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 218, Loss: 0.5513, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 219, Loss: 0.5448, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 220, Loss: 0.5551, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 221, Loss: 0.5514, Train: 0.7000, Test: 0.4444\n",
            "Epoch: 222, Loss: 0.5441, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 223, Loss: 0.5479, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 224, Loss: 0.5444, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 225, Loss: 0.5451, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 226, Loss: 0.5496, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 227, Loss: 0.5524, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 228, Loss: 0.5526, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 229, Loss: 0.5523, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 230, Loss: 0.5437, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 231, Loss: 0.5463, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 232, Loss: 0.5531, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 233, Loss: 0.5444, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 234, Loss: 0.5493, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 235, Loss: 0.5455, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 236, Loss: 0.5385, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 237, Loss: 0.5488, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 238, Loss: 0.5467, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 239, Loss: 0.5479, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 240, Loss: 0.5450, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 241, Loss: 0.5433, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 242, Loss: 0.5483, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 243, Loss: 0.5422, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 244, Loss: 0.5396, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 245, Loss: 0.5431, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 246, Loss: 0.5398, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 247, Loss: 0.5426, Train: 0.7235, Test: 0.6111\n",
            "Epoch: 248, Loss: 0.5484, Train: 0.7176, Test: 0.6667\n",
            "Epoch: 249, Loss: 0.5357, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 250, Loss: 0.5414, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 251, Loss: 0.5304, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.5379, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 253, Loss: 0.5340, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 254, Loss: 0.5343, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 255, Loss: 0.5333, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.5413, Train: 0.7353, Test: 0.7222\n",
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.54,    Train:0.72,    Test:0.73\n",
            "Median time per epoch: 0.0447s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model2：GIN\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        total_correct += int((pred == data.y).sum())\n",
        "    return total_correct / len(loader.dataset)\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset[:start] + dataset[end:]\n",
        "    test_dataset = dataset[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model = GIN(\n",
        "        in_channels=params[\"input_features\"],\n",
        "        hidden_channels=params[\"hidden_features_gin\"],\n",
        "        out_channels=params[\"num_classes\"],\n",
        "        num_layers=params[\"num_gin_layers\"],\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss) * data.num_graphs\n",
        "        return total_loss / len(train_loader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "\n",
        "        total_correct = 0\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=-1)\n",
        "            total_correct += int((pred == data.y).sum())\n",
        "        return total_correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc = test(train_loader)\n",
        "        test_acc = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUmm1Mm1kSZh",
        "outputId": "d075f578-4b6d-4952-8d25-a58220b69f43"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.6948, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 002, Loss: 0.6851, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 003, Loss: 0.6712, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 004, Loss: 0.6566, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 005, Loss: 0.6520, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 006, Loss: 0.6462, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 007, Loss: 0.6408, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 008, Loss: 0.6355, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 009, Loss: 0.6270, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 010, Loss: 0.6223, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 011, Loss: 0.6244, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 012, Loss: 0.6172, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.6110, Train: 0.6765, Test: 0.7222\n",
            "Epoch: 014, Loss: 0.6049, Train: 0.6941, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.6000, Train: 0.7059, Test: 0.7222\n",
            "Epoch: 016, Loss: 0.5957, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 017, Loss: 0.5912, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 018, Loss: 0.5889, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 019, Loss: 0.5873, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 020, Loss: 0.5818, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 021, Loss: 0.5804, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 022, Loss: 0.5742, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 023, Loss: 0.5694, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 024, Loss: 0.5656, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 025, Loss: 0.5627, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 026, Loss: 0.5578, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 027, Loss: 0.5526, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 028, Loss: 0.5462, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 029, Loss: 0.5439, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 030, Loss: 0.5391, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 031, Loss: 0.5395, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 032, Loss: 0.5325, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 033, Loss: 0.5297, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 034, Loss: 0.5170, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 035, Loss: 0.5160, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 036, Loss: 0.5076, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 037, Loss: 0.5089, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 038, Loss: 0.5096, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 039, Loss: 0.5027, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 040, Loss: 0.4937, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 041, Loss: 0.4931, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 042, Loss: 0.4843, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 043, Loss: 0.4837, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 044, Loss: 0.4761, Train: 0.8176, Test: 0.8333\n",
            "Epoch: 045, Loss: 0.4679, Train: 0.8294, Test: 0.8333\n",
            "Epoch: 046, Loss: 0.4702, Train: 0.8353, Test: 0.8333\n",
            "Epoch: 047, Loss: 0.4590, Train: 0.8353, Test: 0.8333\n",
            "Epoch: 048, Loss: 0.4600, Train: 0.8353, Test: 0.8333\n",
            "Epoch: 049, Loss: 0.4488, Train: 0.8353, Test: 0.8333\n",
            "Epoch: 050, Loss: 0.4485, Train: 0.8353, Test: 0.8333\n",
            "Epoch: 051, Loss: 0.4476, Train: 0.8529, Test: 0.8333\n",
            "Epoch: 052, Loss: 0.4395, Train: 0.8529, Test: 0.8333\n",
            "Epoch: 053, Loss: 0.4350, Train: 0.8529, Test: 0.8333\n",
            "Epoch: 054, Loss: 0.4340, Train: 0.8647, Test: 0.8333\n",
            "Epoch: 055, Loss: 0.4231, Train: 0.8647, Test: 0.8333\n",
            "Epoch: 056, Loss: 0.4197, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 057, Loss: 0.4206, Train: 0.8706, Test: 0.8333\n",
            "Epoch: 058, Loss: 0.4134, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 059, Loss: 0.4166, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 060, Loss: 0.3982, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 061, Loss: 0.3982, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 062, Loss: 0.3844, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 063, Loss: 0.3947, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 064, Loss: 0.3872, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 065, Loss: 0.3822, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 066, Loss: 0.3739, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 067, Loss: 0.3690, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 068, Loss: 0.3660, Train: 0.9000, Test: 0.8333\n",
            "Epoch: 069, Loss: 0.3612, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 070, Loss: 0.3530, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 071, Loss: 0.3600, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 072, Loss: 0.3485, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 073, Loss: 0.3448, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 074, Loss: 0.3455, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.3461, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.3339, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.3340, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.3235, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.3255, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 080, Loss: 0.3489, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 081, Loss: 0.3179, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 082, Loss: 0.3168, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 083, Loss: 0.3120, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 084, Loss: 0.3028, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 085, Loss: 0.3043, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 086, Loss: 0.3151, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 087, Loss: 0.2925, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2960, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2932, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2935, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.2902, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2802, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2785, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2849, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.2800, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.2724, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.2795, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.2722, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.2704, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.2683, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 101, Loss: 0.2688, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 102, Loss: 0.2621, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.2665, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.2523, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 105, Loss: 0.2486, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2473, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2465, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2395, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2436, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 110, Loss: 0.2474, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 111, Loss: 0.2408, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 112, Loss: 0.2396, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 113, Loss: 0.2328, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 114, Loss: 0.2332, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 115, Loss: 0.2282, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 116, Loss: 0.2258, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 117, Loss: 0.2286, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 118, Loss: 0.2278, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 119, Loss: 0.2330, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.2474, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 121, Loss: 0.2226, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 122, Loss: 0.2195, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.2259, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.2169, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.2163, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 126, Loss: 0.2151, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 127, Loss: 0.2227, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 128, Loss: 0.2197, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 129, Loss: 0.2162, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 130, Loss: 0.2053, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 131, Loss: 0.2075, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 132, Loss: 0.2055, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 133, Loss: 0.1966, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 134, Loss: 0.1980, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 135, Loss: 0.2074, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 136, Loss: 0.2071, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 137, Loss: 0.1883, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 138, Loss: 0.1922, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 139, Loss: 0.1942, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 140, Loss: 0.1913, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 141, Loss: 0.1927, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 142, Loss: 0.1959, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 143, Loss: 0.1851, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 144, Loss: 0.1851, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 145, Loss: 0.1864, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 146, Loss: 0.1944, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 147, Loss: 0.1943, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 148, Loss: 0.1740, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 149, Loss: 0.1802, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 150, Loss: 0.1882, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 151, Loss: 0.1875, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 152, Loss: 0.1830, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 153, Loss: 0.1740, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 154, Loss: 0.2007, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 155, Loss: 0.1816, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 156, Loss: 0.1732, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 157, Loss: 0.1762, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 158, Loss: 0.1696, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 159, Loss: 0.1748, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 160, Loss: 0.1848, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 161, Loss: 0.1656, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 162, Loss: 0.1684, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 163, Loss: 0.1670, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 164, Loss: 0.1705, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 165, Loss: 0.1769, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 166, Loss: 0.1756, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 167, Loss: 0.1569, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 168, Loss: 0.1537, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 169, Loss: 0.1684, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 170, Loss: 0.1667, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 171, Loss: 0.1491, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 172, Loss: 0.1672, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 173, Loss: 0.1524, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 174, Loss: 0.1844, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 175, Loss: 0.1584, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 176, Loss: 0.1568, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 177, Loss: 0.1621, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 178, Loss: 0.1535, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 179, Loss: 0.1554, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 180, Loss: 0.1530, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 181, Loss: 0.1461, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 182, Loss: 0.1596, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 183, Loss: 0.1506, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 184, Loss: 0.1604, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 185, Loss: 0.1410, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 186, Loss: 0.1583, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 187, Loss: 0.1505, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 188, Loss: 0.1557, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 189, Loss: 0.1485, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 190, Loss: 0.1420, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 191, Loss: 0.1537, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 192, Loss: 0.1474, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 193, Loss: 0.1451, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 194, Loss: 0.1396, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 195, Loss: 0.1388, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 196, Loss: 0.1501, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 197, Loss: 0.1319, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 198, Loss: 0.1367, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 199, Loss: 0.1360, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 200, Loss: 0.1399, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 201, Loss: 0.1397, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 202, Loss: 0.1396, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 203, Loss: 0.1270, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.1303, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.1368, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.1383, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 207, Loss: 0.1321, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 208, Loss: 0.1361, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 209, Loss: 0.1313, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 210, Loss: 0.1368, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 211, Loss: 0.1318, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 212, Loss: 0.1321, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 213, Loss: 0.1350, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 214, Loss: 0.1397, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 215, Loss: 0.1256, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.1311, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 217, Loss: 0.1386, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 218, Loss: 0.1365, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 219, Loss: 0.1276, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 220, Loss: 0.1134, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 221, Loss: 0.1302, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 222, Loss: 0.1323, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 223, Loss: 0.1227, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 224, Loss: 0.1298, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 225, Loss: 0.1303, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.1225, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 227, Loss: 0.1267, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 228, Loss: 0.1312, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 229, Loss: 0.1224, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 230, Loss: 0.1255, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 231, Loss: 0.1224, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 232, Loss: 0.1189, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.1220, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.1394, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.1204, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.1187, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 237, Loss: 0.1184, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 238, Loss: 0.1184, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 239, Loss: 0.1151, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 240, Loss: 0.1196, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 241, Loss: 0.1181, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 242, Loss: 0.1201, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 243, Loss: 0.1141, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 244, Loss: 0.1120, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 245, Loss: 0.1212, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 246, Loss: 0.1129, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 247, Loss: 0.1149, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 248, Loss: 0.1070, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 249, Loss: 0.1116, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 250, Loss: 0.1128, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 251, Loss: 0.1248, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 252, Loss: 0.1143, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 253, Loss: 0.1251, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 254, Loss: 0.1177, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 255, Loss: 0.1193, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 256, Loss: 0.1191, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.7090, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 002, Loss: 0.6890, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 003, Loss: 0.6741, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 004, Loss: 0.6652, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 005, Loss: 0.6537, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 006, Loss: 0.6498, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 007, Loss: 0.6427, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 008, Loss: 0.6360, Train: 0.3471, Test: 0.2222\n",
            "Epoch: 009, Loss: 0.6266, Train: 0.3529, Test: 0.2222\n",
            "Epoch: 010, Loss: 0.6223, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 011, Loss: 0.6142, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 012, Loss: 0.6059, Train: 0.8059, Test: 0.9444\n",
            "Epoch: 013, Loss: 0.6033, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.5896, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 015, Loss: 0.5885, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 016, Loss: 0.5817, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 017, Loss: 0.5789, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 018, Loss: 0.5733, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 019, Loss: 0.5712, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 020, Loss: 0.5658, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 021, Loss: 0.5631, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 022, Loss: 0.5619, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 023, Loss: 0.5578, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 024, Loss: 0.5469, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 025, Loss: 0.5461, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 026, Loss: 0.5382, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 027, Loss: 0.5382, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 028, Loss: 0.5344, Train: 0.7941, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.5315, Train: 0.7941, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.5271, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.5205, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.5166, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.5120, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.5047, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.5085, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.4987, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.5007, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.4905, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.4906, Train: 0.8059, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.4833, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.4807, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.4777, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.4712, Train: 0.8059, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.4694, Train: 0.8059, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.4618, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.4607, Train: 0.8353, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.4594, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.4484, Train: 0.8353, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.4490, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.4415, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.4367, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.4332, Train: 0.8412, Test: 0.9444\n",
            "Epoch: 053, Loss: 0.4278, Train: 0.8412, Test: 1.0000\n",
            "Epoch: 054, Loss: 0.4270, Train: 0.8353, Test: 1.0000\n",
            "Epoch: 055, Loss: 0.4184, Train: 0.8529, Test: 1.0000\n",
            "Epoch: 056, Loss: 0.4125, Train: 0.8588, Test: 1.0000\n",
            "Epoch: 057, Loss: 0.4102, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 058, Loss: 0.4128, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 059, Loss: 0.4077, Train: 0.8706, Test: 1.0000\n",
            "Epoch: 060, Loss: 0.3983, Train: 0.8706, Test: 1.0000\n",
            "Epoch: 061, Loss: 0.3967, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 062, Loss: 0.3935, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 063, Loss: 0.3844, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 064, Loss: 0.3802, Train: 0.8647, Test: 1.0000\n",
            "Epoch: 065, Loss: 0.3762, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 066, Loss: 0.3683, Train: 0.8706, Test: 1.0000\n",
            "Epoch: 067, Loss: 0.3702, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 068, Loss: 0.3699, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 069, Loss: 0.3616, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 070, Loss: 0.3662, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 071, Loss: 0.3590, Train: 0.8824, Test: 1.0000\n",
            "Epoch: 072, Loss: 0.3565, Train: 0.8882, Test: 1.0000\n",
            "Epoch: 073, Loss: 0.3485, Train: 0.8824, Test: 1.0000\n",
            "Epoch: 074, Loss: 0.3457, Train: 0.8765, Test: 1.0000\n",
            "Epoch: 075, Loss: 0.3387, Train: 0.8824, Test: 1.0000\n",
            "Epoch: 076, Loss: 0.3377, Train: 0.8824, Test: 1.0000\n",
            "Epoch: 077, Loss: 0.3329, Train: 0.8882, Test: 1.0000\n",
            "Epoch: 078, Loss: 0.3282, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 079, Loss: 0.3363, Train: 0.9000, Test: 1.0000\n",
            "Epoch: 080, Loss: 0.3173, Train: 0.8824, Test: 1.0000\n",
            "Epoch: 081, Loss: 0.3254, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 082, Loss: 0.3169, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 083, Loss: 0.3234, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 084, Loss: 0.3093, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 085, Loss: 0.3209, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 086, Loss: 0.3175, Train: 0.9000, Test: 0.9444\n",
            "Epoch: 087, Loss: 0.3110, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 088, Loss: 0.3025, Train: 0.8882, Test: 1.0000\n",
            "Epoch: 089, Loss: 0.2972, Train: 0.8941, Test: 1.0000\n",
            "Epoch: 090, Loss: 0.2967, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 091, Loss: 0.2975, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 092, Loss: 0.2859, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 093, Loss: 0.2795, Train: 0.9059, Test: 1.0000\n",
            "Epoch: 094, Loss: 0.2900, Train: 0.9059, Test: 1.0000\n",
            "Epoch: 095, Loss: 0.2867, Train: 0.9059, Test: 1.0000\n",
            "Epoch: 096, Loss: 0.2835, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 097, Loss: 0.2840, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 098, Loss: 0.2799, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 099, Loss: 0.2734, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 100, Loss: 0.2680, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 101, Loss: 0.2616, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 102, Loss: 0.2642, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 103, Loss: 0.2646, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 104, Loss: 0.2542, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 105, Loss: 0.2574, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 106, Loss: 0.2529, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 107, Loss: 0.2624, Train: 0.9118, Test: 1.0000\n",
            "Epoch: 108, Loss: 0.2495, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 109, Loss: 0.2491, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 110, Loss: 0.2444, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 111, Loss: 0.2505, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 112, Loss: 0.2406, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 113, Loss: 0.2446, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 114, Loss: 0.2367, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 115, Loss: 0.2313, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 116, Loss: 0.2405, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 117, Loss: 0.2288, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 118, Loss: 0.2348, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 119, Loss: 0.2310, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 120, Loss: 0.2365, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 121, Loss: 0.2336, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 122, Loss: 0.2345, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 123, Loss: 0.2266, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 124, Loss: 0.2185, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 125, Loss: 0.2267, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 126, Loss: 0.2137, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 127, Loss: 0.2189, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 128, Loss: 0.2219, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 129, Loss: 0.2138, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 130, Loss: 0.2105, Train: 0.9176, Test: 1.0000\n",
            "Epoch: 131, Loss: 0.2097, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 132, Loss: 0.2121, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 133, Loss: 0.2008, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 134, Loss: 0.2008, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 135, Loss: 0.2076, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 136, Loss: 0.2072, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 137, Loss: 0.2119, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 138, Loss: 0.2124, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 139, Loss: 0.2133, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 140, Loss: 0.1973, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 141, Loss: 0.1953, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 142, Loss: 0.1964, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 143, Loss: 0.2154, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 144, Loss: 0.1977, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 145, Loss: 0.1893, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 146, Loss: 0.1902, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 147, Loss: 0.1897, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 148, Loss: 0.1865, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 149, Loss: 0.1849, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 150, Loss: 0.1926, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 151, Loss: 0.1757, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 152, Loss: 0.1921, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 153, Loss: 0.1824, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 154, Loss: 0.1899, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 155, Loss: 0.1818, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 156, Loss: 0.1811, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 157, Loss: 0.1761, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 158, Loss: 0.2059, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 159, Loss: 0.1782, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 160, Loss: 0.1806, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 161, Loss: 0.1949, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 162, Loss: 0.1762, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 163, Loss: 0.1751, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 164, Loss: 0.1616, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 165, Loss: 0.1646, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 166, Loss: 0.1779, Train: 0.9235, Test: 1.0000\n",
            "Epoch: 167, Loss: 0.1715, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 168, Loss: 0.1678, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 169, Loss: 0.1657, Train: 0.9294, Test: 1.0000\n",
            "Epoch: 170, Loss: 0.1711, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 171, Loss: 0.1724, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 172, Loss: 0.1608, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 173, Loss: 0.1639, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 174, Loss: 0.1661, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 175, Loss: 0.1708, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 176, Loss: 0.1597, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 177, Loss: 0.1654, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 178, Loss: 0.1603, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 179, Loss: 0.1572, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 180, Loss: 0.1695, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 181, Loss: 0.1613, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 182, Loss: 0.1589, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 183, Loss: 0.1634, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 184, Loss: 0.1521, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 185, Loss: 0.1604, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 186, Loss: 0.1493, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 187, Loss: 0.1607, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 188, Loss: 0.1484, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 189, Loss: 0.1762, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 190, Loss: 0.1641, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 191, Loss: 0.1502, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 192, Loss: 0.1648, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 193, Loss: 0.1510, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 194, Loss: 0.1451, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 195, Loss: 0.1498, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 196, Loss: 0.1503, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 197, Loss: 0.1457, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 198, Loss: 0.1489, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 199, Loss: 0.1469, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 200, Loss: 0.1440, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 201, Loss: 0.1519, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 202, Loss: 0.1392, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 203, Loss: 0.1386, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.1385, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.1471, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.1458, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 207, Loss: 0.1388, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 208, Loss: 0.1395, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 209, Loss: 0.1437, Train: 0.9529, Test: 1.0000\n",
            "Epoch: 210, Loss: 0.1341, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 211, Loss: 0.1442, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 212, Loss: 0.1302, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 213, Loss: 0.1394, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 214, Loss: 0.1558, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 215, Loss: 0.1257, Train: 0.9588, Test: 1.0000\n",
            "Epoch: 216, Loss: 0.1493, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 217, Loss: 0.1343, Train: 0.9353, Test: 1.0000\n",
            "Epoch: 218, Loss: 0.1486, Train: 0.9588, Test: 1.0000\n",
            "Epoch: 219, Loss: 0.1401, Train: 0.9647, Test: 1.0000\n",
            "Epoch: 220, Loss: 0.1262, Train: 0.9529, Test: 1.0000\n",
            "Epoch: 221, Loss: 0.1339, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 222, Loss: 0.1392, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 223, Loss: 0.1318, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 224, Loss: 0.1349, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 225, Loss: 0.1218, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 226, Loss: 0.1377, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 227, Loss: 0.1265, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 228, Loss: 0.1364, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 229, Loss: 0.1171, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 230, Loss: 0.1319, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 231, Loss: 0.1321, Train: 0.9529, Test: 1.0000\n",
            "Epoch: 232, Loss: 0.1219, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.1183, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.1273, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.1246, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.1192, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 237, Loss: 0.1227, Train: 0.9471, Test: 1.0000\n",
            "Epoch: 238, Loss: 0.1163, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 239, Loss: 0.1255, Train: 0.9412, Test: 1.0000\n",
            "Epoch: 240, Loss: 0.1255, Train: 0.9647, Test: 1.0000\n",
            "Epoch: 241, Loss: 0.1356, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 242, Loss: 0.1117, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 243, Loss: 0.1174, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 244, Loss: 0.1134, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 245, Loss: 0.1253, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 246, Loss: 0.1170, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 247, Loss: 0.1186, Train: 0.9529, Test: 1.0000\n",
            "Epoch: 248, Loss: 0.1133, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 249, Loss: 0.1193, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 250, Loss: 0.1156, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 251, Loss: 0.1230, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.1203, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 253, Loss: 0.1133, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 254, Loss: 0.1175, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 255, Loss: 0.1123, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 256, Loss: 0.1122, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.6573, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 002, Loss: 0.6450, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 003, Loss: 0.6348, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 004, Loss: 0.6224, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 005, Loss: 0.6131, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 006, Loss: 0.6066, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 007, Loss: 0.5973, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 008, Loss: 0.5896, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 009, Loss: 0.5816, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 010, Loss: 0.5779, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 011, Loss: 0.5702, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 012, Loss: 0.5638, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 013, Loss: 0.5582, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 014, Loss: 0.5508, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 015, Loss: 0.5442, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 016, Loss: 0.5423, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.5382, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 018, Loss: 0.5290, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 019, Loss: 0.5273, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 020, Loss: 0.5198, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 021, Loss: 0.5146, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.5063, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.5029, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.4942, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.4981, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.4886, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 027, Loss: 0.4867, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 028, Loss: 0.4721, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 029, Loss: 0.4757, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 030, Loss: 0.4731, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 031, Loss: 0.4618, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 032, Loss: 0.4671, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 033, Loss: 0.4574, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 034, Loss: 0.4513, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 035, Loss: 0.4490, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 036, Loss: 0.4456, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 037, Loss: 0.4385, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 038, Loss: 0.4331, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 039, Loss: 0.4316, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 040, Loss: 0.4273, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 041, Loss: 0.4210, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 042, Loss: 0.4170, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 043, Loss: 0.4141, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 044, Loss: 0.4119, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 045, Loss: 0.4069, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 046, Loss: 0.4009, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 047, Loss: 0.4070, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 048, Loss: 0.3945, Train: 0.8353, Test: 0.7778\n",
            "Epoch: 049, Loss: 0.3914, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.3874, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.3850, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.3813, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.3686, Train: 0.8353, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.3796, Train: 0.8353, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.3671, Train: 0.8471, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.3719, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 057, Loss: 0.3648, Train: 0.8588, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.3713, Train: 0.8588, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.3537, Train: 0.8529, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.3647, Train: 0.8588, Test: 0.7222\n",
            "Epoch: 061, Loss: 0.3415, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.3458, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.3438, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.3359, Train: 0.8765, Test: 0.8889\n",
            "Epoch: 065, Loss: 0.3365, Train: 0.8824, Test: 0.8889\n",
            "Epoch: 066, Loss: 0.3294, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 067, Loss: 0.3383, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 068, Loss: 0.3377, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 069, Loss: 0.3354, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 070, Loss: 0.3212, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 071, Loss: 0.3097, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 072, Loss: 0.3089, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 073, Loss: 0.3104, Train: 0.9000, Test: 0.8333\n",
            "Epoch: 074, Loss: 0.3090, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.3064, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.2988, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.3024, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.2927, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.2817, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 080, Loss: 0.2827, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 081, Loss: 0.2813, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 082, Loss: 0.2809, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 083, Loss: 0.2749, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 084, Loss: 0.2778, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 085, Loss: 0.2740, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 086, Loss: 0.2824, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 087, Loss: 0.2686, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2689, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2669, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2631, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.2523, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2692, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2479, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2512, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.2518, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.2428, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.2428, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.2439, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.2340, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.2314, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 101, Loss: 0.2324, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 102, Loss: 0.2366, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.2331, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.2324, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.2325, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2351, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2278, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2250, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2256, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 110, Loss: 0.2190, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 111, Loss: 0.2186, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.2238, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 113, Loss: 0.2172, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.2123, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.2053, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 116, Loss: 0.2155, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.2167, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 118, Loss: 0.2036, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 119, Loss: 0.2106, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 120, Loss: 0.2028, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 121, Loss: 0.2089, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 122, Loss: 0.1969, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 123, Loss: 0.2039, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 124, Loss: 0.2006, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 125, Loss: 0.2014, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 126, Loss: 0.2077, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 127, Loss: 0.1950, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 128, Loss: 0.1917, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 129, Loss: 0.2138, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 130, Loss: 0.1895, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 131, Loss: 0.1911, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 132, Loss: 0.1909, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 133, Loss: 0.1779, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 134, Loss: 0.1885, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 135, Loss: 0.1828, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 136, Loss: 0.1836, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 137, Loss: 0.1748, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 138, Loss: 0.1790, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 139, Loss: 0.1834, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 140, Loss: 0.1760, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 141, Loss: 0.1812, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 142, Loss: 0.1821, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 143, Loss: 0.1794, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 144, Loss: 0.1796, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 145, Loss: 0.1703, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 146, Loss: 0.1747, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 147, Loss: 0.1692, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 148, Loss: 0.1773, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 149, Loss: 0.1835, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 150, Loss: 0.1656, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 151, Loss: 0.1618, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 152, Loss: 0.1889, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 153, Loss: 0.1631, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 154, Loss: 0.1744, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 155, Loss: 0.1599, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 156, Loss: 0.1654, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 157, Loss: 0.1596, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 158, Loss: 0.1510, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 159, Loss: 0.1485, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 160, Loss: 0.1638, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 161, Loss: 0.1575, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 162, Loss: 0.1582, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 163, Loss: 0.1514, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 164, Loss: 0.1550, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 165, Loss: 0.1622, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 166, Loss: 0.1528, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 167, Loss: 0.1472, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 168, Loss: 0.1450, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 169, Loss: 0.1506, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 170, Loss: 0.1525, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 171, Loss: 0.1532, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 172, Loss: 0.1438, Train: 0.9588, Test: 0.7222\n",
            "Epoch: 173, Loss: 0.1417, Train: 0.9588, Test: 0.7222\n",
            "Epoch: 174, Loss: 0.1556, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 175, Loss: 0.1453, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 176, Loss: 0.1473, Train: 0.9588, Test: 0.7222\n",
            "Epoch: 177, Loss: 0.1466, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 178, Loss: 0.1498, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 179, Loss: 0.1439, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 180, Loss: 0.1406, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 181, Loss: 0.1445, Train: 0.9588, Test: 0.7222\n",
            "Epoch: 182, Loss: 0.1328, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 183, Loss: 0.1503, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 184, Loss: 0.1309, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 185, Loss: 0.1497, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 186, Loss: 0.1386, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 187, Loss: 0.1387, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 188, Loss: 0.1370, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 189, Loss: 0.1462, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 190, Loss: 0.1340, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 191, Loss: 0.1392, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 192, Loss: 0.1352, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 193, Loss: 0.1242, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 194, Loss: 0.1355, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 195, Loss: 0.1293, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 196, Loss: 0.1226, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 197, Loss: 0.1249, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 198, Loss: 0.1278, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 199, Loss: 0.1151, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 200, Loss: 0.1388, Train: 0.9647, Test: 0.6667\n",
            "Epoch: 201, Loss: 0.1216, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 202, Loss: 0.1246, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 203, Loss: 0.1228, Train: 0.9529, Test: 0.7222\n",
            "Epoch: 204, Loss: 0.1441, Train: 0.9588, Test: 0.7222\n",
            "Epoch: 205, Loss: 0.1270, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 206, Loss: 0.1223, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 207, Loss: 0.1219, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 208, Loss: 0.1152, Train: 0.9471, Test: 0.6667\n",
            "Epoch: 209, Loss: 0.1391, Train: 0.9529, Test: 0.6667\n",
            "Epoch: 210, Loss: 0.1242, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 211, Loss: 0.1200, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 212, Loss: 0.1257, Train: 0.9647, Test: 0.6667\n",
            "Epoch: 213, Loss: 0.1304, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 214, Loss: 0.1109, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 215, Loss: 0.1322, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 216, Loss: 0.1049, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 217, Loss: 0.1240, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 218, Loss: 0.1196, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 219, Loss: 0.1229, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 220, Loss: 0.1242, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 221, Loss: 0.1203, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 222, Loss: 0.1192, Train: 0.9647, Test: 0.7778\n",
            "Epoch: 223, Loss: 0.1070, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 224, Loss: 0.1063, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 225, Loss: 0.1134, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 226, Loss: 0.1112, Train: 0.9647, Test: 0.6667\n",
            "Epoch: 227, Loss: 0.1344, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 228, Loss: 0.1143, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 229, Loss: 0.1055, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 230, Loss: 0.1246, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 231, Loss: 0.1125, Train: 0.9647, Test: 0.7778\n",
            "Epoch: 232, Loss: 0.1045, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 233, Loss: 0.1087, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 234, Loss: 0.1151, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 235, Loss: 0.1053, Train: 0.9588, Test: 0.6667\n",
            "Epoch: 236, Loss: 0.1000, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 237, Loss: 0.1021, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 238, Loss: 0.1021, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 239, Loss: 0.0997, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 240, Loss: 0.1075, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 241, Loss: 0.0999, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 242, Loss: 0.0993, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 243, Loss: 0.1015, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 244, Loss: 0.1038, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 245, Loss: 0.0972, Train: 0.9647, Test: 0.6667\n",
            "Epoch: 246, Loss: 0.1050, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 247, Loss: 0.1015, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 248, Loss: 0.1081, Train: 0.9647, Test: 0.7222\n",
            "Epoch: 249, Loss: 0.1055, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 250, Loss: 0.0966, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 251, Loss: 0.1116, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.1029, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 253, Loss: 0.0938, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 254, Loss: 0.0984, Train: 0.9706, Test: 0.7222\n",
            "Epoch: 255, Loss: 0.1062, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.1004, Train: 0.9706, Test: 0.6667\n",
            "Epoch: 001, Loss: 0.6879, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 002, Loss: 0.6736, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 003, Loss: 0.6597, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 004, Loss: 0.6449, Train: 0.6941, Test: 0.5000\n",
            "Epoch: 005, Loss: 0.6287, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 006, Loss: 0.6130, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 007, Loss: 0.6035, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 008, Loss: 0.5962, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 009, Loss: 0.5902, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 010, Loss: 0.5847, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 011, Loss: 0.5775, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 012, Loss: 0.5684, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 013, Loss: 0.5583, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 014, Loss: 0.5570, Train: 0.7118, Test: 0.5000\n",
            "Epoch: 015, Loss: 0.5409, Train: 0.7176, Test: 0.5000\n",
            "Epoch: 016, Loss: 0.5411, Train: 0.7176, Test: 0.5556\n",
            "Epoch: 017, Loss: 0.5341, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 018, Loss: 0.5350, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 019, Loss: 0.5278, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 020, Loss: 0.5262, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 021, Loss: 0.5202, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 022, Loss: 0.5154, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 023, Loss: 0.5087, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 024, Loss: 0.5070, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 025, Loss: 0.5027, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.4930, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.4919, Train: 0.7647, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.4856, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.4837, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 030, Loss: 0.4867, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 031, Loss: 0.4784, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 032, Loss: 0.4702, Train: 0.7824, Test: 0.7222\n",
            "Epoch: 033, Loss: 0.4601, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 034, Loss: 0.4660, Train: 0.7824, Test: 0.7222\n",
            "Epoch: 035, Loss: 0.4559, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 036, Loss: 0.4540, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 037, Loss: 0.4494, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.4460, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.4433, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.4373, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.4339, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.4235, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.4242, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.4223, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.4264, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 046, Loss: 0.4189, Train: 0.8412, Test: 0.7222\n",
            "Epoch: 047, Loss: 0.4182, Train: 0.8412, Test: 0.7222\n",
            "Epoch: 048, Loss: 0.4016, Train: 0.8412, Test: 0.7222\n",
            "Epoch: 049, Loss: 0.4053, Train: 0.8412, Test: 0.7222\n",
            "Epoch: 050, Loss: 0.3982, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 051, Loss: 0.3985, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 052, Loss: 0.3940, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 053, Loss: 0.3965, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 054, Loss: 0.3970, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 055, Loss: 0.3820, Train: 0.8471, Test: 0.7222\n",
            "Epoch: 056, Loss: 0.3935, Train: 0.8412, Test: 0.7222\n",
            "Epoch: 057, Loss: 0.3820, Train: 0.8529, Test: 0.7222\n",
            "Epoch: 058, Loss: 0.3735, Train: 0.8529, Test: 0.7222\n",
            "Epoch: 059, Loss: 0.3669, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 060, Loss: 0.3742, Train: 0.8706, Test: 0.7222\n",
            "Epoch: 061, Loss: 0.3678, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 062, Loss: 0.3645, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 063, Loss: 0.3705, Train: 0.8765, Test: 0.7222\n",
            "Epoch: 064, Loss: 0.3487, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.3507, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.3558, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.3495, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 068, Loss: 0.3530, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 069, Loss: 0.3408, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 070, Loss: 0.3351, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 071, Loss: 0.3381, Train: 0.8706, Test: 0.8333\n",
            "Epoch: 072, Loss: 0.3335, Train: 0.8706, Test: 0.8333\n",
            "Epoch: 073, Loss: 0.3289, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 074, Loss: 0.3225, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.3363, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.3349, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.3193, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.3234, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.3166, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 080, Loss: 0.3146, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 081, Loss: 0.3139, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 082, Loss: 0.3015, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 083, Loss: 0.3049, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 084, Loss: 0.3007, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 085, Loss: 0.2949, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 086, Loss: 0.2986, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 087, Loss: 0.3016, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2848, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2972, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2733, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.2858, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2839, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2820, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2745, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 095, Loss: 0.2734, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 096, Loss: 0.2707, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 097, Loss: 0.2842, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 098, Loss: 0.2546, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 099, Loss: 0.2626, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 100, Loss: 0.2534, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 101, Loss: 0.2688, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 102, Loss: 0.2515, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 103, Loss: 0.2466, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 104, Loss: 0.2597, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 105, Loss: 0.2389, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 106, Loss: 0.2600, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 107, Loss: 0.2481, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 108, Loss: 0.2411, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 109, Loss: 0.2475, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 110, Loss: 0.2524, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 111, Loss: 0.2427, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 112, Loss: 0.2292, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 113, Loss: 0.2286, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 114, Loss: 0.2302, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 115, Loss: 0.2241, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 116, Loss: 0.2298, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 117, Loss: 0.2175, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 118, Loss: 0.2251, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 119, Loss: 0.2203, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.2172, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 121, Loss: 0.2287, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 122, Loss: 0.2074, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.2321, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.2097, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.2171, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 126, Loss: 0.2105, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 127, Loss: 0.2019, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 128, Loss: 0.2020, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 129, Loss: 0.1974, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 130, Loss: 0.2039, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 131, Loss: 0.1981, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 132, Loss: 0.1990, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 133, Loss: 0.2013, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 134, Loss: 0.2078, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 135, Loss: 0.2001, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 136, Loss: 0.1966, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 137, Loss: 0.1965, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 138, Loss: 0.1871, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 139, Loss: 0.1950, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 140, Loss: 0.1904, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 141, Loss: 0.1865, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 142, Loss: 0.1833, Train: 0.9176, Test: 0.8889\n",
            "Epoch: 143, Loss: 0.1787, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 144, Loss: 0.1818, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 145, Loss: 0.1767, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 146, Loss: 0.1812, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 147, Loss: 0.1794, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 148, Loss: 0.1662, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 149, Loss: 0.1649, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 150, Loss: 0.1916, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 151, Loss: 0.1852, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 152, Loss: 0.1742, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 153, Loss: 0.1824, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 154, Loss: 0.1750, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 155, Loss: 0.1695, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 156, Loss: 0.1659, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 157, Loss: 0.1736, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 158, Loss: 0.1743, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 159, Loss: 0.1657, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 160, Loss: 0.1739, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 161, Loss: 0.1674, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 162, Loss: 0.1650, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 163, Loss: 0.1704, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 164, Loss: 0.1669, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 165, Loss: 0.1575, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 166, Loss: 0.1685, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 167, Loss: 0.1645, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 168, Loss: 0.1575, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 169, Loss: 0.1739, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 170, Loss: 0.1568, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 171, Loss: 0.1652, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 172, Loss: 0.1728, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 173, Loss: 0.1600, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 174, Loss: 0.1574, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 175, Loss: 0.1547, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 176, Loss: 0.1494, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 177, Loss: 0.1487, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 178, Loss: 0.1625, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 179, Loss: 0.1620, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 180, Loss: 0.1401, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 181, Loss: 0.1647, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 182, Loss: 0.1522, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 183, Loss: 0.1505, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.1501, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 185, Loss: 0.1518, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 186, Loss: 0.1475, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 187, Loss: 0.1504, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 188, Loss: 0.1457, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 189, Loss: 0.1435, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 190, Loss: 0.1359, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 191, Loss: 0.1360, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 192, Loss: 0.1547, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 193, Loss: 0.1381, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 194, Loss: 0.1390, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 195, Loss: 0.1410, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 196, Loss: 0.1379, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 197, Loss: 0.1437, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 198, Loss: 0.1557, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 199, Loss: 0.1364, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 200, Loss: 0.1270, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 201, Loss: 0.1393, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 202, Loss: 0.1383, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 203, Loss: 0.1318, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.1378, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.1406, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 206, Loss: 0.1247, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 207, Loss: 0.1372, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 208, Loss: 0.1355, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 209, Loss: 0.1311, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 210, Loss: 0.1459, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 211, Loss: 0.1373, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 212, Loss: 0.1356, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 213, Loss: 0.1336, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 214, Loss: 0.1331, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 215, Loss: 0.1259, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.1399, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 217, Loss: 0.1279, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 218, Loss: 0.1333, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 219, Loss: 0.1324, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 220, Loss: 0.1300, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 221, Loss: 0.1574, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 222, Loss: 0.1181, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 223, Loss: 0.1219, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 224, Loss: 0.1148, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 225, Loss: 0.1288, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.1198, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 227, Loss: 0.1315, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 228, Loss: 0.1152, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 229, Loss: 0.1166, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 230, Loss: 0.1228, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 231, Loss: 0.1185, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 232, Loss: 0.1101, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 233, Loss: 0.1126, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 234, Loss: 0.1267, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 235, Loss: 0.1224, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 236, Loss: 0.1149, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 237, Loss: 0.1112, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 238, Loss: 0.1233, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 239, Loss: 0.1150, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 240, Loss: 0.1494, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 241, Loss: 0.1207, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.1209, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 243, Loss: 0.1207, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 244, Loss: 0.1044, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 245, Loss: 0.1199, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 246, Loss: 0.1062, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 247, Loss: 0.1193, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 248, Loss: 0.1137, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 249, Loss: 0.1049, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 250, Loss: 0.1119, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 251, Loss: 0.1207, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.1096, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 253, Loss: 0.1108, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 254, Loss: 0.1145, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 255, Loss: 0.1169, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 256, Loss: 0.1109, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 001, Loss: 0.6975, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 002, Loss: 0.6885, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 003, Loss: 0.6713, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 004, Loss: 0.6608, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 005, Loss: 0.6490, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 006, Loss: 0.6435, Train: 0.4588, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.6378, Train: 0.6882, Test: 0.7778\n",
            "Epoch: 008, Loss: 0.6325, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 009, Loss: 0.6253, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.6152, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 011, Loss: 0.6078, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 012, Loss: 0.6032, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.5977, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 014, Loss: 0.5958, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.5884, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 016, Loss: 0.5856, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 017, Loss: 0.5767, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 018, Loss: 0.5712, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 019, Loss: 0.5668, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 020, Loss: 0.5620, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 021, Loss: 0.5554, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 022, Loss: 0.5498, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 023, Loss: 0.5406, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 024, Loss: 0.5418, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 025, Loss: 0.5367, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 026, Loss: 0.5278, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 027, Loss: 0.5254, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 028, Loss: 0.5249, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 029, Loss: 0.5174, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 030, Loss: 0.5132, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 031, Loss: 0.5120, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 032, Loss: 0.4991, Train: 0.8000, Test: 0.7778\n",
            "Epoch: 033, Loss: 0.5018, Train: 0.7882, Test: 0.7778\n",
            "Epoch: 034, Loss: 0.4915, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 035, Loss: 0.4928, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 036, Loss: 0.4824, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 037, Loss: 0.4805, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 038, Loss: 0.4822, Train: 0.7941, Test: 0.7778\n",
            "Epoch: 039, Loss: 0.4629, Train: 0.8000, Test: 0.7778\n",
            "Epoch: 040, Loss: 0.4672, Train: 0.8000, Test: 0.7778\n",
            "Epoch: 041, Loss: 0.4643, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 042, Loss: 0.4633, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 043, Loss: 0.4635, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 044, Loss: 0.4535, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 045, Loss: 0.4485, Train: 0.8118, Test: 0.7778\n",
            "Epoch: 046, Loss: 0.4443, Train: 0.8176, Test: 0.7778\n",
            "Epoch: 047, Loss: 0.4399, Train: 0.8176, Test: 0.7778\n",
            "Epoch: 048, Loss: 0.4374, Train: 0.8176, Test: 0.7778\n",
            "Epoch: 049, Loss: 0.4292, Train: 0.8176, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.4326, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.4237, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.4160, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.4205, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.4136, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.4122, Train: 0.8235, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.4088, Train: 0.8294, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.4070, Train: 0.8412, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.3964, Train: 0.8471, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.3948, Train: 0.8471, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.3893, Train: 0.8471, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.3890, Train: 0.8529, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.3834, Train: 0.8588, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.3828, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.3781, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.3781, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.3747, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.3651, Train: 0.8706, Test: 0.7222\n",
            "Epoch: 068, Loss: 0.3666, Train: 0.8706, Test: 0.7222\n",
            "Epoch: 069, Loss: 0.3640, Train: 0.8706, Test: 0.7222\n",
            "Epoch: 070, Loss: 0.3671, Train: 0.8706, Test: 0.7222\n",
            "Epoch: 071, Loss: 0.3698, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.3597, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.3552, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.3550, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 075, Loss: 0.3466, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 076, Loss: 0.3518, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 077, Loss: 0.3501, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 078, Loss: 0.3401, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 079, Loss: 0.3476, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 080, Loss: 0.3315, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 081, Loss: 0.3289, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 082, Loss: 0.3255, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 083, Loss: 0.3288, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 084, Loss: 0.3230, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 085, Loss: 0.3257, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 086, Loss: 0.3156, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 087, Loss: 0.3175, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 088, Loss: 0.3173, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 089, Loss: 0.3138, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 090, Loss: 0.3110, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 091, Loss: 0.3085, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 092, Loss: 0.3289, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 093, Loss: 0.3091, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 094, Loss: 0.3040, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 095, Loss: 0.2901, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 096, Loss: 0.2961, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 097, Loss: 0.2895, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 098, Loss: 0.2891, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 099, Loss: 0.2833, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 100, Loss: 0.2926, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 101, Loss: 0.2783, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 102, Loss: 0.2819, Train: 0.9000, Test: 0.7778\n",
            "Epoch: 103, Loss: 0.2787, Train: 0.9059, Test: 0.7778\n",
            "Epoch: 104, Loss: 0.2869, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.2757, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2619, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2769, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2632, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2602, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 110, Loss: 0.2614, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 111, Loss: 0.2591, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.2646, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 113, Loss: 0.2514, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 114, Loss: 0.2460, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.2480, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 116, Loss: 0.2606, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.2488, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.2487, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 119, Loss: 0.2369, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 120, Loss: 0.2458, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 121, Loss: 0.2396, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 122, Loss: 0.2368, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 123, Loss: 0.2424, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 124, Loss: 0.2297, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 125, Loss: 0.2424, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 126, Loss: 0.2347, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 127, Loss: 0.2252, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 128, Loss: 0.2285, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 129, Loss: 0.2222, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 130, Loss: 0.2159, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 131, Loss: 0.2225, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 132, Loss: 0.2142, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 133, Loss: 0.2189, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 134, Loss: 0.2191, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 135, Loss: 0.2325, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 136, Loss: 0.2101, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 137, Loss: 0.2125, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 138, Loss: 0.2098, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 139, Loss: 0.2094, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.2102, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.2082, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.2000, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 143, Loss: 0.2231, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 144, Loss: 0.2102, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 145, Loss: 0.2000, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 146, Loss: 0.1976, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 147, Loss: 0.1964, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.2056, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.1932, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 150, Loss: 0.1917, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 151, Loss: 0.1973, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 152, Loss: 0.1923, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 153, Loss: 0.1904, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 154, Loss: 0.1943, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 155, Loss: 0.2009, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 156, Loss: 0.1901, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 157, Loss: 0.1832, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.1846, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 159, Loss: 0.1876, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 160, Loss: 0.1862, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 161, Loss: 0.1761, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 162, Loss: 0.1841, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.1731, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.1726, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.1695, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 166, Loss: 0.1759, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 167, Loss: 0.1819, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 168, Loss: 0.1780, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 169, Loss: 0.1821, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 170, Loss: 0.1845, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 171, Loss: 0.1758, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 172, Loss: 0.1822, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 173, Loss: 0.1846, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 174, Loss: 0.1672, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 175, Loss: 0.1722, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.1632, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.1848, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.1552, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 179, Loss: 0.1708, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 180, Loss: 0.1785, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 181, Loss: 0.1591, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 182, Loss: 0.1664, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 183, Loss: 0.1577, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.1533, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 185, Loss: 0.1600, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 186, Loss: 0.1649, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 187, Loss: 0.1610, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.1587, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.1652, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.1465, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 191, Loss: 0.1626, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 192, Loss: 0.1623, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 193, Loss: 0.1516, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 194, Loss: 0.1527, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 195, Loss: 0.1486, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.1523, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.1509, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.1632, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.1514, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.1704, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.1423, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 202, Loss: 0.1444, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 203, Loss: 0.1460, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 204, Loss: 0.1425, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 205, Loss: 0.1492, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 206, Loss: 0.1418, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 207, Loss: 0.1449, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 208, Loss: 0.1488, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.1473, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.1526, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 211, Loss: 0.1469, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 212, Loss: 0.1432, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.1742, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.1399, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.1585, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.1457, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 217, Loss: 0.1419, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 218, Loss: 0.1449, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 219, Loss: 0.1270, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.1297, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.1289, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 222, Loss: 0.1393, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.1537, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.1392, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 225, Loss: 0.1332, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.1314, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.1347, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.1395, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.1245, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 230, Loss: 0.1276, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 231, Loss: 0.1300, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 232, Loss: 0.1313, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.1426, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.1388, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 235, Loss: 0.1361, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 236, Loss: 0.1309, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.1281, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.1392, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.1280, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 240, Loss: 0.1303, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 241, Loss: 0.1338, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 242, Loss: 0.1240, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.1295, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.1334, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 245, Loss: 0.1286, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 246, Loss: 0.1321, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 247, Loss: 0.1362, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 248, Loss: 0.1235, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.1303, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.1256, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.1245, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 252, Loss: 0.1458, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 253, Loss: 0.1273, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 254, Loss: 0.1190, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 255, Loss: 0.1242, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 256, Loss: 0.1192, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.6614, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 002, Loss: 0.6422, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.6344, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6255, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6192, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6108, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6008, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.5986, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.5947, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.5869, Train: 0.6353, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.5834, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.5776, Train: 0.7235, Test: 0.8333\n",
            "Epoch: 013, Loss: 0.5741, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 014, Loss: 0.5699, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 015, Loss: 0.5651, Train: 0.7235, Test: 0.8333\n",
            "Epoch: 016, Loss: 0.5586, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 017, Loss: 0.5596, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 018, Loss: 0.5496, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 019, Loss: 0.5458, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 020, Loss: 0.5428, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 021, Loss: 0.5430, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 022, Loss: 0.5447, Train: 0.7294, Test: 0.8333\n",
            "Epoch: 023, Loss: 0.5320, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 024, Loss: 0.5294, Train: 0.7353, Test: 0.8333\n",
            "Epoch: 025, Loss: 0.5205, Train: 0.7412, Test: 0.8333\n",
            "Epoch: 026, Loss: 0.5206, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 027, Loss: 0.5192, Train: 0.7471, Test: 0.8889\n",
            "Epoch: 028, Loss: 0.5131, Train: 0.7471, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.5126, Train: 0.7529, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.5109, Train: 0.7588, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.5057, Train: 0.7588, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.5022, Train: 0.7647, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.4936, Train: 0.7706, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.4850, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.4818, Train: 0.7882, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.4702, Train: 0.7941, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.4731, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.4660, Train: 0.8000, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.4659, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.4612, Train: 0.8235, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.4630, Train: 0.8235, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.4479, Train: 0.8235, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.4456, Train: 0.8235, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.4386, Train: 0.8235, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.4385, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.4393, Train: 0.8353, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.4339, Train: 0.8353, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.4280, Train: 0.8412, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.4231, Train: 0.8412, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.4190, Train: 0.8471, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.4146, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.4088, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 053, Loss: 0.4030, Train: 0.8588, Test: 0.8889\n",
            "Epoch: 054, Loss: 0.3980, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 055, Loss: 0.3994, Train: 0.8647, Test: 0.8889\n",
            "Epoch: 056, Loss: 0.3963, Train: 0.8588, Test: 0.9444\n",
            "Epoch: 057, Loss: 0.3945, Train: 0.8588, Test: 0.9444\n",
            "Epoch: 058, Loss: 0.3834, Train: 0.8588, Test: 0.9444\n",
            "Epoch: 059, Loss: 0.3833, Train: 0.8706, Test: 0.9444\n",
            "Epoch: 060, Loss: 0.3781, Train: 0.8706, Test: 0.9444\n",
            "Epoch: 061, Loss: 0.3825, Train: 0.8765, Test: 0.9444\n",
            "Epoch: 062, Loss: 0.3806, Train: 0.8706, Test: 0.9444\n",
            "Epoch: 063, Loss: 0.3656, Train: 0.8765, Test: 0.9444\n",
            "Epoch: 064, Loss: 0.3654, Train: 0.8765, Test: 0.9444\n",
            "Epoch: 065, Loss: 0.3550, Train: 0.8765, Test: 0.9444\n",
            "Epoch: 066, Loss: 0.3520, Train: 0.8765, Test: 0.9444\n",
            "Epoch: 067, Loss: 0.3427, Train: 0.8882, Test: 0.9444\n",
            "Epoch: 068, Loss: 0.3534, Train: 0.8941, Test: 0.9444\n",
            "Epoch: 069, Loss: 0.3443, Train: 0.8882, Test: 0.9444\n",
            "Epoch: 070, Loss: 0.3336, Train: 0.8882, Test: 0.9444\n",
            "Epoch: 071, Loss: 0.3324, Train: 0.8941, Test: 0.9444\n",
            "Epoch: 072, Loss: 0.3377, Train: 0.8882, Test: 0.9444\n",
            "Epoch: 073, Loss: 0.3254, Train: 0.9059, Test: 0.9444\n",
            "Epoch: 074, Loss: 0.3154, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 075, Loss: 0.3120, Train: 0.9000, Test: 0.9444\n",
            "Epoch: 076, Loss: 0.3152, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 077, Loss: 0.3060, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 078, Loss: 0.3064, Train: 0.9118, Test: 0.8889\n",
            "Epoch: 079, Loss: 0.3025, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 080, Loss: 0.3000, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 081, Loss: 0.2926, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 082, Loss: 0.2911, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 083, Loss: 0.2918, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 084, Loss: 0.2823, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 085, Loss: 0.2785, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 086, Loss: 0.2744, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 087, Loss: 0.2775, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2619, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2733, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2660, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 091, Loss: 0.2527, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2510, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2544, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2502, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.2491, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.2441, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.2339, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.2458, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.2347, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.2333, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 101, Loss: 0.2313, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 102, Loss: 0.2372, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.2262, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.2278, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.2209, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2282, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2270, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2210, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2167, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 110, Loss: 0.2143, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 111, Loss: 0.2214, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.2192, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 113, Loss: 0.2027, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 114, Loss: 0.2023, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.2008, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 116, Loss: 0.2051, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.2087, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.2057, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 119, Loss: 0.1983, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 120, Loss: 0.1938, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 121, Loss: 0.1893, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 122, Loss: 0.2040, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 123, Loss: 0.1950, Train: 0.9235, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.1889, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.1832, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 126, Loss: 0.1937, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 127, Loss: 0.1794, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 128, Loss: 0.1801, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 129, Loss: 0.1786, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 130, Loss: 0.1753, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 131, Loss: 0.1882, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 132, Loss: 0.1779, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 133, Loss: 0.1745, Train: 0.9294, Test: 0.8889\n",
            "Epoch: 134, Loss: 0.1744, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 135, Loss: 0.1725, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 136, Loss: 0.1632, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 137, Loss: 0.1660, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 138, Loss: 0.1709, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 139, Loss: 0.1603, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.1584, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.1633, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.1601, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 143, Loss: 0.1698, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 144, Loss: 0.1583, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 145, Loss: 0.1517, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 146, Loss: 0.1617, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 147, Loss: 0.1496, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.1516, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.1548, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 150, Loss: 0.1557, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 151, Loss: 0.1559, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 152, Loss: 0.1449, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 153, Loss: 0.1495, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 154, Loss: 0.1527, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 155, Loss: 0.1443, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 156, Loss: 0.1550, Train: 0.9353, Test: 0.8889\n",
            "Epoch: 157, Loss: 0.1429, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.1371, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 159, Loss: 0.1610, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 160, Loss: 0.1623, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 161, Loss: 0.1612, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 162, Loss: 0.1382, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.1376, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.1425, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.1504, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 166, Loss: 0.1442, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 167, Loss: 0.1374, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 168, Loss: 0.1428, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 169, Loss: 0.1346, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 170, Loss: 0.1414, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 171, Loss: 0.1299, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 172, Loss: 0.1487, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 173, Loss: 0.1390, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 174, Loss: 0.1309, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 175, Loss: 0.1418, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.1306, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.1289, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.1367, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 179, Loss: 0.1361, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 180, Loss: 0.1362, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 181, Loss: 0.1292, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 182, Loss: 0.1264, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 183, Loss: 0.1405, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.1323, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 185, Loss: 0.1245, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 186, Loss: 0.1320, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 187, Loss: 0.1338, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.1199, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.1241, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.1218, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 191, Loss: 0.1290, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 192, Loss: 0.1230, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 193, Loss: 0.1185, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 194, Loss: 0.1316, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 195, Loss: 0.1153, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.1232, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.1211, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 198, Loss: 0.1189, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 199, Loss: 0.1241, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 200, Loss: 0.1207, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 201, Loss: 0.1202, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 202, Loss: 0.1247, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 203, Loss: 0.1215, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 204, Loss: 0.1133, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 205, Loss: 0.1149, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.1191, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.1127, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.1161, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 209, Loss: 0.1148, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 210, Loss: 0.1153, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.1053, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.1148, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.1096, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 214, Loss: 0.1202, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 215, Loss: 0.1134, Train: 0.9529, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.1159, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 217, Loss: 0.1134, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 218, Loss: 0.1079, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 219, Loss: 0.1156, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.1097, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.1186, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.1148, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.1030, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.1033, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 225, Loss: 0.1017, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.1038, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 227, Loss: 0.1145, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.1027, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.1055, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.1086, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.1020, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.1035, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.1021, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.0951, Train: 0.9588, Test: 0.8889\n",
            "Epoch: 235, Loss: 0.1160, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 236, Loss: 0.1089, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.1006, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.1004, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.1026, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.0952, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.1114, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.1003, Train: 0.9647, Test: 0.8889\n",
            "Epoch: 243, Loss: 0.1003, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 244, Loss: 0.1004, Train: 0.9765, Test: 0.8889\n",
            "Epoch: 245, Loss: 0.0962, Train: 0.9765, Test: 0.8889\n",
            "Epoch: 246, Loss: 0.0985, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 247, Loss: 0.0936, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 248, Loss: 0.0996, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.0965, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.0928, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 251, Loss: 0.1027, Train: 0.9765, Test: 0.8889\n",
            "Epoch: 252, Loss: 0.0958, Train: 0.9706, Test: 0.8889\n",
            "Epoch: 253, Loss: 0.0922, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.0987, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.0935, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.1005, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.6935, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 002, Loss: 0.6704, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.6544, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6471, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6392, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6264, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6185, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.6088, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.6020, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.5979, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.5927, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.5823, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 013, Loss: 0.5735, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.5705, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 015, Loss: 0.5591, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 016, Loss: 0.5611, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 017, Loss: 0.5566, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 018, Loss: 0.5460, Train: 0.7118, Test: 0.9444\n",
            "Epoch: 019, Loss: 0.5468, Train: 0.7118, Test: 0.9444\n",
            "Epoch: 020, Loss: 0.5440, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 021, Loss: 0.5337, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 022, Loss: 0.5307, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 023, Loss: 0.5266, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 024, Loss: 0.5151, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 025, Loss: 0.5121, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 026, Loss: 0.5076, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 027, Loss: 0.4974, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 028, Loss: 0.4939, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 029, Loss: 0.4934, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 030, Loss: 0.4860, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 031, Loss: 0.4801, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 032, Loss: 0.4760, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 033, Loss: 0.4679, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 034, Loss: 0.4689, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 035, Loss: 0.4583, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 036, Loss: 0.4508, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 037, Loss: 0.4677, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 038, Loss: 0.4412, Train: 0.8000, Test: 1.0000\n",
            "Epoch: 039, Loss: 0.4380, Train: 0.8235, Test: 1.0000\n",
            "Epoch: 040, Loss: 0.4367, Train: 0.8294, Test: 1.0000\n",
            "Epoch: 041, Loss: 0.4289, Train: 0.8353, Test: 1.0000\n",
            "Epoch: 042, Loss: 0.4303, Train: 0.8412, Test: 1.0000\n",
            "Epoch: 043, Loss: 0.4205, Train: 0.8412, Test: 0.9444\n",
            "Epoch: 044, Loss: 0.4209, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.4115, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.4149, Train: 0.8412, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.4034, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.3957, Train: 0.8529, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.3985, Train: 0.8588, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.3948, Train: 0.8588, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.3896, Train: 0.8647, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.3809, Train: 0.8765, Test: 0.8889\n",
            "Epoch: 053, Loss: 0.3753, Train: 0.8824, Test: 0.8889\n",
            "Epoch: 054, Loss: 0.3753, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 055, Loss: 0.3693, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 056, Loss: 0.3699, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 057, Loss: 0.3565, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 058, Loss: 0.3472, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 059, Loss: 0.3524, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 060, Loss: 0.3522, Train: 0.9059, Test: 0.8889\n",
            "Epoch: 061, Loss: 0.3511, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 062, Loss: 0.3456, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 063, Loss: 0.3286, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 064, Loss: 0.3371, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 065, Loss: 0.3344, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 066, Loss: 0.3208, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 067, Loss: 0.3195, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 068, Loss: 0.3073, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 069, Loss: 0.3153, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 070, Loss: 0.3153, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 071, Loss: 0.3098, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 072, Loss: 0.3012, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 073, Loss: 0.3111, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 074, Loss: 0.2916, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.2867, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.2878, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.2868, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.2903, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.2780, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 080, Loss: 0.2737, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 081, Loss: 0.2800, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 082, Loss: 0.2696, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 083, Loss: 0.2614, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 084, Loss: 0.2703, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 085, Loss: 0.2670, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 086, Loss: 0.2568, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 087, Loss: 0.2596, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2611, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2402, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2546, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.2458, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2509, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2394, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2405, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.2394, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.2446, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.2413, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.2342, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.2379, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.2352, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 101, Loss: 0.2277, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 102, Loss: 0.2399, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.2198, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.2195, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.2218, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2205, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2077, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2193, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2149, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 110, Loss: 0.2151, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 111, Loss: 0.2188, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.2015, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 113, Loss: 0.2073, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 114, Loss: 0.2066, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.2117, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 116, Loss: 0.2052, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.2070, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.2106, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 119, Loss: 0.1852, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 120, Loss: 0.1962, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 121, Loss: 0.2019, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 122, Loss: 0.1967, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 123, Loss: 0.1939, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 124, Loss: 0.1985, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 125, Loss: 0.1842, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 126, Loss: 0.1962, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 127, Loss: 0.1925, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 128, Loss: 0.1927, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 129, Loss: 0.1843, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 130, Loss: 0.1805, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 131, Loss: 0.1859, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 132, Loss: 0.1868, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 133, Loss: 0.1776, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 134, Loss: 0.1822, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 135, Loss: 0.1738, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 136, Loss: 0.1680, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 137, Loss: 0.1766, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 138, Loss: 0.1774, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 139, Loss: 0.1752, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.1741, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.1710, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.1629, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 143, Loss: 0.1633, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 144, Loss: 0.1734, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 145, Loss: 0.1665, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 146, Loss: 0.1565, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 147, Loss: 0.1704, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.1689, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.1613, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 150, Loss: 0.1626, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 151, Loss: 0.1606, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 152, Loss: 0.1585, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 153, Loss: 0.1658, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 154, Loss: 0.1590, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 155, Loss: 0.1763, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 156, Loss: 0.1586, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 157, Loss: 0.1599, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.1542, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 159, Loss: 0.1600, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 160, Loss: 0.1521, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 161, Loss: 0.1516, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 162, Loss: 0.1598, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.1502, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.1583, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.1553, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 166, Loss: 0.1514, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 167, Loss: 0.1473, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 168, Loss: 0.1474, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 169, Loss: 0.1533, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 170, Loss: 0.1516, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 171, Loss: 0.1417, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 172, Loss: 0.1426, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 173, Loss: 0.1402, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 174, Loss: 0.1475, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 175, Loss: 0.1475, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.1659, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.1450, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.1419, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 179, Loss: 0.1493, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 180, Loss: 0.1398, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 181, Loss: 0.1679, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 182, Loss: 0.1359, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 183, Loss: 0.1399, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 184, Loss: 0.1345, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 185, Loss: 0.1459, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 186, Loss: 0.1342, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 187, Loss: 0.1409, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.1371, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.1438, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.1341, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 191, Loss: 0.1522, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 192, Loss: 0.1294, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 193, Loss: 0.1326, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 194, Loss: 0.1405, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 195, Loss: 0.1396, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.1615, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.1273, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.1362, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.1384, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.1432, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.1227, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 202, Loss: 0.1293, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 203, Loss: 0.1282, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 204, Loss: 0.1282, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 205, Loss: 0.1387, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.1242, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.1170, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.1385, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.1506, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.1328, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.1452, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.1249, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.1304, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.1284, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.1286, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 216, Loss: 0.1219, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.1297, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.1276, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.1239, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.1171, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.1260, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.1136, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.1152, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.1237, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.1128, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.1133, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.1112, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.1224, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.1184, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.1150, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.1110, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.1226, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.1115, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.1257, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 235, Loss: 0.1366, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.1007, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.1170, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.1078, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.1060, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.1202, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.1009, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.1145, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.1034, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.1076, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.1075, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.0979, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.1050, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.0983, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.1061, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.1078, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.1302, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.0961, Train: 0.9765, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.1035, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.1116, Train: 0.9706, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.1142, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.0966, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.7222, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 002, Loss: 0.7081, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 003, Loss: 0.6931, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 004, Loss: 0.6871, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 005, Loss: 0.6705, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 006, Loss: 0.6591, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 007, Loss: 0.6494, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 008, Loss: 0.6370, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 009, Loss: 0.6292, Train: 0.3529, Test: 0.3333\n",
            "Epoch: 010, Loss: 0.6217, Train: 0.6176, Test: 0.5556\n",
            "Epoch: 011, Loss: 0.6155, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.6076, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.6002, Train: 0.8412, Test: 0.7778\n",
            "Epoch: 014, Loss: 0.5957, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.5884, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 016, Loss: 0.5857, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.5833, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 018, Loss: 0.5742, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 019, Loss: 0.5664, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 020, Loss: 0.5658, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.5571, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.5560, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.5529, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.5413, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.5446, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.5322, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.5285, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.5244, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.5194, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 030, Loss: 0.5196, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 031, Loss: 0.5111, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 032, Loss: 0.5062, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 033, Loss: 0.4980, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 034, Loss: 0.4959, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 035, Loss: 0.4888, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 036, Loss: 0.4835, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 037, Loss: 0.4736, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.4733, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.4691, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.4688, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.4621, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.4555, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.4489, Train: 0.8412, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.4447, Train: 0.8471, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.4381, Train: 0.8529, Test: 0.6667\n",
            "Epoch: 046, Loss: 0.4368, Train: 0.8529, Test: 0.6667\n",
            "Epoch: 047, Loss: 0.4314, Train: 0.8529, Test: 0.6667\n",
            "Epoch: 048, Loss: 0.4258, Train: 0.8529, Test: 0.6667\n",
            "Epoch: 049, Loss: 0.4243, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 050, Loss: 0.4203, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 051, Loss: 0.4122, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 052, Loss: 0.4162, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 053, Loss: 0.4000, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 054, Loss: 0.3985, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 055, Loss: 0.4026, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 056, Loss: 0.3899, Train: 0.8765, Test: 0.6667\n",
            "Epoch: 057, Loss: 0.3889, Train: 0.8765, Test: 0.6667\n",
            "Epoch: 058, Loss: 0.3849, Train: 0.8765, Test: 0.6667\n",
            "Epoch: 059, Loss: 0.3745, Train: 0.8824, Test: 0.6667\n",
            "Epoch: 060, Loss: 0.3775, Train: 0.8824, Test: 0.6667\n",
            "Epoch: 061, Loss: 0.3685, Train: 0.8882, Test: 0.6667\n",
            "Epoch: 062, Loss: 0.3646, Train: 0.8882, Test: 0.7222\n",
            "Epoch: 063, Loss: 0.3646, Train: 0.8882, Test: 0.7222\n",
            "Epoch: 064, Loss: 0.3580, Train: 0.8882, Test: 0.7222\n",
            "Epoch: 065, Loss: 0.3555, Train: 0.8941, Test: 0.7222\n",
            "Epoch: 066, Loss: 0.3503, Train: 0.8941, Test: 0.7222\n",
            "Epoch: 067, Loss: 0.3380, Train: 0.8882, Test: 0.7222\n",
            "Epoch: 068, Loss: 0.3348, Train: 0.8882, Test: 0.7222\n",
            "Epoch: 069, Loss: 0.3324, Train: 0.9000, Test: 0.7222\n",
            "Epoch: 070, Loss: 0.3347, Train: 0.9000, Test: 0.7222\n",
            "Epoch: 071, Loss: 0.3230, Train: 0.9059, Test: 0.7222\n",
            "Epoch: 072, Loss: 0.3225, Train: 0.9118, Test: 0.7222\n",
            "Epoch: 073, Loss: 0.3221, Train: 0.9118, Test: 0.7222\n",
            "Epoch: 074, Loss: 0.3194, Train: 0.9118, Test: 0.7222\n",
            "Epoch: 075, Loss: 0.3168, Train: 0.9118, Test: 0.7222\n",
            "Epoch: 076, Loss: 0.3124, Train: 0.9118, Test: 0.7222\n",
            "Epoch: 077, Loss: 0.3081, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 078, Loss: 0.3029, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 079, Loss: 0.3060, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 080, Loss: 0.3007, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 081, Loss: 0.3015, Train: 0.9176, Test: 0.7222\n",
            "Epoch: 082, Loss: 0.2938, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 083, Loss: 0.2960, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 084, Loss: 0.2842, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 085, Loss: 0.2899, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 086, Loss: 0.2973, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 087, Loss: 0.2847, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 088, Loss: 0.2701, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 089, Loss: 0.2795, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 090, Loss: 0.2777, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 091, Loss: 0.2673, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 092, Loss: 0.2611, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 093, Loss: 0.2708, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 094, Loss: 0.2584, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 095, Loss: 0.2630, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 096, Loss: 0.2753, Train: 0.9176, Test: 0.7222\n",
            "Epoch: 097, Loss: 0.2559, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 098, Loss: 0.2633, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 099, Loss: 0.2566, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 100, Loss: 0.2526, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 101, Loss: 0.2642, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 102, Loss: 0.2377, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 103, Loss: 0.2495, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 104, Loss: 0.2437, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 105, Loss: 0.2435, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 106, Loss: 0.2314, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 107, Loss: 0.2359, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 108, Loss: 0.2284, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 109, Loss: 0.2310, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 110, Loss: 0.2299, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 111, Loss: 0.2287, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 112, Loss: 0.2280, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 113, Loss: 0.2283, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 114, Loss: 0.2258, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 115, Loss: 0.2284, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 116, Loss: 0.2219, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 117, Loss: 0.2199, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 118, Loss: 0.2119, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 119, Loss: 0.2199, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 120, Loss: 0.2100, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 121, Loss: 0.2138, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 122, Loss: 0.2209, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 123, Loss: 0.2106, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 124, Loss: 0.2158, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 125, Loss: 0.2203, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 126, Loss: 0.2051, Train: 0.9353, Test: 0.6667\n",
            "Epoch: 127, Loss: 0.2109, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 128, Loss: 0.2066, Train: 0.9176, Test: 0.6667\n",
            "Epoch: 129, Loss: 0.1993, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 130, Loss: 0.1922, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 131, Loss: 0.2044, Train: 0.9294, Test: 0.6667\n",
            "Epoch: 132, Loss: 0.1982, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 133, Loss: 0.1962, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 134, Loss: 0.1986, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 135, Loss: 0.1980, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 136, Loss: 0.2036, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 137, Loss: 0.2004, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 138, Loss: 0.1928, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 139, Loss: 0.1893, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 140, Loss: 0.1828, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 141, Loss: 0.1930, Train: 0.9176, Test: 0.7222\n",
            "Epoch: 142, Loss: 0.1944, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 143, Loss: 0.2012, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 144, Loss: 0.1836, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 145, Loss: 0.1904, Train: 0.9235, Test: 0.6667\n",
            "Epoch: 146, Loss: 0.1850, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 147, Loss: 0.1911, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 148, Loss: 0.2074, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 149, Loss: 0.1811, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 150, Loss: 0.1822, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 151, Loss: 0.1805, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 152, Loss: 0.1758, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 153, Loss: 0.1935, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 154, Loss: 0.1856, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 155, Loss: 0.1724, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 156, Loss: 0.1718, Train: 0.9176, Test: 0.7778\n",
            "Epoch: 157, Loss: 0.1862, Train: 0.9176, Test: 0.7222\n",
            "Epoch: 158, Loss: 0.1729, Train: 0.9235, Test: 0.7222\n",
            "Epoch: 159, Loss: 0.1704, Train: 0.9176, Test: 0.7778\n",
            "Epoch: 160, Loss: 0.1797, Train: 0.9176, Test: 0.7778\n",
            "Epoch: 161, Loss: 0.1737, Train: 0.9176, Test: 0.7222\n",
            "Epoch: 162, Loss: 0.1645, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 163, Loss: 0.1817, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 164, Loss: 0.1690, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 165, Loss: 0.1898, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 166, Loss: 0.1742, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 167, Loss: 0.1736, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 168, Loss: 0.1714, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 169, Loss: 0.1651, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 170, Loss: 0.1628, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 171, Loss: 0.1623, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 172, Loss: 0.1646, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 173, Loss: 0.1640, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 174, Loss: 0.1584, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 175, Loss: 0.1605, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 176, Loss: 0.1718, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 177, Loss: 0.1584, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 178, Loss: 0.1670, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 179, Loss: 0.1521, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 180, Loss: 0.1613, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 181, Loss: 0.1607, Train: 0.9294, Test: 0.7222\n",
            "Epoch: 182, Loss: 0.1533, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 183, Loss: 0.1621, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 184, Loss: 0.1555, Train: 0.9353, Test: 0.7222\n",
            "Epoch: 185, Loss: 0.1527, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 186, Loss: 0.1470, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 187, Loss: 0.1618, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 188, Loss: 0.1553, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 189, Loss: 0.1604, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 190, Loss: 0.1619, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 191, Loss: 0.1595, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 192, Loss: 0.1431, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 193, Loss: 0.1652, Train: 0.9471, Test: 0.7222\n",
            "Epoch: 194, Loss: 0.1418, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 195, Loss: 0.1479, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 196, Loss: 0.1626, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 197, Loss: 0.1754, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 198, Loss: 0.1474, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 199, Loss: 0.1704, Train: 0.9353, Test: 0.7778\n",
            "Epoch: 200, Loss: 0.1368, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 201, Loss: 0.1392, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 202, Loss: 0.1403, Train: 0.9412, Test: 0.7222\n",
            "Epoch: 203, Loss: 0.1379, Train: 0.9235, Test: 0.7778\n",
            "Epoch: 204, Loss: 0.1322, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 205, Loss: 0.1439, Train: 0.9412, Test: 0.8889\n",
            "Epoch: 206, Loss: 0.1376, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.1427, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 208, Loss: 0.1354, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 209, Loss: 0.1439, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 210, Loss: 0.1417, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.1489, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.1449, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 213, Loss: 0.1564, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.1378, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.1507, Train: 0.9294, Test: 0.7778\n",
            "Epoch: 216, Loss: 0.1433, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.1440, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.1500, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.1388, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.1503, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 221, Loss: 0.1350, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 222, Loss: 0.1291, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 223, Loss: 0.1442, Train: 0.9412, Test: 0.7778\n",
            "Epoch: 224, Loss: 0.1364, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.1522, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.1288, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.1387, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.1244, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.1278, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.1272, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.1261, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.1291, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.1409, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.1322, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 235, Loss: 0.1278, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.1252, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.1354, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.1273, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.1217, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.1377, Train: 0.9471, Test: 0.7778\n",
            "Epoch: 241, Loss: 0.1389, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 242, Loss: 0.1324, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 243, Loss: 0.1322, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.1181, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.1186, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.1220, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.1359, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.1215, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.1325, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.1271, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.1303, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.1276, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.1310, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.1270, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.1199, Train: 0.9588, Test: 0.7778\n",
            "Epoch: 256, Loss: 0.1310, Train: 0.9529, Test: 0.7778\n",
            "Epoch: 001, Loss: 0.6819, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 002, Loss: 0.6601, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 003, Loss: 0.6437, Train: 0.6412, Test: 0.6667\n",
            "Epoch: 004, Loss: 0.6307, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 005, Loss: 0.6210, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 006, Loss: 0.6072, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.5976, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 008, Loss: 0.5941, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 009, Loss: 0.5848, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.5819, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 011, Loss: 0.5704, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.5650, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.5636, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 014, Loss: 0.5545, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.5454, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 016, Loss: 0.5417, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.5381, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 018, Loss: 0.5327, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 019, Loss: 0.5310, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 020, Loss: 0.5263, Train: 0.7176, Test: 0.6111\n",
            "Epoch: 021, Loss: 0.5171, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 022, Loss: 0.5193, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 023, Loss: 0.5045, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 024, Loss: 0.5032, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 025, Loss: 0.4975, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 026, Loss: 0.4963, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 027, Loss: 0.4956, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 028, Loss: 0.4874, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 029, Loss: 0.4847, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 030, Loss: 0.4896, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 031, Loss: 0.4802, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 032, Loss: 0.4740, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 033, Loss: 0.4671, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 034, Loss: 0.4629, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 035, Loss: 0.4639, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 036, Loss: 0.4558, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 037, Loss: 0.4490, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 038, Loss: 0.4493, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 039, Loss: 0.4438, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 040, Loss: 0.4424, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 041, Loss: 0.4380, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 042, Loss: 0.4377, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 043, Loss: 0.4352, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 044, Loss: 0.4304, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 045, Loss: 0.4275, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 046, Loss: 0.4208, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 047, Loss: 0.4183, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 048, Loss: 0.4105, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 049, Loss: 0.4094, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 050, Loss: 0.4057, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 051, Loss: 0.4044, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 052, Loss: 0.4141, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 053, Loss: 0.3927, Train: 0.8294, Test: 0.6111\n",
            "Epoch: 054, Loss: 0.3922, Train: 0.8294, Test: 0.6111\n",
            "Epoch: 055, Loss: 0.3976, Train: 0.8353, Test: 0.6111\n",
            "Epoch: 056, Loss: 0.3958, Train: 0.8529, Test: 0.6111\n",
            "Epoch: 057, Loss: 0.3817, Train: 0.8529, Test: 0.6667\n",
            "Epoch: 058, Loss: 0.3909, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 059, Loss: 0.3968, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 060, Loss: 0.3738, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 061, Loss: 0.3704, Train: 0.8588, Test: 0.6667\n",
            "Epoch: 062, Loss: 0.3682, Train: 0.8647, Test: 0.6667\n",
            "Epoch: 063, Loss: 0.3717, Train: 0.8647, Test: 0.6667\n",
            "Epoch: 064, Loss: 0.3608, Train: 0.8647, Test: 0.6667\n",
            "Epoch: 065, Loss: 0.3623, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 066, Loss: 0.3558, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 067, Loss: 0.3543, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 068, Loss: 0.3532, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 069, Loss: 0.3433, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 070, Loss: 0.3531, Train: 0.8647, Test: 0.7222\n",
            "Epoch: 071, Loss: 0.3407, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.3464, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.3310, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.3399, Train: 0.8647, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.3370, Train: 0.8706, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.3259, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.3348, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.3184, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.3200, Train: 0.8765, Test: 0.8333\n",
            "Epoch: 080, Loss: 0.3174, Train: 0.8765, Test: 0.8889\n",
            "Epoch: 081, Loss: 0.3132, Train: 0.8824, Test: 0.8333\n",
            "Epoch: 082, Loss: 0.3053, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 083, Loss: 0.2964, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 084, Loss: 0.3118, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 085, Loss: 0.2972, Train: 0.8882, Test: 0.8889\n",
            "Epoch: 086, Loss: 0.3021, Train: 0.8941, Test: 0.8889\n",
            "Epoch: 087, Loss: 0.2961, Train: 0.9000, Test: 0.9444\n",
            "Epoch: 088, Loss: 0.2977, Train: 0.9059, Test: 0.9444\n",
            "Epoch: 089, Loss: 0.2958, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 090, Loss: 0.2896, Train: 0.9000, Test: 0.9444\n",
            "Epoch: 091, Loss: 0.2808, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 092, Loss: 0.2833, Train: 0.9000, Test: 0.8889\n",
            "Epoch: 093, Loss: 0.2804, Train: 0.9059, Test: 0.9444\n",
            "Epoch: 094, Loss: 0.2764, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 095, Loss: 0.2717, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 096, Loss: 0.2672, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 097, Loss: 0.2795, Train: 0.9059, Test: 0.9444\n",
            "Epoch: 098, Loss: 0.2723, Train: 0.9059, Test: 0.9444\n",
            "Epoch: 099, Loss: 0.2697, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 100, Loss: 0.2633, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 101, Loss: 0.2659, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 102, Loss: 0.2612, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 103, Loss: 0.2556, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 104, Loss: 0.2614, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 105, Loss: 0.2493, Train: 0.9118, Test: 0.9444\n",
            "Epoch: 106, Loss: 0.2550, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 107, Loss: 0.2619, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 108, Loss: 0.2478, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 109, Loss: 0.2500, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 110, Loss: 0.2398, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 111, Loss: 0.2411, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 112, Loss: 0.2395, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 113, Loss: 0.2319, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 114, Loss: 0.2403, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 115, Loss: 0.2360, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 116, Loss: 0.2426, Train: 0.9176, Test: 0.9444\n",
            "Epoch: 117, Loss: 0.2318, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 118, Loss: 0.2265, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 119, Loss: 0.2282, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 120, Loss: 0.2305, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 121, Loss: 0.2209, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 122, Loss: 0.2230, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 123, Loss: 0.2173, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 124, Loss: 0.2207, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 125, Loss: 0.2261, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 126, Loss: 0.2139, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 127, Loss: 0.2168, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 128, Loss: 0.2121, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 129, Loss: 0.2034, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 130, Loss: 0.2039, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 131, Loss: 0.2082, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 132, Loss: 0.2147, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 133, Loss: 0.2076, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 134, Loss: 0.2019, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 135, Loss: 0.2064, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 136, Loss: 0.2049, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 137, Loss: 0.2299, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 138, Loss: 0.2006, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 139, Loss: 0.1975, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 140, Loss: 0.2053, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 141, Loss: 0.1933, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 142, Loss: 0.1968, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 143, Loss: 0.2002, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 144, Loss: 0.2073, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 145, Loss: 0.1911, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 146, Loss: 0.1864, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 147, Loss: 0.2064, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 148, Loss: 0.1971, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 149, Loss: 0.1995, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 150, Loss: 0.1925, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 151, Loss: 0.1984, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 152, Loss: 0.1780, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 153, Loss: 0.1873, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 154, Loss: 0.1859, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 155, Loss: 0.1745, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 156, Loss: 0.1962, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 157, Loss: 0.1773, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 158, Loss: 0.1786, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 159, Loss: 0.1750, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 160, Loss: 0.1776, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 161, Loss: 0.1904, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 162, Loss: 0.1829, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 163, Loss: 0.1710, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 164, Loss: 0.1707, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 165, Loss: 0.1765, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 166, Loss: 0.1714, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 167, Loss: 0.1608, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 168, Loss: 0.1829, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 169, Loss: 0.1692, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 170, Loss: 0.1764, Train: 0.9235, Test: 0.9444\n",
            "Epoch: 171, Loss: 0.1653, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 172, Loss: 0.1648, Train: 0.9294, Test: 0.9444\n",
            "Epoch: 173, Loss: 0.1760, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 174, Loss: 0.1648, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 175, Loss: 0.1686, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 176, Loss: 0.1711, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 177, Loss: 0.1738, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 178, Loss: 0.1662, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 179, Loss: 0.1687, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 180, Loss: 0.1670, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 181, Loss: 0.1673, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 182, Loss: 0.1589, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 183, Loss: 0.1674, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 184, Loss: 0.1553, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 185, Loss: 0.1528, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 186, Loss: 0.1704, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 187, Loss: 0.1485, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 188, Loss: 0.1596, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 189, Loss: 0.1580, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 190, Loss: 0.1803, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 191, Loss: 0.1512, Train: 0.9353, Test: 0.9444\n",
            "Epoch: 192, Loss: 0.1566, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 193, Loss: 0.1467, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 194, Loss: 0.1688, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 195, Loss: 0.1585, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 196, Loss: 0.1420, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 197, Loss: 0.1500, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 198, Loss: 0.1506, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 199, Loss: 0.1541, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 200, Loss: 0.1459, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 201, Loss: 0.1548, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 202, Loss: 0.1414, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 203, Loss: 0.1500, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.1436, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.1424, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.1592, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 207, Loss: 0.1414, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 208, Loss: 0.1496, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 209, Loss: 0.1415, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 210, Loss: 0.1457, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 211, Loss: 0.1391, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 212, Loss: 0.1321, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 213, Loss: 0.1336, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 214, Loss: 0.1467, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 215, Loss: 0.1338, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 216, Loss: 0.1343, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 217, Loss: 0.1494, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 218, Loss: 0.1432, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 219, Loss: 0.1313, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 220, Loss: 0.1332, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 221, Loss: 0.1409, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 222, Loss: 0.1331, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 223, Loss: 0.1430, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 224, Loss: 0.1564, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 225, Loss: 0.1381, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 226, Loss: 0.1296, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 227, Loss: 0.1369, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 228, Loss: 0.1386, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 229, Loss: 0.1309, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 230, Loss: 0.1262, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 231, Loss: 0.1401, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 232, Loss: 0.1281, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.1200, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.1199, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.1263, Train: 0.9706, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.1336, Train: 0.9706, Test: 0.9444\n",
            "Epoch: 237, Loss: 0.1232, Train: 0.9647, Test: 0.9444\n",
            "Epoch: 238, Loss: 0.1234, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 239, Loss: 0.1223, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 240, Loss: 0.1384, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 241, Loss: 0.1170, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 242, Loss: 0.1160, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 243, Loss: 0.1389, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 244, Loss: 0.1217, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 245, Loss: 0.1240, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 246, Loss: 0.1472, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 247, Loss: 0.1242, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 248, Loss: 0.1284, Train: 0.9412, Test: 0.9444\n",
            "Epoch: 249, Loss: 0.1306, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 250, Loss: 0.1201, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 251, Loss: 0.1171, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.1198, Train: 0.9529, Test: 0.9444\n",
            "Epoch: 253, Loss: 0.1153, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 254, Loss: 0.1205, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 255, Loss: 0.1102, Train: 0.9588, Test: 0.9444\n",
            "Epoch: 256, Loss: 0.1311, Train: 0.9471, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.7006, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 002, Loss: 0.6855, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 003, Loss: 0.6695, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 004, Loss: 0.6615, Train: 0.2765, Test: 0.3889\n",
            "Epoch: 005, Loss: 0.6535, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 006, Loss: 0.6417, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 007, Loss: 0.6364, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 008, Loss: 0.6289, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 009, Loss: 0.6220, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 010, Loss: 0.6171, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 011, Loss: 0.6125, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 012, Loss: 0.6011, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 013, Loss: 0.5965, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 014, Loss: 0.5872, Train: 0.6941, Test: 0.4444\n",
            "Epoch: 015, Loss: 0.5846, Train: 0.7059, Test: 0.4444\n",
            "Epoch: 016, Loss: 0.5801, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 017, Loss: 0.5757, Train: 0.7471, Test: 0.5556\n",
            "Epoch: 018, Loss: 0.5679, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 019, Loss: 0.5609, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 020, Loss: 0.5578, Train: 0.7588, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.5561, Train: 0.7529, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.5488, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.5417, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.5352, Train: 0.7529, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.5300, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.5282, Train: 0.7529, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.5177, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 028, Loss: 0.5208, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 029, Loss: 0.5104, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 030, Loss: 0.5113, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 031, Loss: 0.5053, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 032, Loss: 0.4941, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 033, Loss: 0.4998, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 034, Loss: 0.4930, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 035, Loss: 0.4836, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 036, Loss: 0.4820, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 037, Loss: 0.4751, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 038, Loss: 0.4707, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 039, Loss: 0.4720, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 040, Loss: 0.4721, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 041, Loss: 0.4563, Train: 0.8294, Test: 0.7222\n",
            "Epoch: 042, Loss: 0.4575, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 043, Loss: 0.4471, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 044, Loss: 0.4367, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 045, Loss: 0.4365, Train: 0.8353, Test: 0.7222\n",
            "Epoch: 046, Loss: 0.4258, Train: 0.8353, Test: 0.7778\n",
            "Epoch: 047, Loss: 0.4280, Train: 0.8471, Test: 0.7778\n",
            "Epoch: 048, Loss: 0.4149, Train: 0.8529, Test: 0.7778\n",
            "Epoch: 049, Loss: 0.4155, Train: 0.8647, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.4046, Train: 0.8765, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.4018, Train: 0.8706, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.3959, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.3967, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.3880, Train: 0.8824, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.3821, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.3754, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.3738, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.3655, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.3639, Train: 0.8882, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.3559, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.3519, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.3485, Train: 0.8941, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.3447, Train: 0.9000, Test: 0.8333\n",
            "Epoch: 064, Loss: 0.3408, Train: 0.8882, Test: 0.8333\n",
            "Epoch: 065, Loss: 0.3321, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 066, Loss: 0.3329, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 067, Loss: 0.3319, Train: 0.8941, Test: 0.8333\n",
            "Epoch: 068, Loss: 0.3316, Train: 0.9000, Test: 0.8333\n",
            "Epoch: 069, Loss: 0.3294, Train: 0.9000, Test: 0.8333\n",
            "Epoch: 070, Loss: 0.3382, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 071, Loss: 0.3166, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 072, Loss: 0.3105, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 073, Loss: 0.3202, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 074, Loss: 0.3103, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 075, Loss: 0.3094, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 076, Loss: 0.2993, Train: 0.9059, Test: 0.8333\n",
            "Epoch: 077, Loss: 0.3012, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 078, Loss: 0.2989, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 079, Loss: 0.2947, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 080, Loss: 0.2914, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 081, Loss: 0.2833, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 082, Loss: 0.2801, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 083, Loss: 0.2764, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 084, Loss: 0.2789, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 085, Loss: 0.2844, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 086, Loss: 0.2720, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 087, Loss: 0.2798, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 088, Loss: 0.2649, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 089, Loss: 0.2646, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.2701, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.2633, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.2535, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.2511, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.2670, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.2488, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.2533, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.2536, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.2451, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.2516, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.2434, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 101, Loss: 0.2543, Train: 0.9118, Test: 0.8333\n",
            "Epoch: 102, Loss: 0.2365, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.2263, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.2386, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.2267, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.2290, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.2310, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.2322, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.2207, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 110, Loss: 0.2195, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 111, Loss: 0.2182, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.2187, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 113, Loss: 0.2293, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 114, Loss: 0.2262, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.2217, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 116, Loss: 0.2198, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.2059, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.2143, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 119, Loss: 0.2071, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 120, Loss: 0.2093, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 121, Loss: 0.2057, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 122, Loss: 0.2201, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 123, Loss: 0.2054, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 124, Loss: 0.2101, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 125, Loss: 0.2020, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 126, Loss: 0.2023, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 127, Loss: 0.1990, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 128, Loss: 0.1929, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 129, Loss: 0.2013, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 130, Loss: 0.1989, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 131, Loss: 0.1858, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 132, Loss: 0.1905, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 133, Loss: 0.2033, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 134, Loss: 0.1861, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 135, Loss: 0.1884, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 136, Loss: 0.1866, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 137, Loss: 0.1868, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 138, Loss: 0.1854, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 139, Loss: 0.1776, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.1791, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.1778, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.1807, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 143, Loss: 0.1774, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 144, Loss: 0.1712, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 145, Loss: 0.1773, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 146, Loss: 0.1808, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 147, Loss: 0.1794, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.1704, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.1801, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 150, Loss: 0.1769, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 151, Loss: 0.1687, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 152, Loss: 0.1689, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 153, Loss: 0.1601, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 154, Loss: 0.1735, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 155, Loss: 0.1729, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 156, Loss: 0.1748, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 157, Loss: 0.1822, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.1652, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 159, Loss: 0.1588, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 160, Loss: 0.1731, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 161, Loss: 0.1645, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 162, Loss: 0.1614, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.1635, Train: 0.9235, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.1643, Train: 0.9176, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.1559, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 166, Loss: 0.1610, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 167, Loss: 0.1566, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 168, Loss: 0.1499, Train: 0.9294, Test: 0.8333\n",
            "Epoch: 169, Loss: 0.1560, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 170, Loss: 0.1496, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 171, Loss: 0.1540, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 172, Loss: 0.1546, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 173, Loss: 0.1518, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 174, Loss: 0.1647, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 175, Loss: 0.1557, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.1525, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.1536, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.1497, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 179, Loss: 0.1557, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 180, Loss: 0.1413, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 181, Loss: 0.1425, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 182, Loss: 0.1403, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 183, Loss: 0.1582, Train: 0.9353, Test: 0.8333\n",
            "Epoch: 184, Loss: 0.1511, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 185, Loss: 0.1393, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 186, Loss: 0.1459, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 187, Loss: 0.1375, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.1493, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.1446, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.1491, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 191, Loss: 0.1524, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 192, Loss: 0.1347, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 193, Loss: 0.1377, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 194, Loss: 0.1500, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 195, Loss: 0.1292, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.1386, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.1327, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.1356, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.1327, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.1377, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.1698, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 202, Loss: 0.1332, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 203, Loss: 0.1278, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 204, Loss: 0.1371, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 205, Loss: 0.1409, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.1279, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.1286, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.1421, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.1335, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.1295, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.1301, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.1324, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.1348, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.1344, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.1410, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 216, Loss: 0.1272, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.1270, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.1388, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.1246, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.1370, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.1185, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.1250, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.1197, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.1242, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.1255, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.1241, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.1138, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.1169, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.1350, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.1126, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.1242, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.1313, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.1359, Train: 0.9471, Test: 0.8889\n",
            "Epoch: 234, Loss: 0.1272, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 235, Loss: 0.1321, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.1167, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.1282, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.1230, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.1181, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.1251, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.1368, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.1450, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.1377, Train: 0.9412, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.1196, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.1143, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.1101, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.1382, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.1100, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.1307, Train: 0.9647, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.1113, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.1142, Train: 0.9471, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.1222, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.1069, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.1085, Train: 0.9529, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.1190, Train: 0.9588, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.1050, Train: 0.9647, Test: 0.8333\n",
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.11,    Train:0.96,    Test:0.85\n",
            "Median time per epoch: 0.0413s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3：k-GNN\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        optimizer.zero_grad()\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        total_correct += int((pred == data.y).sum())\n",
        "    return total_correct / len(loader.dataset)\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset[:start] + dataset[end:]\n",
        "    test_dataset = dataset[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model =  kGNN(hidden_channels=params[\"hidden_features_kgnn\"]).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss) * data.num_graphs\n",
        "        return total_loss / len(train_loader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "\n",
        "        total_correct = 0\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=-1)\n",
        "            total_correct += int((pred == data.y).sum())\n",
        "        return total_correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc = test(train_loader)\n",
        "        test_acc = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKXXVDiSxgHs",
        "outputId": "1955fe90-4b75-40cd-b95f-e395a54cf811"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.7662, Train: 0.3412, Test: 0.2778\n",
            "Epoch: 002, Loss: 0.7401, Train: 0.3412, Test: 0.2778\n",
            "Epoch: 003, Loss: 0.7391, Train: 0.6941, Test: 0.7222\n",
            "Epoch: 004, Loss: 0.7042, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 005, Loss: 0.6852, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 006, Loss: 0.6954, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 007, Loss: 0.6565, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 008, Loss: 0.6563, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 009, Loss: 0.6399, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 010, Loss: 0.6549, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 011, Loss: 0.6555, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 012, Loss: 0.6240, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 013, Loss: 0.6653, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 014, Loss: 0.6430, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 015, Loss: 0.6406, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 016, Loss: 0.6350, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 017, Loss: 0.6325, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 018, Loss: 0.6221, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 019, Loss: 0.6160, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 020, Loss: 0.6310, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 021, Loss: 0.6412, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 022, Loss: 0.6156, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 023, Loss: 0.6287, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 024, Loss: 0.6082, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 025, Loss: 0.5881, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 026, Loss: 0.6120, Train: 0.6588, Test: 0.7222\n",
            "Epoch: 027, Loss: 0.6112, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 028, Loss: 0.5986, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 029, Loss: 0.5775, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 030, Loss: 0.5970, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 031, Loss: 0.5999, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 032, Loss: 0.5953, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 033, Loss: 0.5923, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 034, Loss: 0.5876, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 035, Loss: 0.5701, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 036, Loss: 0.5735, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 037, Loss: 0.5833, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 038, Loss: 0.6011, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 039, Loss: 0.5704, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 040, Loss: 0.5908, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 041, Loss: 0.5731, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 042, Loss: 0.5584, Train: 0.6647, Test: 0.7222\n",
            "Epoch: 043, Loss: 0.5684, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 044, Loss: 0.5607, Train: 0.6706, Test: 0.7222\n",
            "Epoch: 045, Loss: 0.5450, Train: 0.6824, Test: 0.7222\n",
            "Epoch: 046, Loss: 0.5578, Train: 0.6882, Test: 0.7222\n",
            "Epoch: 047, Loss: 0.5901, Train: 0.6941, Test: 0.7222\n",
            "Epoch: 048, Loss: 0.5844, Train: 0.6941, Test: 0.7222\n",
            "Epoch: 049, Loss: 0.5506, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.5626, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.5535, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.5891, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.5546, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.5449, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.5406, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.5514, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.5631, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.5404, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.5505, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.5375, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.5401, Train: 0.6941, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.5423, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.5470, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.5588, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.5470, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.5423, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.5195, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 068, Loss: 0.5406, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 069, Loss: 0.5351, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 070, Loss: 0.5268, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 071, Loss: 0.5151, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.5352, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.5428, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.5387, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 075, Loss: 0.5149, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 076, Loss: 0.5209, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 077, Loss: 0.5325, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 078, Loss: 0.5320, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 079, Loss: 0.5403, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 080, Loss: 0.5110, Train: 0.6706, Test: 0.8333\n",
            "Epoch: 081, Loss: 0.5219, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 082, Loss: 0.5115, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 083, Loss: 0.5326, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 084, Loss: 0.5160, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 085, Loss: 0.5253, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 086, Loss: 0.5000, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 087, Loss: 0.5232, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 088, Loss: 0.5203, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 089, Loss: 0.5112, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 090, Loss: 0.5162, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 091, Loss: 0.5194, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 092, Loss: 0.5235, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 093, Loss: 0.5027, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 094, Loss: 0.5086, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 095, Loss: 0.5193, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 096, Loss: 0.5038, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 097, Loss: 0.5268, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 098, Loss: 0.5196, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 099, Loss: 0.5060, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 100, Loss: 0.5016, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 101, Loss: 0.5128, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 102, Loss: 0.4867, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 103, Loss: 0.5092, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 104, Loss: 0.5037, Train: 0.6882, Test: 0.8889\n",
            "Epoch: 105, Loss: 0.5034, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 106, Loss: 0.4979, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 107, Loss: 0.4977, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 108, Loss: 0.5009, Train: 0.6824, Test: 0.8889\n",
            "Epoch: 109, Loss: 0.4950, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 110, Loss: 0.5024, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 111, Loss: 0.5140, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 112, Loss: 0.5009, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 113, Loss: 0.4977, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 114, Loss: 0.5027, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 115, Loss: 0.4903, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 116, Loss: 0.4987, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 117, Loss: 0.4929, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 118, Loss: 0.5079, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 119, Loss: 0.4998, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.5021, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 121, Loss: 0.4896, Train: 0.7000, Test: 0.8889\n",
            "Epoch: 122, Loss: 0.4778, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.4949, Train: 0.6941, Test: 0.8889\n",
            "Epoch: 124, Loss: 0.4878, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 125, Loss: 0.4999, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 126, Loss: 0.4951, Train: 0.7059, Test: 0.8889\n",
            "Epoch: 127, Loss: 0.4832, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 128, Loss: 0.4854, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 129, Loss: 0.5017, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 130, Loss: 0.4792, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 131, Loss: 0.4901, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 132, Loss: 0.4752, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 133, Loss: 0.4896, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 134, Loss: 0.4820, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 135, Loss: 0.4872, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 136, Loss: 0.4827, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 137, Loss: 0.4770, Train: 0.7176, Test: 0.8889\n",
            "Epoch: 138, Loss: 0.4799, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 139, Loss: 0.4727, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 140, Loss: 0.4863, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 141, Loss: 0.4872, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 142, Loss: 0.4777, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 143, Loss: 0.4732, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 144, Loss: 0.4869, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 145, Loss: 0.4835, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 146, Loss: 0.4761, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 147, Loss: 0.4690, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 148, Loss: 0.4727, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 149, Loss: 0.4759, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 150, Loss: 0.4754, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 151, Loss: 0.4762, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 152, Loss: 0.4734, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 153, Loss: 0.4666, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 154, Loss: 0.4640, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 155, Loss: 0.4739, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 156, Loss: 0.4702, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 157, Loss: 0.4686, Train: 0.7235, Test: 0.8889\n",
            "Epoch: 158, Loss: 0.4766, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 159, Loss: 0.4730, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 160, Loss: 0.4679, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 161, Loss: 0.4640, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 162, Loss: 0.4596, Train: 0.7647, Test: 0.8889\n",
            "Epoch: 163, Loss: 0.4702, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 164, Loss: 0.4734, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 165, Loss: 0.4686, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 166, Loss: 0.4580, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 167, Loss: 0.4719, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 168, Loss: 0.4675, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 169, Loss: 0.4610, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 170, Loss: 0.4595, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 171, Loss: 0.4412, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 172, Loss: 0.4735, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 173, Loss: 0.4775, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 174, Loss: 0.4555, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 175, Loss: 0.4708, Train: 0.7706, Test: 0.8889\n",
            "Epoch: 176, Loss: 0.4684, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.4697, Train: 0.7706, Test: 0.8889\n",
            "Epoch: 178, Loss: 0.4487, Train: 0.7647, Test: 0.8889\n",
            "Epoch: 179, Loss: 0.4593, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 180, Loss: 0.4429, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 181, Loss: 0.4634, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 182, Loss: 0.4587, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 183, Loss: 0.4674, Train: 0.7647, Test: 0.8889\n",
            "Epoch: 184, Loss: 0.4437, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 185, Loss: 0.4465, Train: 0.7588, Test: 0.8889\n",
            "Epoch: 186, Loss: 0.4481, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 187, Loss: 0.4377, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 188, Loss: 0.4467, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 189, Loss: 0.4483, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 190, Loss: 0.4511, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 191, Loss: 0.4498, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 192, Loss: 0.4437, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 193, Loss: 0.4398, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 194, Loss: 0.4391, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 195, Loss: 0.4396, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 196, Loss: 0.4381, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 197, Loss: 0.4407, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 198, Loss: 0.4230, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 199, Loss: 0.4395, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 200, Loss: 0.4368, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 201, Loss: 0.4493, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 202, Loss: 0.4413, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 203, Loss: 0.4448, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.4254, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.4349, Train: 0.7529, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.4289, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 207, Loss: 0.4344, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.4355, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.4274, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.4328, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.4352, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.4309, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.4276, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 214, Loss: 0.4324, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 215, Loss: 0.4301, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 216, Loss: 0.4332, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.4293, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.4208, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.4262, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.4332, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.4190, Train: 0.7882, Test: 0.8889\n",
            "Epoch: 222, Loss: 0.4217, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.4320, Train: 0.7882, Test: 0.8889\n",
            "Epoch: 224, Loss: 0.4242, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 225, Loss: 0.4300, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 226, Loss: 0.4173, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.4151, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.4226, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.4232, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.4168, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 231, Loss: 0.4172, Train: 0.7882, Test: 0.8889\n",
            "Epoch: 232, Loss: 0.4110, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 233, Loss: 0.4215, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 234, Loss: 0.4108, Train: 0.7824, Test: 0.8889\n",
            "Epoch: 235, Loss: 0.4233, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.4102, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.4195, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.4166, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.4090, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.4104, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.4031, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.4143, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.4090, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.3991, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.4131, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.4103, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.4030, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.4174, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.4042, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.4016, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.3968, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.4130, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.4033, Train: 0.7882, Test: 0.8889\n",
            "Epoch: 254, Loss: 0.4065, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.4049, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.4107, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.7464, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 002, Loss: 0.7279, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 003, Loss: 0.6462, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 004, Loss: 0.6678, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 005, Loss: 0.6585, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 006, Loss: 0.6658, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 007, Loss: 0.6161, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 008, Loss: 0.6368, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 009, Loss: 0.6310, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 010, Loss: 0.6258, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 011, Loss: 0.6343, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 012, Loss: 0.6354, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 013, Loss: 0.6196, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 014, Loss: 0.6414, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 015, Loss: 0.6124, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 016, Loss: 0.6420, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 017, Loss: 0.6328, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 018, Loss: 0.6078, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 019, Loss: 0.6258, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 020, Loss: 0.6155, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 021, Loss: 0.6421, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 022, Loss: 0.6105, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 023, Loss: 0.6062, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 024, Loss: 0.5815, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 025, Loss: 0.5972, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 026, Loss: 0.6163, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 027, Loss: 0.6000, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 028, Loss: 0.6083, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 029, Loss: 0.5975, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 030, Loss: 0.5856, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 031, Loss: 0.5758, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 032, Loss: 0.5965, Train: 0.6529, Test: 0.7778\n",
            "Epoch: 033, Loss: 0.5838, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 034, Loss: 0.5807, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 035, Loss: 0.6072, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 036, Loss: 0.5983, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 037, Loss: 0.6087, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 038, Loss: 0.5810, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 039, Loss: 0.5852, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 040, Loss: 0.5652, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 041, Loss: 0.5914, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 042, Loss: 0.5742, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 043, Loss: 0.5655, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 044, Loss: 0.5586, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 045, Loss: 0.5847, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 046, Loss: 0.5708, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 047, Loss: 0.5676, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 048, Loss: 0.5652, Train: 0.6588, Test: 0.7778\n",
            "Epoch: 049, Loss: 0.5704, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 050, Loss: 0.5669, Train: 0.6647, Test: 0.7778\n",
            "Epoch: 051, Loss: 0.5516, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 052, Loss: 0.5527, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.5674, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.5584, Train: 0.6706, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.5414, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.5320, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.5398, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.5376, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.5369, Train: 0.6882, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.5440, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.5356, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.5351, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.5445, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.5235, Train: 0.6765, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.5282, Train: 0.6941, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.5286, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.5247, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 068, Loss: 0.5089, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 069, Loss: 0.5239, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 070, Loss: 0.5257, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 071, Loss: 0.5350, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.5152, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.5204, Train: 0.7059, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.5071, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 075, Loss: 0.4988, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 076, Loss: 0.5248, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 077, Loss: 0.5128, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 078, Loss: 0.5162, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 079, Loss: 0.5088, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 080, Loss: 0.4956, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 081, Loss: 0.5128, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 082, Loss: 0.5157, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 083, Loss: 0.5036, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 084, Loss: 0.5029, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 085, Loss: 0.5009, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 086, Loss: 0.5199, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 087, Loss: 0.5078, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 088, Loss: 0.5057, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 089, Loss: 0.4992, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 090, Loss: 0.4999, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 091, Loss: 0.5108, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 092, Loss: 0.4853, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 093, Loss: 0.4909, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 094, Loss: 0.4891, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 095, Loss: 0.4896, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 096, Loss: 0.5025, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 097, Loss: 0.4789, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.4874, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 099, Loss: 0.4639, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 100, Loss: 0.4948, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 101, Loss: 0.4670, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 102, Loss: 0.4922, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 103, Loss: 0.4959, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 104, Loss: 0.4827, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 105, Loss: 0.4847, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 106, Loss: 0.4780, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.4778, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 108, Loss: 0.4829, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 109, Loss: 0.4770, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 110, Loss: 0.4749, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 111, Loss: 0.4841, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 112, Loss: 0.4641, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 113, Loss: 0.4801, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.4825, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 115, Loss: 0.4663, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 116, Loss: 0.4774, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 117, Loss: 0.4682, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.4775, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 119, Loss: 0.4698, Train: 0.7706, Test: 0.8889\n",
            "Epoch: 120, Loss: 0.4817, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 121, Loss: 0.4664, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 122, Loss: 0.4720, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 123, Loss: 0.4614, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 124, Loss: 0.4554, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 125, Loss: 0.4590, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 126, Loss: 0.4697, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 127, Loss: 0.4558, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 128, Loss: 0.4712, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 129, Loss: 0.4589, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 130, Loss: 0.4627, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 131, Loss: 0.4564, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 132, Loss: 0.4609, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 133, Loss: 0.4635, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 134, Loss: 0.4552, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 135, Loss: 0.4540, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 136, Loss: 0.4513, Train: 0.7588, Test: 0.9444\n",
            "Epoch: 137, Loss: 0.4697, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 138, Loss: 0.4686, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 139, Loss: 0.4631, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 140, Loss: 0.4623, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.4623, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.4652, Train: 0.7765, Test: 0.8889\n",
            "Epoch: 143, Loss: 0.4428, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 144, Loss: 0.4406, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 145, Loss: 0.4486, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 146, Loss: 0.4461, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 147, Loss: 0.4437, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 148, Loss: 0.4512, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 149, Loss: 0.4413, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 150, Loss: 0.4482, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 151, Loss: 0.4537, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 152, Loss: 0.4359, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 153, Loss: 0.4527, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 154, Loss: 0.4544, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 155, Loss: 0.4365, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 156, Loss: 0.4404, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 157, Loss: 0.4359, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 158, Loss: 0.4328, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 159, Loss: 0.4350, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 160, Loss: 0.4395, Train: 0.7647, Test: 0.9444\n",
            "Epoch: 161, Loss: 0.4313, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 162, Loss: 0.4436, Train: 0.7706, Test: 0.9444\n",
            "Epoch: 163, Loss: 0.4410, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 164, Loss: 0.4382, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 165, Loss: 0.4342, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 166, Loss: 0.4424, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 167, Loss: 0.4299, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 168, Loss: 0.4452, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 169, Loss: 0.4211, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 170, Loss: 0.4273, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 171, Loss: 0.4309, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 172, Loss: 0.4366, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 173, Loss: 0.4373, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 174, Loss: 0.4263, Train: 0.7765, Test: 0.9444\n",
            "Epoch: 175, Loss: 0.4252, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 176, Loss: 0.4251, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 177, Loss: 0.4307, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 178, Loss: 0.4218, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 179, Loss: 0.4129, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 180, Loss: 0.4247, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 181, Loss: 0.4121, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 182, Loss: 0.4316, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 183, Loss: 0.4229, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 184, Loss: 0.4112, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 185, Loss: 0.4184, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 186, Loss: 0.4199, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 187, Loss: 0.4262, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 188, Loss: 0.4149, Train: 0.7824, Test: 0.9444\n",
            "Epoch: 189, Loss: 0.4249, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 190, Loss: 0.4155, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 191, Loss: 0.4143, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 192, Loss: 0.4218, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 193, Loss: 0.4156, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 194, Loss: 0.4122, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 195, Loss: 0.4142, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 196, Loss: 0.4144, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 197, Loss: 0.4208, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 198, Loss: 0.4184, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 199, Loss: 0.4208, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 200, Loss: 0.4169, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 201, Loss: 0.4114, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 202, Loss: 0.4153, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 203, Loss: 0.4093, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 204, Loss: 0.4142, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 205, Loss: 0.4053, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 206, Loss: 0.4139, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 207, Loss: 0.4062, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 208, Loss: 0.3935, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 209, Loss: 0.4024, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 210, Loss: 0.4004, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 211, Loss: 0.4037, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 212, Loss: 0.4089, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 213, Loss: 0.4043, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 214, Loss: 0.4022, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 215, Loss: 0.4070, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 216, Loss: 0.3908, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 217, Loss: 0.4033, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 218, Loss: 0.4015, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 219, Loss: 0.4018, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 220, Loss: 0.4020, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 221, Loss: 0.3925, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 222, Loss: 0.3884, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 223, Loss: 0.3953, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 224, Loss: 0.3912, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 225, Loss: 0.4060, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 226, Loss: 0.3921, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 227, Loss: 0.3775, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 228, Loss: 0.3915, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 229, Loss: 0.3971, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 230, Loss: 0.4001, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 231, Loss: 0.3898, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 232, Loss: 0.3978, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.3823, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.3938, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.3902, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.3837, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 237, Loss: 0.3913, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 238, Loss: 0.3963, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 239, Loss: 0.3836, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 240, Loss: 0.3874, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 241, Loss: 0.3948, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 242, Loss: 0.3867, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 243, Loss: 0.3762, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 244, Loss: 0.3781, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 245, Loss: 0.3848, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 246, Loss: 0.3796, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 247, Loss: 0.3751, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 248, Loss: 0.3864, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 249, Loss: 0.3857, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 250, Loss: 0.3766, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 251, Loss: 0.3815, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.3870, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 253, Loss: 0.3786, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 254, Loss: 0.3728, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 255, Loss: 0.3798, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 256, Loss: 0.3812, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.6558, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 002, Loss: 0.6367, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 003, Loss: 0.6364, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 004, Loss: 0.6209, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 005, Loss: 0.6199, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 006, Loss: 0.6173, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 007, Loss: 0.6062, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 008, Loss: 0.6180, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 009, Loss: 0.6017, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 010, Loss: 0.6237, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 011, Loss: 0.6110, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 012, Loss: 0.5864, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 013, Loss: 0.5867, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 014, Loss: 0.5925, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 015, Loss: 0.5932, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 016, Loss: 0.5809, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 017, Loss: 0.5793, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 018, Loss: 0.5729, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 019, Loss: 0.5833, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 020, Loss: 0.5767, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 021, Loss: 0.5622, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 022, Loss: 0.5708, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 023, Loss: 0.5689, Train: 0.6706, Test: 0.6111\n",
            "Epoch: 024, Loss: 0.5821, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 025, Loss: 0.5781, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 026, Loss: 0.5552, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 027, Loss: 0.5574, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 028, Loss: 0.5766, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 029, Loss: 0.5764, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 030, Loss: 0.5573, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 031, Loss: 0.5664, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 032, Loss: 0.5544, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 033, Loss: 0.5527, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 034, Loss: 0.5439, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 035, Loss: 0.5368, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 036, Loss: 0.5531, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 037, Loss: 0.5333, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 038, Loss: 0.5498, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 039, Loss: 0.5372, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 040, Loss: 0.5300, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 041, Loss: 0.5485, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 042, Loss: 0.5247, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 043, Loss: 0.5264, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 044, Loss: 0.5323, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 045, Loss: 0.5288, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 046, Loss: 0.5165, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 047, Loss: 0.5130, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 048, Loss: 0.5124, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 049, Loss: 0.5001, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 050, Loss: 0.5217, Train: 0.7235, Test: 0.6111\n",
            "Epoch: 051, Loss: 0.4953, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 052, Loss: 0.4987, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 053, Loss: 0.4942, Train: 0.7235, Test: 0.6111\n",
            "Epoch: 054, Loss: 0.4973, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 055, Loss: 0.5029, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 056, Loss: 0.4991, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 057, Loss: 0.5038, Train: 0.7294, Test: 0.6111\n",
            "Epoch: 058, Loss: 0.4970, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 059, Loss: 0.4856, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 060, Loss: 0.4908, Train: 0.7294, Test: 0.5000\n",
            "Epoch: 061, Loss: 0.4863, Train: 0.7294, Test: 0.5000\n",
            "Epoch: 062, Loss: 0.4928, Train: 0.7353, Test: 0.5000\n",
            "Epoch: 063, Loss: 0.4794, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 064, Loss: 0.4832, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 065, Loss: 0.4794, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 066, Loss: 0.4894, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 067, Loss: 0.4757, Train: 0.7353, Test: 0.5000\n",
            "Epoch: 068, Loss: 0.4861, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 069, Loss: 0.4893, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 070, Loss: 0.4913, Train: 0.7353, Test: 0.5000\n",
            "Epoch: 071, Loss: 0.4728, Train: 0.7353, Test: 0.5000\n",
            "Epoch: 072, Loss: 0.4787, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 073, Loss: 0.4754, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 074, Loss: 0.4759, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 075, Loss: 0.4839, Train: 0.7412, Test: 0.5000\n",
            "Epoch: 076, Loss: 0.4790, Train: 0.7353, Test: 0.5000\n",
            "Epoch: 077, Loss: 0.4707, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 078, Loss: 0.4807, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 079, Loss: 0.4664, Train: 0.7353, Test: 0.5556\n",
            "Epoch: 080, Loss: 0.4676, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 081, Loss: 0.4648, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 082, Loss: 0.4678, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 083, Loss: 0.4670, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 084, Loss: 0.4713, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 085, Loss: 0.4655, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 086, Loss: 0.4645, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 087, Loss: 0.4559, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 088, Loss: 0.4603, Train: 0.7529, Test: 0.5556\n",
            "Epoch: 089, Loss: 0.4624, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 090, Loss: 0.4669, Train: 0.7529, Test: 0.5556\n",
            "Epoch: 091, Loss: 0.4513, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 092, Loss: 0.4607, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 093, Loss: 0.4714, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 094, Loss: 0.4478, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 095, Loss: 0.4557, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 096, Loss: 0.4594, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 097, Loss: 0.4589, Train: 0.7412, Test: 0.6111\n",
            "Epoch: 098, Loss: 0.4654, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 099, Loss: 0.4553, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 100, Loss: 0.4455, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 101, Loss: 0.4498, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 102, Loss: 0.4529, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 103, Loss: 0.4508, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 104, Loss: 0.4524, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 105, Loss: 0.4497, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 106, Loss: 0.4388, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 107, Loss: 0.4406, Train: 0.7471, Test: 0.6111\n",
            "Epoch: 108, Loss: 0.4395, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 109, Loss: 0.4514, Train: 0.7529, Test: 0.6111\n",
            "Epoch: 110, Loss: 0.4564, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 111, Loss: 0.4325, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 112, Loss: 0.4448, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 113, Loss: 0.4450, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 114, Loss: 0.4431, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 115, Loss: 0.4467, Train: 0.7588, Test: 0.6111\n",
            "Epoch: 116, Loss: 0.4396, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 117, Loss: 0.4397, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 118, Loss: 0.4277, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 119, Loss: 0.4300, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 120, Loss: 0.4393, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 121, Loss: 0.4298, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 122, Loss: 0.4432, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 123, Loss: 0.4357, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 124, Loss: 0.4392, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 125, Loss: 0.4410, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 126, Loss: 0.4381, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 127, Loss: 0.4336, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 128, Loss: 0.4247, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 129, Loss: 0.4421, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 130, Loss: 0.4325, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 131, Loss: 0.4175, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 132, Loss: 0.4348, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 133, Loss: 0.4284, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 134, Loss: 0.4243, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 135, Loss: 0.4272, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 136, Loss: 0.4265, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 137, Loss: 0.4320, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 138, Loss: 0.4348, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 139, Loss: 0.4299, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 140, Loss: 0.4153, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 141, Loss: 0.4182, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 142, Loss: 0.4187, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 143, Loss: 0.4172, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 144, Loss: 0.4184, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 145, Loss: 0.4122, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 146, Loss: 0.4382, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 147, Loss: 0.4298, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 148, Loss: 0.4105, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 149, Loss: 0.4053, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 150, Loss: 0.4200, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 151, Loss: 0.4120, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 152, Loss: 0.4210, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 153, Loss: 0.4074, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 154, Loss: 0.4169, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 155, Loss: 0.4074, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 156, Loss: 0.3965, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 157, Loss: 0.4014, Train: 0.8000, Test: 0.7222\n",
            "Epoch: 158, Loss: 0.4038, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 159, Loss: 0.4166, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 160, Loss: 0.4116, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 161, Loss: 0.4123, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 162, Loss: 0.4030, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 163, Loss: 0.3993, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 164, Loss: 0.4069, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 165, Loss: 0.4088, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 166, Loss: 0.4041, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 167, Loss: 0.3984, Train: 0.7941, Test: 0.7222\n",
            "Epoch: 168, Loss: 0.3972, Train: 0.7882, Test: 0.7222\n",
            "Epoch: 169, Loss: 0.3948, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 170, Loss: 0.4015, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 171, Loss: 0.4065, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 172, Loss: 0.4037, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 173, Loss: 0.4019, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 174, Loss: 0.4034, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 175, Loss: 0.3968, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 176, Loss: 0.4063, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 177, Loss: 0.4062, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 178, Loss: 0.3886, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 179, Loss: 0.4030, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 180, Loss: 0.3883, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 181, Loss: 0.3928, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 182, Loss: 0.3873, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 183, Loss: 0.3942, Train: 0.8059, Test: 0.7222\n",
            "Epoch: 184, Loss: 0.3903, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 185, Loss: 0.3940, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 186, Loss: 0.3978, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 187, Loss: 0.3974, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 188, Loss: 0.3892, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 189, Loss: 0.3814, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 190, Loss: 0.3816, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 191, Loss: 0.3932, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 192, Loss: 0.3694, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 193, Loss: 0.3716, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 194, Loss: 0.3763, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 195, Loss: 0.3733, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 196, Loss: 0.3777, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 197, Loss: 0.3863, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 198, Loss: 0.3904, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 199, Loss: 0.3715, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 200, Loss: 0.3822, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 201, Loss: 0.3743, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 202, Loss: 0.3849, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 203, Loss: 0.3745, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 204, Loss: 0.3649, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 205, Loss: 0.3795, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 206, Loss: 0.3669, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 207, Loss: 0.3724, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 208, Loss: 0.3708, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 209, Loss: 0.3703, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 210, Loss: 0.3657, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 211, Loss: 0.3677, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 212, Loss: 0.3683, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 213, Loss: 0.3726, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 214, Loss: 0.3675, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 215, Loss: 0.3643, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 216, Loss: 0.3663, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 217, Loss: 0.3668, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 218, Loss: 0.3700, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 219, Loss: 0.3689, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 220, Loss: 0.3659, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 221, Loss: 0.3669, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 222, Loss: 0.3589, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 223, Loss: 0.3606, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 224, Loss: 0.3537, Train: 0.8235, Test: 0.7222\n",
            "Epoch: 225, Loss: 0.3562, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 226, Loss: 0.3599, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 227, Loss: 0.3620, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 228, Loss: 0.3607, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 229, Loss: 0.3535, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 230, Loss: 0.3496, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 231, Loss: 0.3623, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 232, Loss: 0.3655, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 233, Loss: 0.3512, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 234, Loss: 0.3637, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 235, Loss: 0.3563, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 236, Loss: 0.3459, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 237, Loss: 0.3434, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 238, Loss: 0.3540, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 239, Loss: 0.3511, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 240, Loss: 0.3540, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 241, Loss: 0.3533, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 242, Loss: 0.3388, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 243, Loss: 0.3457, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 244, Loss: 0.3452, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 245, Loss: 0.3472, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 246, Loss: 0.3459, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 247, Loss: 0.3409, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 248, Loss: 0.3482, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 249, Loss: 0.3528, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 250, Loss: 0.3497, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 251, Loss: 0.3482, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 252, Loss: 0.3472, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 253, Loss: 0.3369, Train: 0.8118, Test: 0.7222\n",
            "Epoch: 254, Loss: 0.3366, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 255, Loss: 0.3299, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.3391, Train: 0.8176, Test: 0.7222\n",
            "Epoch: 001, Loss: 0.8694, Train: 0.3235, Test: 0.4444\n",
            "Epoch: 002, Loss: 0.8158, Train: 0.3235, Test: 0.4444\n",
            "Epoch: 003, Loss: 0.7873, Train: 0.3235, Test: 0.4444\n",
            "Epoch: 004, Loss: 0.7574, Train: 0.3118, Test: 0.4444\n",
            "Epoch: 005, Loss: 0.7118, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 006, Loss: 0.7042, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 007, Loss: 0.6742, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 008, Loss: 0.6512, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 009, Loss: 0.6577, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 010, Loss: 0.6573, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 011, Loss: 0.6237, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 012, Loss: 0.6164, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 013, Loss: 0.6309, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 014, Loss: 0.6141, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 015, Loss: 0.6159, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 016, Loss: 0.6173, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 017, Loss: 0.6230, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 018, Loss: 0.6317, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 019, Loss: 0.5980, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 020, Loss: 0.5856, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 021, Loss: 0.6169, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 022, Loss: 0.5808, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 023, Loss: 0.6080, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 024, Loss: 0.5996, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 025, Loss: 0.6078, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 026, Loss: 0.5758, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 027, Loss: 0.6016, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 028, Loss: 0.5672, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 029, Loss: 0.5961, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 030, Loss: 0.5934, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 031, Loss: 0.5744, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 032, Loss: 0.5871, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 033, Loss: 0.5814, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 034, Loss: 0.5813, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 035, Loss: 0.5721, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 036, Loss: 0.5946, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 037, Loss: 0.5779, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 038, Loss: 0.5778, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 039, Loss: 0.5749, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 040, Loss: 0.5632, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 041, Loss: 0.5804, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 042, Loss: 0.5589, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 043, Loss: 0.5599, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 044, Loss: 0.5944, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 045, Loss: 0.5799, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 046, Loss: 0.5649, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 047, Loss: 0.5776, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 048, Loss: 0.5803, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 049, Loss: 0.5619, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 050, Loss: 0.5791, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 051, Loss: 0.5887, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 052, Loss: 0.5497, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 053, Loss: 0.5707, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 054, Loss: 0.5501, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 055, Loss: 0.5584, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 056, Loss: 0.5431, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 057, Loss: 0.5443, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 058, Loss: 0.5371, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 059, Loss: 0.5559, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 060, Loss: 0.5439, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 061, Loss: 0.5624, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 062, Loss: 0.5571, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 063, Loss: 0.5435, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 064, Loss: 0.5331, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 065, Loss: 0.5486, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 066, Loss: 0.5355, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 067, Loss: 0.5363, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 068, Loss: 0.5277, Train: 0.6882, Test: 0.5556\n",
            "Epoch: 069, Loss: 0.5310, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 070, Loss: 0.5483, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 071, Loss: 0.5291, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 072, Loss: 0.5143, Train: 0.6941, Test: 0.5556\n",
            "Epoch: 073, Loss: 0.5387, Train: 0.6882, Test: 0.6111\n",
            "Epoch: 074, Loss: 0.5218, Train: 0.7000, Test: 0.6111\n",
            "Epoch: 075, Loss: 0.5127, Train: 0.7118, Test: 0.6111\n",
            "Epoch: 076, Loss: 0.5292, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 077, Loss: 0.5188, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 078, Loss: 0.5232, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 079, Loss: 0.5139, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 080, Loss: 0.5250, Train: 0.7059, Test: 0.6111\n",
            "Epoch: 081, Loss: 0.5012, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 082, Loss: 0.5186, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 083, Loss: 0.4975, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 084, Loss: 0.5149, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 085, Loss: 0.5141, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 086, Loss: 0.5009, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 087, Loss: 0.5036, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 088, Loss: 0.4930, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 089, Loss: 0.4911, Train: 0.7176, Test: 0.6667\n",
            "Epoch: 090, Loss: 0.5044, Train: 0.7176, Test: 0.6667\n",
            "Epoch: 091, Loss: 0.4968, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 092, Loss: 0.4915, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 093, Loss: 0.4803, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 094, Loss: 0.4864, Train: 0.7059, Test: 0.7222\n",
            "Epoch: 095, Loss: 0.4805, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 096, Loss: 0.4909, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 097, Loss: 0.4925, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 098, Loss: 0.4881, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 099, Loss: 0.4949, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 100, Loss: 0.4775, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 101, Loss: 0.4825, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 102, Loss: 0.4721, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 103, Loss: 0.4767, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 104, Loss: 0.4713, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 105, Loss: 0.4819, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 106, Loss: 0.4731, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 107, Loss: 0.4588, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 108, Loss: 0.4810, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 109, Loss: 0.4906, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 110, Loss: 0.4743, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 111, Loss: 0.4766, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 112, Loss: 0.4559, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 113, Loss: 0.4598, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 114, Loss: 0.4646, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 115, Loss: 0.4694, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 116, Loss: 0.4753, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 117, Loss: 0.4686, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 118, Loss: 0.4688, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 119, Loss: 0.4600, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 120, Loss: 0.4611, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 121, Loss: 0.4525, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 122, Loss: 0.4429, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 123, Loss: 0.4535, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 124, Loss: 0.4469, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 125, Loss: 0.4613, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 126, Loss: 0.4590, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 127, Loss: 0.4530, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 128, Loss: 0.4498, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 129, Loss: 0.4510, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 130, Loss: 0.4440, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 131, Loss: 0.4615, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 132, Loss: 0.4329, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 133, Loss: 0.4462, Train: 0.7647, Test: 0.6111\n",
            "Epoch: 134, Loss: 0.4467, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 135, Loss: 0.4592, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 136, Loss: 0.4427, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 137, Loss: 0.4355, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 138, Loss: 0.4401, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 139, Loss: 0.4525, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 140, Loss: 0.4389, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 141, Loss: 0.4370, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 142, Loss: 0.4377, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 143, Loss: 0.4481, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 144, Loss: 0.4457, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 145, Loss: 0.4294, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 146, Loss: 0.4439, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 147, Loss: 0.4365, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 148, Loss: 0.4328, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 149, Loss: 0.4278, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 150, Loss: 0.4266, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 151, Loss: 0.4368, Train: 0.7706, Test: 0.6111\n",
            "Epoch: 152, Loss: 0.4237, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 153, Loss: 0.4266, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 154, Loss: 0.4245, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 155, Loss: 0.4202, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 156, Loss: 0.4230, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 157, Loss: 0.4189, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 158, Loss: 0.4325, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 159, Loss: 0.4255, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 160, Loss: 0.4134, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 161, Loss: 0.4244, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 162, Loss: 0.4218, Train: 0.7765, Test: 0.6111\n",
            "Epoch: 163, Loss: 0.4234, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 164, Loss: 0.4142, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 165, Loss: 0.4292, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 166, Loss: 0.4278, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 167, Loss: 0.4130, Train: 0.7824, Test: 0.6111\n",
            "Epoch: 168, Loss: 0.4098, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 169, Loss: 0.4179, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 170, Loss: 0.4185, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 171, Loss: 0.4060, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 172, Loss: 0.4058, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 173, Loss: 0.4194, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 174, Loss: 0.4074, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 175, Loss: 0.4099, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 176, Loss: 0.4150, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 177, Loss: 0.4024, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 178, Loss: 0.4092, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 179, Loss: 0.4106, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 180, Loss: 0.4129, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 181, Loss: 0.4032, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 182, Loss: 0.4014, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 183, Loss: 0.3999, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 184, Loss: 0.3946, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 185, Loss: 0.4137, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 186, Loss: 0.4012, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 187, Loss: 0.4032, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 188, Loss: 0.4082, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 189, Loss: 0.3992, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 190, Loss: 0.4005, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 191, Loss: 0.4064, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 192, Loss: 0.3894, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 193, Loss: 0.3965, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 194, Loss: 0.3964, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 195, Loss: 0.3949, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 196, Loss: 0.4025, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 197, Loss: 0.3967, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 198, Loss: 0.3901, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 199, Loss: 0.3907, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 200, Loss: 0.3898, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 201, Loss: 0.3916, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 202, Loss: 0.3959, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 203, Loss: 0.3734, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 204, Loss: 0.3866, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 205, Loss: 0.3865, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 206, Loss: 0.3770, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 207, Loss: 0.3892, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 208, Loss: 0.3847, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 209, Loss: 0.3887, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 210, Loss: 0.3860, Train: 0.7882, Test: 0.6111\n",
            "Epoch: 211, Loss: 0.3783, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 212, Loss: 0.3844, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 213, Loss: 0.3819, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 214, Loss: 0.3753, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 215, Loss: 0.3867, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 216, Loss: 0.3736, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 217, Loss: 0.3794, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 218, Loss: 0.3755, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 219, Loss: 0.3679, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 220, Loss: 0.3732, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 221, Loss: 0.3635, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 222, Loss: 0.3729, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 223, Loss: 0.3668, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 224, Loss: 0.3689, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 225, Loss: 0.3633, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 226, Loss: 0.3686, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 227, Loss: 0.3672, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 228, Loss: 0.3614, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 229, Loss: 0.3732, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 230, Loss: 0.3585, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 231, Loss: 0.3676, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 232, Loss: 0.3599, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 233, Loss: 0.3762, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 234, Loss: 0.3615, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 235, Loss: 0.3598, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 236, Loss: 0.3634, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 237, Loss: 0.3642, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 238, Loss: 0.3634, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 239, Loss: 0.3703, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 240, Loss: 0.3574, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 241, Loss: 0.3555, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 242, Loss: 0.3628, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 243, Loss: 0.3520, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 244, Loss: 0.3598, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 245, Loss: 0.3507, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 246, Loss: 0.3577, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 247, Loss: 0.3496, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 248, Loss: 0.3471, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 249, Loss: 0.3451, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 250, Loss: 0.3422, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 251, Loss: 0.3471, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.3600, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 253, Loss: 0.3450, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 254, Loss: 0.3643, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 255, Loss: 0.3453, Train: 0.8294, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.3439, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 001, Loss: 1.0585, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 002, Loss: 0.9549, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 003, Loss: 0.9001, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 004, Loss: 0.8353, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 005, Loss: 0.7739, Train: 0.3176, Test: 0.5000\n",
            "Epoch: 006, Loss: 0.7427, Train: 0.3176, Test: 0.4444\n",
            "Epoch: 007, Loss: 0.7221, Train: 0.7941, Test: 0.6111\n",
            "Epoch: 008, Loss: 0.6994, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 009, Loss: 0.6834, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 010, Loss: 0.6594, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 011, Loss: 0.6585, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 012, Loss: 0.6251, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 013, Loss: 0.6075, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 014, Loss: 0.6073, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 015, Loss: 0.5901, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 016, Loss: 0.5985, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 017, Loss: 0.5899, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 018, Loss: 0.6113, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 019, Loss: 0.5993, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 020, Loss: 0.5995, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 021, Loss: 0.5759, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 022, Loss: 0.6221, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 023, Loss: 0.5985, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 024, Loss: 0.6129, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 025, Loss: 0.5935, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 026, Loss: 0.5700, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 027, Loss: 0.5927, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 028, Loss: 0.5977, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 029, Loss: 0.5922, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 030, Loss: 0.5869, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 031, Loss: 0.5723, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 032, Loss: 0.5913, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 033, Loss: 0.5933, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 034, Loss: 0.5728, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 035, Loss: 0.5995, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 036, Loss: 0.5877, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 037, Loss: 0.5786, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 038, Loss: 0.5584, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 039, Loss: 0.5803, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 040, Loss: 0.5702, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 041, Loss: 0.5658, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 042, Loss: 0.5805, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 043, Loss: 0.5703, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 044, Loss: 0.5648, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 045, Loss: 0.5538, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 046, Loss: 0.5653, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 047, Loss: 0.5690, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 048, Loss: 0.5813, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 049, Loss: 0.5524, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 050, Loss: 0.5797, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 051, Loss: 0.5493, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 052, Loss: 0.5414, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 053, Loss: 0.5415, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 054, Loss: 0.5386, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 055, Loss: 0.5674, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 056, Loss: 0.5165, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 057, Loss: 0.5557, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 058, Loss: 0.5187, Train: 0.6824, Test: 0.5000\n",
            "Epoch: 059, Loss: 0.5333, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 060, Loss: 0.5446, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 061, Loss: 0.5541, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 062, Loss: 0.5246, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 063, Loss: 0.5356, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 064, Loss: 0.5239, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 065, Loss: 0.5140, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 066, Loss: 0.5405, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 067, Loss: 0.5276, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 068, Loss: 0.5436, Train: 0.6765, Test: 0.5556\n",
            "Epoch: 069, Loss: 0.5525, Train: 0.6824, Test: 0.5556\n",
            "Epoch: 070, Loss: 0.5393, Train: 0.6824, Test: 0.6111\n",
            "Epoch: 071, Loss: 0.5257, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 072, Loss: 0.5244, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 073, Loss: 0.5403, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 074, Loss: 0.5124, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 075, Loss: 0.5233, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 076, Loss: 0.5170, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 077, Loss: 0.5393, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 078, Loss: 0.4990, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 079, Loss: 0.5144, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 080, Loss: 0.5047, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 081, Loss: 0.5125, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 082, Loss: 0.5005, Train: 0.6765, Test: 0.6111\n",
            "Epoch: 083, Loss: 0.5122, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 084, Loss: 0.5094, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 085, Loss: 0.5227, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 086, Loss: 0.5048, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 087, Loss: 0.4962, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 088, Loss: 0.5002, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 089, Loss: 0.4851, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 090, Loss: 0.5039, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 091, Loss: 0.4937, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 092, Loss: 0.5009, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 093, Loss: 0.4976, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 094, Loss: 0.4973, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 095, Loss: 0.4910, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 096, Loss: 0.4901, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 097, Loss: 0.4932, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 098, Loss: 0.4785, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 099, Loss: 0.4797, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 100, Loss: 0.4959, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 101, Loss: 0.4840, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 102, Loss: 0.4759, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 103, Loss: 0.4930, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 104, Loss: 0.5019, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 105, Loss: 0.4871, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 106, Loss: 0.4730, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 107, Loss: 0.4867, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 108, Loss: 0.4955, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 109, Loss: 0.4799, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 110, Loss: 0.4736, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 111, Loss: 0.4646, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 112, Loss: 0.4838, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 113, Loss: 0.4801, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.4748, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 115, Loss: 0.4845, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 116, Loss: 0.4787, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 117, Loss: 0.4678, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 118, Loss: 0.4632, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 119, Loss: 0.4652, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 120, Loss: 0.4703, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 121, Loss: 0.4751, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 122, Loss: 0.4772, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 123, Loss: 0.4541, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 124, Loss: 0.4695, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 125, Loss: 0.4602, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 126, Loss: 0.4503, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 127, Loss: 0.4626, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 128, Loss: 0.4698, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 129, Loss: 0.4692, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 130, Loss: 0.4610, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 131, Loss: 0.4647, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 132, Loss: 0.4730, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 133, Loss: 0.4648, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 134, Loss: 0.4627, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 135, Loss: 0.4618, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 136, Loss: 0.4704, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 137, Loss: 0.4731, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 138, Loss: 0.4532, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 139, Loss: 0.4622, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 140, Loss: 0.4562, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 141, Loss: 0.4536, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 142, Loss: 0.4570, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 143, Loss: 0.4607, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 144, Loss: 0.4531, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 145, Loss: 0.4561, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 146, Loss: 0.4587, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 147, Loss: 0.4503, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 148, Loss: 0.4480, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 149, Loss: 0.4554, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 150, Loss: 0.4632, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 151, Loss: 0.4510, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 152, Loss: 0.4515, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 153, Loss: 0.4594, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 154, Loss: 0.4430, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 155, Loss: 0.4437, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 156, Loss: 0.4384, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 157, Loss: 0.4549, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 158, Loss: 0.4555, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 159, Loss: 0.4579, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 160, Loss: 0.4440, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 161, Loss: 0.4559, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 162, Loss: 0.4369, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 163, Loss: 0.4437, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 164, Loss: 0.4567, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 165, Loss: 0.4513, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 166, Loss: 0.4529, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 167, Loss: 0.4613, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 168, Loss: 0.4307, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 169, Loss: 0.4403, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 170, Loss: 0.4479, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 171, Loss: 0.4434, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 172, Loss: 0.4446, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 173, Loss: 0.4387, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 174, Loss: 0.4422, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 175, Loss: 0.4349, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 176, Loss: 0.4506, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 177, Loss: 0.4475, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 178, Loss: 0.4460, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 179, Loss: 0.4478, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 180, Loss: 0.4260, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 181, Loss: 0.4370, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 182, Loss: 0.4338, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 183, Loss: 0.4327, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 184, Loss: 0.4440, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 185, Loss: 0.4400, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 186, Loss: 0.4326, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 187, Loss: 0.4415, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 188, Loss: 0.4432, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 189, Loss: 0.4300, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 190, Loss: 0.4360, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 191, Loss: 0.4445, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 192, Loss: 0.4361, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 193, Loss: 0.4283, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 194, Loss: 0.4291, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 195, Loss: 0.4291, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 196, Loss: 0.4338, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 197, Loss: 0.4256, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 198, Loss: 0.4306, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 199, Loss: 0.4280, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 200, Loss: 0.4375, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 201, Loss: 0.4266, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 202, Loss: 0.4182, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 203, Loss: 0.4362, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 204, Loss: 0.4188, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 205, Loss: 0.4167, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 206, Loss: 0.4343, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 207, Loss: 0.4290, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 208, Loss: 0.4174, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 209, Loss: 0.4285, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 210, Loss: 0.4179, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 211, Loss: 0.4217, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 212, Loss: 0.4246, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 213, Loss: 0.4304, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 214, Loss: 0.4214, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 215, Loss: 0.4096, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 216, Loss: 0.4187, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 217, Loss: 0.4053, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 218, Loss: 0.4183, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 219, Loss: 0.4196, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 220, Loss: 0.4130, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 221, Loss: 0.4173, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 222, Loss: 0.4266, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 223, Loss: 0.4191, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 224, Loss: 0.4183, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 225, Loss: 0.4180, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 226, Loss: 0.4171, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 227, Loss: 0.4177, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 228, Loss: 0.4002, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 229, Loss: 0.4094, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 230, Loss: 0.4053, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 231, Loss: 0.4170, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 232, Loss: 0.4144, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 233, Loss: 0.4129, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 234, Loss: 0.4116, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 235, Loss: 0.4202, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 236, Loss: 0.4138, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 237, Loss: 0.4081, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 238, Loss: 0.4190, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 239, Loss: 0.4147, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 240, Loss: 0.4221, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 241, Loss: 0.4080, Train: 0.7882, Test: 0.7778\n",
            "Epoch: 242, Loss: 0.4110, Train: 0.7882, Test: 0.7778\n",
            "Epoch: 243, Loss: 0.4026, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 244, Loss: 0.4013, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 245, Loss: 0.4135, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 246, Loss: 0.4040, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 247, Loss: 0.3973, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 248, Loss: 0.4058, Train: 0.7882, Test: 0.7778\n",
            "Epoch: 249, Loss: 0.3975, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 250, Loss: 0.4106, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 251, Loss: 0.4036, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 252, Loss: 0.3953, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 253, Loss: 0.4033, Train: 0.8000, Test: 0.7778\n",
            "Epoch: 254, Loss: 0.3942, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 255, Loss: 0.3982, Train: 0.8059, Test: 0.7778\n",
            "Epoch: 256, Loss: 0.3977, Train: 0.7824, Test: 0.7778\n",
            "Epoch: 001, Loss: 0.6409, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 002, Loss: 0.6432, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.6232, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6319, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6413, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6255, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6130, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.6196, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.6329, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.6329, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.6172, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.6229, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 013, Loss: 0.6215, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.6241, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 015, Loss: 0.6199, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 016, Loss: 0.6037, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 017, Loss: 0.6211, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 018, Loss: 0.6095, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 019, Loss: 0.6102, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 020, Loss: 0.6060, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 021, Loss: 0.5805, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 022, Loss: 0.5955, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 023, Loss: 0.6004, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 024, Loss: 0.6033, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 025, Loss: 0.5917, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 026, Loss: 0.5814, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 027, Loss: 0.5955, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 028, Loss: 0.5836, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.5881, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.5748, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.5762, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.5906, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.5656, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.5777, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.5760, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.5731, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.5670, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.5602, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.5599, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.5669, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.5502, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.5576, Train: 0.6647, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.5599, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.5537, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.5494, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.5449, Train: 0.6765, Test: 0.8333\n",
            "Epoch: 047, Loss: 0.5486, Train: 0.6765, Test: 0.8333\n",
            "Epoch: 048, Loss: 0.5496, Train: 0.6824, Test: 0.8333\n",
            "Epoch: 049, Loss: 0.5469, Train: 0.6824, Test: 0.8333\n",
            "Epoch: 050, Loss: 0.5372, Train: 0.6824, Test: 0.8333\n",
            "Epoch: 051, Loss: 0.5444, Train: 0.6941, Test: 0.8333\n",
            "Epoch: 052, Loss: 0.5350, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 053, Loss: 0.5385, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 054, Loss: 0.5443, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 055, Loss: 0.5410, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 056, Loss: 0.5374, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 057, Loss: 0.5351, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 058, Loss: 0.5171, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 059, Loss: 0.5272, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 060, Loss: 0.5276, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 061, Loss: 0.5210, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 062, Loss: 0.5247, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.5283, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 064, Loss: 0.5144, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 065, Loss: 0.5102, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 066, Loss: 0.5156, Train: 0.7118, Test: 0.7778\n",
            "Epoch: 067, Loss: 0.5084, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 068, Loss: 0.5146, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 069, Loss: 0.5090, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 070, Loss: 0.5128, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 071, Loss: 0.5106, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 072, Loss: 0.4991, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 073, Loss: 0.4986, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 074, Loss: 0.4876, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 075, Loss: 0.4914, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 076, Loss: 0.5072, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 077, Loss: 0.4954, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 078, Loss: 0.5131, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 079, Loss: 0.5015, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 080, Loss: 0.4889, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 081, Loss: 0.4931, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 082, Loss: 0.5096, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 083, Loss: 0.4865, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 084, Loss: 0.5053, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 085, Loss: 0.4847, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 086, Loss: 0.4982, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 087, Loss: 0.4919, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 088, Loss: 0.4802, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 089, Loss: 0.4732, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 090, Loss: 0.4793, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 091, Loss: 0.4798, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 092, Loss: 0.4710, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 093, Loss: 0.4849, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 094, Loss: 0.4849, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 095, Loss: 0.4771, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 096, Loss: 0.4599, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 097, Loss: 0.4826, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 098, Loss: 0.4763, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 099, Loss: 0.4799, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 100, Loss: 0.4700, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 101, Loss: 0.4696, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 102, Loss: 0.4747, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 103, Loss: 0.4640, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 104, Loss: 0.4671, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 105, Loss: 0.4761, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 106, Loss: 0.4845, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 107, Loss: 0.4664, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 108, Loss: 0.4629, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 109, Loss: 0.4708, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 110, Loss: 0.4632, Train: 0.7412, Test: 0.7222\n",
            "Epoch: 111, Loss: 0.4680, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 112, Loss: 0.4688, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 113, Loss: 0.4664, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.4534, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 115, Loss: 0.4688, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 116, Loss: 0.4686, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 117, Loss: 0.4712, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 118, Loss: 0.4606, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 119, Loss: 0.4544, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 120, Loss: 0.4630, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 121, Loss: 0.4621, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 122, Loss: 0.4709, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 123, Loss: 0.4552, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 124, Loss: 0.4523, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 125, Loss: 0.4605, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 126, Loss: 0.4494, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 127, Loss: 0.4559, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 128, Loss: 0.4427, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 129, Loss: 0.4548, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 130, Loss: 0.4589, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 131, Loss: 0.4490, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 132, Loss: 0.4486, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 133, Loss: 0.4556, Train: 0.7647, Test: 0.7778\n",
            "Epoch: 134, Loss: 0.4486, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 135, Loss: 0.4557, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 136, Loss: 0.4443, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 137, Loss: 0.4439, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 138, Loss: 0.4492, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 139, Loss: 0.4502, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.4518, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 141, Loss: 0.4547, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.4495, Train: 0.7824, Test: 0.7778\n",
            "Epoch: 143, Loss: 0.4581, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 144, Loss: 0.4434, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 145, Loss: 0.4423, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 146, Loss: 0.4445, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 147, Loss: 0.4446, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.4378, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.4420, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 150, Loss: 0.4446, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 151, Loss: 0.4352, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 152, Loss: 0.4330, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 153, Loss: 0.4356, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 154, Loss: 0.4416, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 155, Loss: 0.4462, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 156, Loss: 0.4303, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 157, Loss: 0.4196, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.4302, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 159, Loss: 0.4379, Train: 0.7824, Test: 0.7778\n",
            "Epoch: 160, Loss: 0.4282, Train: 0.7706, Test: 0.7778\n",
            "Epoch: 161, Loss: 0.4368, Train: 0.7765, Test: 0.7778\n",
            "Epoch: 162, Loss: 0.4200, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.4357, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.4399, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.4294, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 166, Loss: 0.4274, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 167, Loss: 0.4260, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 168, Loss: 0.4421, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 169, Loss: 0.4345, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 170, Loss: 0.4284, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 171, Loss: 0.4386, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 172, Loss: 0.4307, Train: 0.7824, Test: 0.7778\n",
            "Epoch: 173, Loss: 0.4228, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 174, Loss: 0.4261, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 175, Loss: 0.4210, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.4285, Train: 0.7882, Test: 0.7778\n",
            "Epoch: 177, Loss: 0.4300, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.4268, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 179, Loss: 0.4344, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 180, Loss: 0.4117, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 181, Loss: 0.4215, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 182, Loss: 0.4156, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 183, Loss: 0.4216, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 184, Loss: 0.4133, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 185, Loss: 0.4179, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 186, Loss: 0.4153, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 187, Loss: 0.4168, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.4194, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.4273, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.4171, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 191, Loss: 0.4120, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 192, Loss: 0.4192, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 193, Loss: 0.4184, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 194, Loss: 0.4223, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 195, Loss: 0.4223, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.4113, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.4090, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.4180, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.4139, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.4075, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.4016, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 202, Loss: 0.4179, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 203, Loss: 0.4165, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 204, Loss: 0.4154, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 205, Loss: 0.4029, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.4044, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.4148, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.4026, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.3993, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.4080, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.4116, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.3972, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.4077, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.4190, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.4040, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 216, Loss: 0.3962, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.3935, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.4066, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.3979, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.3977, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.3963, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.4013, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.4043, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.3929, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.4043, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.4061, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.3952, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.3947, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.3856, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.3887, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.3879, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.3953, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.3858, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.3860, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 235, Loss: 0.3857, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.3960, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.4016, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.3872, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.3881, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.3948, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.3825, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.3917, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.3894, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.3840, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.3833, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.3830, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.3863, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.3821, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.3889, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.3781, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 251, Loss: 0.3861, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.3831, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.3875, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.3799, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.3828, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.3698, Train: 0.8118, Test: 0.8333\n",
            "Epoch: 001, Loss: 0.8069, Train: 0.4471, Test: 0.1667\n",
            "Epoch: 002, Loss: 0.7408, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 003, Loss: 0.7326, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 004, Loss: 0.6937, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 005, Loss: 0.6550, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 006, Loss: 0.6741, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 007, Loss: 0.6993, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 008, Loss: 0.6598, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 009, Loss: 0.6755, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 010, Loss: 0.6396, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 011, Loss: 0.6461, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 012, Loss: 0.6582, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 013, Loss: 0.6217, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 014, Loss: 0.6448, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 015, Loss: 0.6329, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 016, Loss: 0.6291, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 017, Loss: 0.6817, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 018, Loss: 0.6320, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 019, Loss: 0.6312, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 020, Loss: 0.6492, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 021, Loss: 0.6473, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 022, Loss: 0.6223, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 023, Loss: 0.6610, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 024, Loss: 0.6348, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 025, Loss: 0.5901, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 026, Loss: 0.6303, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 027, Loss: 0.6103, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 028, Loss: 0.6233, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 029, Loss: 0.6158, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 030, Loss: 0.6064, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 031, Loss: 0.6210, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 032, Loss: 0.6222, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 033, Loss: 0.6211, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 034, Loss: 0.6317, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 035, Loss: 0.5946, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 036, Loss: 0.6117, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 037, Loss: 0.6057, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 038, Loss: 0.6062, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 039, Loss: 0.5966, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 040, Loss: 0.5796, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 041, Loss: 0.5932, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 042, Loss: 0.6005, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 043, Loss: 0.6059, Train: 0.6412, Test: 0.8889\n",
            "Epoch: 044, Loss: 0.5854, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 045, Loss: 0.6002, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 046, Loss: 0.5932, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 047, Loss: 0.5866, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 048, Loss: 0.5777, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 049, Loss: 0.5928, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 050, Loss: 0.5726, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 051, Loss: 0.5866, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 052, Loss: 0.5602, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 053, Loss: 0.5725, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 054, Loss: 0.5660, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 055, Loss: 0.6061, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 056, Loss: 0.5639, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 057, Loss: 0.5746, Train: 0.6529, Test: 0.8889\n",
            "Epoch: 058, Loss: 0.5777, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 059, Loss: 0.5696, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 060, Loss: 0.5519, Train: 0.6471, Test: 0.8889\n",
            "Epoch: 061, Loss: 0.5665, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 062, Loss: 0.5520, Train: 0.6706, Test: 0.8889\n",
            "Epoch: 063, Loss: 0.5391, Train: 0.6588, Test: 0.8889\n",
            "Epoch: 064, Loss: 0.5546, Train: 0.6765, Test: 0.8889\n",
            "Epoch: 065, Loss: 0.5540, Train: 0.6824, Test: 0.9444\n",
            "Epoch: 066, Loss: 0.5481, Train: 0.7059, Test: 0.9444\n",
            "Epoch: 067, Loss: 0.5578, Train: 0.7235, Test: 0.9444\n",
            "Epoch: 068, Loss: 0.5429, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 069, Loss: 0.5328, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 070, Loss: 0.5525, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 071, Loss: 0.5275, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 072, Loss: 0.5278, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 073, Loss: 0.5406, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 074, Loss: 0.5483, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 075, Loss: 0.5254, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 076, Loss: 0.5343, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 077, Loss: 0.5213, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 078, Loss: 0.5309, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 079, Loss: 0.5201, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 080, Loss: 0.5189, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 081, Loss: 0.5289, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 082, Loss: 0.5252, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 083, Loss: 0.5232, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 084, Loss: 0.5149, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 085, Loss: 0.5172, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 086, Loss: 0.5171, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 087, Loss: 0.5177, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 088, Loss: 0.5300, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 089, Loss: 0.5158, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 090, Loss: 0.5156, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 091, Loss: 0.5012, Train: 0.7471, Test: 0.9444\n",
            "Epoch: 092, Loss: 0.5045, Train: 0.7412, Test: 0.9444\n",
            "Epoch: 093, Loss: 0.5060, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 094, Loss: 0.5034, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 095, Loss: 0.4909, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 096, Loss: 0.4947, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 097, Loss: 0.5107, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 098, Loss: 0.5060, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 099, Loss: 0.4978, Train: 0.7294, Test: 0.9444\n",
            "Epoch: 100, Loss: 0.5147, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 101, Loss: 0.4982, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 102, Loss: 0.5083, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 103, Loss: 0.4905, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 104, Loss: 0.4900, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 105, Loss: 0.4883, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 106, Loss: 0.4947, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 107, Loss: 0.4875, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 108, Loss: 0.4925, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 109, Loss: 0.4752, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 110, Loss: 0.4821, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 111, Loss: 0.4702, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 112, Loss: 0.4949, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 113, Loss: 0.4952, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 114, Loss: 0.4689, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 115, Loss: 0.4870, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 116, Loss: 0.4627, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 117, Loss: 0.5004, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 118, Loss: 0.4939, Train: 0.7353, Test: 0.9444\n",
            "Epoch: 119, Loss: 0.4885, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 120, Loss: 0.4741, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 121, Loss: 0.4822, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 122, Loss: 0.4847, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 123, Loss: 0.4775, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 124, Loss: 0.4765, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 125, Loss: 0.4791, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 126, Loss: 0.4618, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 127, Loss: 0.4728, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 128, Loss: 0.4729, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 129, Loss: 0.4656, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 130, Loss: 0.4686, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 131, Loss: 0.4647, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 132, Loss: 0.4589, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 133, Loss: 0.4688, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 134, Loss: 0.4658, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 135, Loss: 0.4759, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 136, Loss: 0.4648, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 137, Loss: 0.4537, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 138, Loss: 0.4586, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 139, Loss: 0.4641, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 140, Loss: 0.4601, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 141, Loss: 0.4523, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 142, Loss: 0.4577, Train: 0.7471, Test: 1.0000\n",
            "Epoch: 143, Loss: 0.4640, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 144, Loss: 0.4562, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 145, Loss: 0.4600, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 146, Loss: 0.4493, Train: 0.7353, Test: 1.0000\n",
            "Epoch: 147, Loss: 0.4698, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 148, Loss: 0.4436, Train: 0.7529, Test: 1.0000\n",
            "Epoch: 149, Loss: 0.4458, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 150, Loss: 0.4463, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 151, Loss: 0.4535, Train: 0.7765, Test: 1.0000\n",
            "Epoch: 152, Loss: 0.4562, Train: 0.7588, Test: 1.0000\n",
            "Epoch: 153, Loss: 0.4551, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 154, Loss: 0.4434, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 155, Loss: 0.4593, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 156, Loss: 0.4409, Train: 0.7412, Test: 1.0000\n",
            "Epoch: 157, Loss: 0.4550, Train: 0.7588, Test: 1.0000\n",
            "Epoch: 158, Loss: 0.4559, Train: 0.7588, Test: 1.0000\n",
            "Epoch: 159, Loss: 0.4479, Train: 0.7529, Test: 1.0000\n",
            "Epoch: 160, Loss: 0.4437, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 161, Loss: 0.4424, Train: 0.7471, Test: 1.0000\n",
            "Epoch: 162, Loss: 0.4371, Train: 0.7588, Test: 1.0000\n",
            "Epoch: 163, Loss: 0.4392, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 164, Loss: 0.4494, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 165, Loss: 0.4381, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 166, Loss: 0.4489, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 167, Loss: 0.4471, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 168, Loss: 0.4449, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 169, Loss: 0.4423, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 170, Loss: 0.4456, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 171, Loss: 0.4452, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 172, Loss: 0.4308, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 173, Loss: 0.4255, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 174, Loss: 0.4319, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 175, Loss: 0.4277, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 176, Loss: 0.4296, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 177, Loss: 0.4337, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 178, Loss: 0.4253, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 179, Loss: 0.4179, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 180, Loss: 0.4442, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 181, Loss: 0.4194, Train: 0.7529, Test: 1.0000\n",
            "Epoch: 182, Loss: 0.4252, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 183, Loss: 0.4292, Train: 0.7765, Test: 1.0000\n",
            "Epoch: 184, Loss: 0.4284, Train: 0.7765, Test: 1.0000\n",
            "Epoch: 185, Loss: 0.4220, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 186, Loss: 0.4297, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 187, Loss: 0.4238, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 188, Loss: 0.4188, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 189, Loss: 0.4088, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 190, Loss: 0.4203, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 191, Loss: 0.4157, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 192, Loss: 0.4112, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 193, Loss: 0.4234, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 194, Loss: 0.4171, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 195, Loss: 0.4106, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 196, Loss: 0.4159, Train: 0.7765, Test: 1.0000\n",
            "Epoch: 197, Loss: 0.4169, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 198, Loss: 0.4171, Train: 0.7647, Test: 1.0000\n",
            "Epoch: 199, Loss: 0.4161, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 200, Loss: 0.4100, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 201, Loss: 0.4126, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 202, Loss: 0.4130, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 203, Loss: 0.4243, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 204, Loss: 0.4140, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 205, Loss: 0.4097, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 206, Loss: 0.4154, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 207, Loss: 0.4069, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 208, Loss: 0.4047, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 209, Loss: 0.4139, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 210, Loss: 0.4068, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 211, Loss: 0.3998, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 212, Loss: 0.3988, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 213, Loss: 0.4013, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 214, Loss: 0.3988, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 215, Loss: 0.4015, Train: 0.7706, Test: 1.0000\n",
            "Epoch: 216, Loss: 0.3977, Train: 0.7824, Test: 1.0000\n",
            "Epoch: 217, Loss: 0.4059, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 218, Loss: 0.4019, Train: 0.7941, Test: 1.0000\n",
            "Epoch: 219, Loss: 0.4035, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 220, Loss: 0.3998, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 221, Loss: 0.4071, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 222, Loss: 0.3926, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 223, Loss: 0.4003, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 224, Loss: 0.3892, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 225, Loss: 0.3945, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 226, Loss: 0.4023, Train: 0.7882, Test: 0.9444\n",
            "Epoch: 227, Loss: 0.3959, Train: 0.7882, Test: 1.0000\n",
            "Epoch: 228, Loss: 0.3884, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 229, Loss: 0.3945, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 230, Loss: 0.3860, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 231, Loss: 0.3887, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 232, Loss: 0.3897, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 233, Loss: 0.3912, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 234, Loss: 0.3869, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 235, Loss: 0.3904, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 236, Loss: 0.3857, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 237, Loss: 0.3748, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 238, Loss: 0.3779, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 239, Loss: 0.3819, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 240, Loss: 0.3881, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 241, Loss: 0.3663, Train: 0.8000, Test: 0.9444\n",
            "Epoch: 242, Loss: 0.3906, Train: 0.7941, Test: 0.9444\n",
            "Epoch: 243, Loss: 0.3813, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 244, Loss: 0.3753, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 245, Loss: 0.3725, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 246, Loss: 0.3780, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 247, Loss: 0.3754, Train: 0.8059, Test: 0.9444\n",
            "Epoch: 248, Loss: 0.3872, Train: 0.8118, Test: 0.9444\n",
            "Epoch: 249, Loss: 0.3799, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 250, Loss: 0.3751, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 251, Loss: 0.3714, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 252, Loss: 0.3758, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 253, Loss: 0.3709, Train: 0.8118, Test: 0.8889\n",
            "Epoch: 254, Loss: 0.3749, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 255, Loss: 0.3811, Train: 0.8294, Test: 0.8889\n",
            "Epoch: 256, Loss: 0.3717, Train: 0.8176, Test: 0.9444\n",
            "Epoch: 001, Loss: 0.7804, Train: 0.2765, Test: 0.5556\n",
            "Epoch: 002, Loss: 0.7296, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 003, Loss: 0.6568, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 004, Loss: 0.6440, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 005, Loss: 0.6443, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 006, Loss: 0.6425, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.6588, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 008, Loss: 0.6318, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 009, Loss: 0.6297, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.5873, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 011, Loss: 0.6190, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.6243, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 013, Loss: 0.6227, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 014, Loss: 0.6266, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 015, Loss: 0.6362, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 016, Loss: 0.5983, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.5951, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 018, Loss: 0.5881, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 019, Loss: 0.5998, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 020, Loss: 0.6014, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.5978, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.6029, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.5941, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.5943, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.5921, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.5792, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.5959, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.5912, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.5838, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 030, Loss: 0.5816, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 031, Loss: 0.5818, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 032, Loss: 0.5636, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 033, Loss: 0.5623, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 034, Loss: 0.5837, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 035, Loss: 0.5719, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 036, Loss: 0.5595, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 037, Loss: 0.5605, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.5683, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.5605, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.5504, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.5447, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.5461, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.5476, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.5475, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.5640, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 046, Loss: 0.5531, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 047, Loss: 0.5524, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 048, Loss: 0.5375, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 049, Loss: 0.5450, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 050, Loss: 0.5436, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 051, Loss: 0.5374, Train: 0.6941, Test: 0.6111\n",
            "Epoch: 052, Loss: 0.5342, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 053, Loss: 0.5308, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 054, Loss: 0.5176, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 055, Loss: 0.5340, Train: 0.7000, Test: 0.5556\n",
            "Epoch: 056, Loss: 0.5290, Train: 0.7059, Test: 0.5556\n",
            "Epoch: 057, Loss: 0.5226, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 058, Loss: 0.5119, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 059, Loss: 0.5256, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 060, Loss: 0.5059, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 061, Loss: 0.5285, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 062, Loss: 0.5074, Train: 0.7118, Test: 0.5556\n",
            "Epoch: 063, Loss: 0.5019, Train: 0.7235, Test: 0.5556\n",
            "Epoch: 064, Loss: 0.5049, Train: 0.7412, Test: 0.5556\n",
            "Epoch: 065, Loss: 0.5217, Train: 0.7529, Test: 0.5556\n",
            "Epoch: 066, Loss: 0.5060, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 067, Loss: 0.5068, Train: 0.7412, Test: 0.4444\n",
            "Epoch: 068, Loss: 0.4979, Train: 0.7412, Test: 0.4444\n",
            "Epoch: 069, Loss: 0.5017, Train: 0.7471, Test: 0.4444\n",
            "Epoch: 070, Loss: 0.5039, Train: 0.7412, Test: 0.4444\n",
            "Epoch: 071, Loss: 0.4985, Train: 0.7471, Test: 0.4444\n",
            "Epoch: 072, Loss: 0.5002, Train: 0.7471, Test: 0.4444\n",
            "Epoch: 073, Loss: 0.4769, Train: 0.7471, Test: 0.4444\n",
            "Epoch: 074, Loss: 0.5019, Train: 0.7412, Test: 0.4444\n",
            "Epoch: 075, Loss: 0.4794, Train: 0.7471, Test: 0.4444\n",
            "Epoch: 076, Loss: 0.4935, Train: 0.7706, Test: 0.4444\n",
            "Epoch: 077, Loss: 0.4875, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 078, Loss: 0.4803, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 079, Loss: 0.4682, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 080, Loss: 0.4712, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 081, Loss: 0.4659, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 082, Loss: 0.4938, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 083, Loss: 0.4705, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 084, Loss: 0.4747, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 085, Loss: 0.4652, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 086, Loss: 0.4790, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 087, Loss: 0.4661, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 088, Loss: 0.4770, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 089, Loss: 0.4774, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 090, Loss: 0.4851, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 091, Loss: 0.4751, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 092, Loss: 0.4640, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 093, Loss: 0.4651, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 094, Loss: 0.4649, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 095, Loss: 0.4632, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 096, Loss: 0.4501, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 097, Loss: 0.4532, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 098, Loss: 0.4720, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 099, Loss: 0.4574, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 100, Loss: 0.4605, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 101, Loss: 0.4580, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 102, Loss: 0.4653, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 103, Loss: 0.4503, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 104, Loss: 0.4433, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 105, Loss: 0.4567, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 106, Loss: 0.4452, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 107, Loss: 0.4635, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 108, Loss: 0.4350, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 109, Loss: 0.4417, Train: 0.7529, Test: 0.5000\n",
            "Epoch: 110, Loss: 0.4427, Train: 0.7471, Test: 0.5000\n",
            "Epoch: 111, Loss: 0.4479, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 112, Loss: 0.4464, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 113, Loss: 0.4441, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 114, Loss: 0.4333, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 115, Loss: 0.4422, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 116, Loss: 0.4431, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 117, Loss: 0.4444, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 118, Loss: 0.4368, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 119, Loss: 0.4403, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 120, Loss: 0.4409, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 121, Loss: 0.4473, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 122, Loss: 0.4454, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 123, Loss: 0.4343, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 124, Loss: 0.4451, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 125, Loss: 0.4401, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 126, Loss: 0.4392, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 127, Loss: 0.4438, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 128, Loss: 0.4190, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 129, Loss: 0.4398, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 130, Loss: 0.4433, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 131, Loss: 0.4357, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 132, Loss: 0.4413, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 133, Loss: 0.4319, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 134, Loss: 0.4383, Train: 0.7706, Test: 0.5000\n",
            "Epoch: 135, Loss: 0.4437, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 136, Loss: 0.4242, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 137, Loss: 0.4431, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 138, Loss: 0.4316, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 139, Loss: 0.4358, Train: 0.7647, Test: 0.5000\n",
            "Epoch: 140, Loss: 0.4328, Train: 0.7765, Test: 0.5000\n",
            "Epoch: 141, Loss: 0.4273, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 142, Loss: 0.4300, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 143, Loss: 0.4240, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 144, Loss: 0.4248, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 145, Loss: 0.4187, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 146, Loss: 0.4314, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 147, Loss: 0.4197, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 148, Loss: 0.4217, Train: 0.7824, Test: 0.5000\n",
            "Epoch: 149, Loss: 0.4199, Train: 0.8000, Test: 0.5000\n",
            "Epoch: 150, Loss: 0.4129, Train: 0.7941, Test: 0.5000\n",
            "Epoch: 151, Loss: 0.4138, Train: 0.8000, Test: 0.5000\n",
            "Epoch: 152, Loss: 0.4183, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 153, Loss: 0.4309, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 154, Loss: 0.4123, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 155, Loss: 0.4215, Train: 0.7941, Test: 0.5000\n",
            "Epoch: 156, Loss: 0.4220, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 157, Loss: 0.4186, Train: 0.7941, Test: 0.5000\n",
            "Epoch: 158, Loss: 0.4247, Train: 0.7882, Test: 0.5000\n",
            "Epoch: 159, Loss: 0.4331, Train: 0.8000, Test: 0.5000\n",
            "Epoch: 160, Loss: 0.4161, Train: 0.7941, Test: 0.5000\n",
            "Epoch: 161, Loss: 0.4127, Train: 0.8000, Test: 0.5000\n",
            "Epoch: 162, Loss: 0.4019, Train: 0.8000, Test: 0.5000\n",
            "Epoch: 163, Loss: 0.4013, Train: 0.8059, Test: 0.5000\n",
            "Epoch: 164, Loss: 0.4182, Train: 0.8059, Test: 0.5000\n",
            "Epoch: 165, Loss: 0.4184, Train: 0.8059, Test: 0.5000\n",
            "Epoch: 166, Loss: 0.4064, Train: 0.8059, Test: 0.5000\n",
            "Epoch: 167, Loss: 0.4286, Train: 0.7941, Test: 0.5000\n",
            "Epoch: 168, Loss: 0.4114, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 169, Loss: 0.4060, Train: 0.8059, Test: 0.5000\n",
            "Epoch: 170, Loss: 0.4183, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 171, Loss: 0.4219, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 172, Loss: 0.4034, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 173, Loss: 0.4079, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 174, Loss: 0.4117, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 175, Loss: 0.4150, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 176, Loss: 0.4079, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 177, Loss: 0.3958, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 178, Loss: 0.4195, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 179, Loss: 0.4187, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 180, Loss: 0.4123, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 181, Loss: 0.3995, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 182, Loss: 0.4031, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 183, Loss: 0.4119, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 184, Loss: 0.4054, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 185, Loss: 0.4017, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 186, Loss: 0.4013, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 187, Loss: 0.4003, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 188, Loss: 0.3978, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 189, Loss: 0.3987, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 190, Loss: 0.4048, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 191, Loss: 0.4120, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 192, Loss: 0.4050, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 193, Loss: 0.3975, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 194, Loss: 0.4078, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 195, Loss: 0.3880, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 196, Loss: 0.3958, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 197, Loss: 0.4003, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 198, Loss: 0.3962, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 199, Loss: 0.3902, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 200, Loss: 0.3901, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 201, Loss: 0.4032, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 202, Loss: 0.3982, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 203, Loss: 0.3959, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 204, Loss: 0.3998, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 205, Loss: 0.3904, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 206, Loss: 0.3839, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 207, Loss: 0.4049, Train: 0.8000, Test: 0.6111\n",
            "Epoch: 208, Loss: 0.3876, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 209, Loss: 0.4014, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 210, Loss: 0.3924, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 211, Loss: 0.3859, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 212, Loss: 0.3807, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 213, Loss: 0.3936, Train: 0.8176, Test: 0.5000\n",
            "Epoch: 214, Loss: 0.3849, Train: 0.8118, Test: 0.5000\n",
            "Epoch: 215, Loss: 0.3857, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 216, Loss: 0.3868, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 217, Loss: 0.3981, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 218, Loss: 0.3972, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 219, Loss: 0.3857, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 220, Loss: 0.3900, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 221, Loss: 0.3937, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 222, Loss: 0.3920, Train: 0.8176, Test: 0.5556\n",
            "Epoch: 223, Loss: 0.3851, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 224, Loss: 0.3843, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 225, Loss: 0.3963, Train: 0.8059, Test: 0.6111\n",
            "Epoch: 226, Loss: 0.3887, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 227, Loss: 0.3832, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 228, Loss: 0.3893, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 229, Loss: 0.3837, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 230, Loss: 0.3790, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 231, Loss: 0.3846, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 232, Loss: 0.3749, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 233, Loss: 0.3815, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 234, Loss: 0.3854, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 235, Loss: 0.3822, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 236, Loss: 0.3814, Train: 0.8118, Test: 0.6111\n",
            "Epoch: 237, Loss: 0.3843, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 238, Loss: 0.3836, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 239, Loss: 0.3913, Train: 0.8235, Test: 0.5556\n",
            "Epoch: 240, Loss: 0.3870, Train: 0.8235, Test: 0.5556\n",
            "Epoch: 241, Loss: 0.3817, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 242, Loss: 0.3722, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 243, Loss: 0.3820, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 244, Loss: 0.3763, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 245, Loss: 0.3878, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 246, Loss: 0.3853, Train: 0.8176, Test: 0.6111\n",
            "Epoch: 247, Loss: 0.3804, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 248, Loss: 0.3712, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 249, Loss: 0.3778, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 250, Loss: 0.3783, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 251, Loss: 0.3765, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.3714, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 253, Loss: 0.3655, Train: 0.8235, Test: 0.6111\n",
            "Epoch: 254, Loss: 0.3695, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 255, Loss: 0.3675, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.3626, Train: 0.8235, Test: 0.6667\n",
            "Epoch: 001, Loss: 0.7364, Train: 0.3353, Test: 0.3333\n",
            "Epoch: 002, Loss: 0.7559, Train: 0.3294, Test: 0.2778\n",
            "Epoch: 003, Loss: 0.7282, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 004, Loss: 0.7065, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 005, Loss: 0.6732, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 006, Loss: 0.6928, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 007, Loss: 0.6566, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 008, Loss: 0.6215, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 009, Loss: 0.6608, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 010, Loss: 0.6394, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 011, Loss: 0.6355, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 012, Loss: 0.6014, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 013, Loss: 0.6011, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 014, Loss: 0.6273, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 015, Loss: 0.6207, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 016, Loss: 0.6185, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 017, Loss: 0.6199, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 018, Loss: 0.6098, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 019, Loss: 0.6336, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 020, Loss: 0.6085, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 021, Loss: 0.6051, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 022, Loss: 0.6153, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 023, Loss: 0.6057, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 024, Loss: 0.6376, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 025, Loss: 0.5932, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 026, Loss: 0.5846, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 027, Loss: 0.5963, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 028, Loss: 0.5733, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 029, Loss: 0.5870, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 030, Loss: 0.5898, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 031, Loss: 0.5639, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 032, Loss: 0.6006, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 033, Loss: 0.5584, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 034, Loss: 0.5792, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 035, Loss: 0.5824, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 036, Loss: 0.5892, Train: 0.6647, Test: 0.6667\n",
            "Epoch: 037, Loss: 0.5876, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 038, Loss: 0.5642, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 039, Loss: 0.5742, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 040, Loss: 0.5773, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 041, Loss: 0.5551, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 042, Loss: 0.5722, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 043, Loss: 0.5451, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 044, Loss: 0.5576, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 045, Loss: 0.5484, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 046, Loss: 0.5589, Train: 0.6706, Test: 0.6667\n",
            "Epoch: 047, Loss: 0.5515, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 048, Loss: 0.5584, Train: 0.6765, Test: 0.6667\n",
            "Epoch: 049, Loss: 0.5498, Train: 0.6824, Test: 0.6667\n",
            "Epoch: 050, Loss: 0.5475, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 051, Loss: 0.5394, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 052, Loss: 0.5368, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 053, Loss: 0.5357, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 054, Loss: 0.5489, Train: 0.7235, Test: 0.6667\n",
            "Epoch: 055, Loss: 0.5343, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 056, Loss: 0.5313, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 057, Loss: 0.5359, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 058, Loss: 0.5298, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 059, Loss: 0.5353, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 060, Loss: 0.5268, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 061, Loss: 0.5427, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 062, Loss: 0.5238, Train: 0.7000, Test: 0.7778\n",
            "Epoch: 063, Loss: 0.5222, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 064, Loss: 0.5217, Train: 0.7000, Test: 0.7222\n",
            "Epoch: 065, Loss: 0.5214, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 066, Loss: 0.5263, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 067, Loss: 0.5271, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 068, Loss: 0.5194, Train: 0.6941, Test: 0.6667\n",
            "Epoch: 069, Loss: 0.5045, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 070, Loss: 0.5250, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 071, Loss: 0.5090, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 072, Loss: 0.5063, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 073, Loss: 0.5263, Train: 0.7000, Test: 0.6667\n",
            "Epoch: 074, Loss: 0.5161, Train: 0.7059, Test: 0.6667\n",
            "Epoch: 075, Loss: 0.5116, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 076, Loss: 0.5054, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 077, Loss: 0.4855, Train: 0.7118, Test: 0.6667\n",
            "Epoch: 078, Loss: 0.4991, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 079, Loss: 0.5129, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 080, Loss: 0.5017, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 081, Loss: 0.4916, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 082, Loss: 0.4859, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 083, Loss: 0.5014, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 084, Loss: 0.4964, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 085, Loss: 0.4906, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 086, Loss: 0.4787, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 087, Loss: 0.4857, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 088, Loss: 0.4839, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 089, Loss: 0.4820, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 090, Loss: 0.4831, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 091, Loss: 0.4870, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 092, Loss: 0.4842, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 093, Loss: 0.4729, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 094, Loss: 0.4730, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 095, Loss: 0.4773, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 096, Loss: 0.4743, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 097, Loss: 0.4690, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 098, Loss: 0.4753, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 099, Loss: 0.4703, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 100, Loss: 0.4693, Train: 0.7176, Test: 0.7222\n",
            "Epoch: 101, Loss: 0.4749, Train: 0.7235, Test: 0.7222\n",
            "Epoch: 102, Loss: 0.4680, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 103, Loss: 0.4774, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 104, Loss: 0.4692, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 105, Loss: 0.4887, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 106, Loss: 0.4610, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 107, Loss: 0.4679, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 108, Loss: 0.4801, Train: 0.7294, Test: 0.7222\n",
            "Epoch: 109, Loss: 0.4642, Train: 0.7353, Test: 0.7222\n",
            "Epoch: 110, Loss: 0.4698, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 111, Loss: 0.4595, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 112, Loss: 0.4661, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 113, Loss: 0.4718, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 114, Loss: 0.4546, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 115, Loss: 0.4713, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 116, Loss: 0.4593, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 117, Loss: 0.4619, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 118, Loss: 0.4778, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 119, Loss: 0.4574, Train: 0.7706, Test: 0.7222\n",
            "Epoch: 120, Loss: 0.4603, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 121, Loss: 0.4650, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 122, Loss: 0.4445, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 123, Loss: 0.4501, Train: 0.7529, Test: 0.7222\n",
            "Epoch: 124, Loss: 0.4588, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 125, Loss: 0.4580, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 126, Loss: 0.4514, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 127, Loss: 0.4507, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 128, Loss: 0.4566, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 129, Loss: 0.4522, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 130, Loss: 0.4507, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 131, Loss: 0.4383, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 132, Loss: 0.4432, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 133, Loss: 0.4466, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 134, Loss: 0.4426, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 135, Loss: 0.4632, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 136, Loss: 0.4527, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 137, Loss: 0.4476, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 138, Loss: 0.4367, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 139, Loss: 0.4432, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 140, Loss: 0.4402, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 141, Loss: 0.4430, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 142, Loss: 0.4542, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 143, Loss: 0.4408, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 144, Loss: 0.4237, Train: 0.7588, Test: 0.7222\n",
            "Epoch: 145, Loss: 0.4292, Train: 0.7647, Test: 0.7222\n",
            "Epoch: 146, Loss: 0.4327, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 147, Loss: 0.4366, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 148, Loss: 0.4389, Train: 0.7706, Test: 0.6667\n",
            "Epoch: 149, Loss: 0.4324, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 150, Loss: 0.4462, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 151, Loss: 0.4483, Train: 0.7765, Test: 0.7222\n",
            "Epoch: 152, Loss: 0.4404, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 153, Loss: 0.4382, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 154, Loss: 0.4295, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 155, Loss: 0.4303, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 156, Loss: 0.4395, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 157, Loss: 0.4260, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 158, Loss: 0.4239, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 159, Loss: 0.4243, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 160, Loss: 0.4453, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 161, Loss: 0.4358, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 162, Loss: 0.4238, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 163, Loss: 0.4292, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 164, Loss: 0.4285, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 165, Loss: 0.4315, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 166, Loss: 0.4189, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 167, Loss: 0.4247, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 168, Loss: 0.4323, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 169, Loss: 0.4308, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 170, Loss: 0.4181, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 171, Loss: 0.4305, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 172, Loss: 0.4034, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 173, Loss: 0.4245, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 174, Loss: 0.4232, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 175, Loss: 0.4269, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 176, Loss: 0.4187, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 177, Loss: 0.4271, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 178, Loss: 0.4246, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 179, Loss: 0.4224, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 180, Loss: 0.4018, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 181, Loss: 0.4271, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 182, Loss: 0.4205, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 183, Loss: 0.4159, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 184, Loss: 0.4190, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 185, Loss: 0.4072, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 186, Loss: 0.4143, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 187, Loss: 0.4136, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 188, Loss: 0.4037, Train: 0.7765, Test: 0.6667\n",
            "Epoch: 189, Loss: 0.4153, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 190, Loss: 0.4129, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 191, Loss: 0.4076, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 192, Loss: 0.4117, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 193, Loss: 0.4112, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 194, Loss: 0.3953, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 195, Loss: 0.3994, Train: 0.7824, Test: 0.6667\n",
            "Epoch: 196, Loss: 0.4032, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 197, Loss: 0.4014, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 198, Loss: 0.4051, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 199, Loss: 0.4140, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 200, Loss: 0.4054, Train: 0.7882, Test: 0.6667\n",
            "Epoch: 201, Loss: 0.4109, Train: 0.7941, Test: 0.6667\n",
            "Epoch: 202, Loss: 0.4048, Train: 0.8000, Test: 0.6667\n",
            "Epoch: 203, Loss: 0.4085, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 204, Loss: 0.4045, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 205, Loss: 0.4149, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 206, Loss: 0.4045, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 207, Loss: 0.3998, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 208, Loss: 0.4029, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 209, Loss: 0.3924, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 210, Loss: 0.3987, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 211, Loss: 0.3924, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 212, Loss: 0.3853, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 213, Loss: 0.3934, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 214, Loss: 0.4046, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 215, Loss: 0.4017, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 216, Loss: 0.4064, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 217, Loss: 0.3945, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 218, Loss: 0.3862, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 219, Loss: 0.3923, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 220, Loss: 0.3853, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 221, Loss: 0.3918, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 222, Loss: 0.3880, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 223, Loss: 0.3879, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 224, Loss: 0.3923, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 225, Loss: 0.3907, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 226, Loss: 0.3785, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 227, Loss: 0.3957, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 228, Loss: 0.3860, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 229, Loss: 0.3912, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 230, Loss: 0.3811, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 231, Loss: 0.3768, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 232, Loss: 0.3929, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 233, Loss: 0.3775, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 234, Loss: 0.3717, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 235, Loss: 0.3824, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 236, Loss: 0.3872, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 237, Loss: 0.3730, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 238, Loss: 0.3790, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 239, Loss: 0.3847, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 240, Loss: 0.3790, Train: 0.8118, Test: 0.6667\n",
            "Epoch: 241, Loss: 0.3685, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 242, Loss: 0.3681, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 243, Loss: 0.3782, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 244, Loss: 0.3730, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 245, Loss: 0.3751, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 246, Loss: 0.3720, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 247, Loss: 0.3800, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 248, Loss: 0.3858, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 249, Loss: 0.3826, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 250, Loss: 0.3802, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 251, Loss: 0.3725, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 252, Loss: 0.3775, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 253, Loss: 0.3681, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 254, Loss: 0.3830, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 255, Loss: 0.3766, Train: 0.8059, Test: 0.6667\n",
            "Epoch: 256, Loss: 0.3628, Train: 0.8176, Test: 0.6667\n",
            "Epoch: 001, Loss: 0.8699, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 002, Loss: 0.8299, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 003, Loss: 0.8098, Train: 0.3059, Test: 0.6111\n",
            "Epoch: 004, Loss: 0.7624, Train: 0.3000, Test: 0.6111\n",
            "Epoch: 005, Loss: 0.7588, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 006, Loss: 0.7169, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 007, Loss: 0.6839, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 008, Loss: 0.6450, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 009, Loss: 0.6283, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 010, Loss: 0.6319, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 011, Loss: 0.6366, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 012, Loss: 0.6102, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 013, Loss: 0.5962, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 014, Loss: 0.5874, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 015, Loss: 0.6077, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 016, Loss: 0.5852, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 017, Loss: 0.6057, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 018, Loss: 0.5848, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 019, Loss: 0.5995, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 020, Loss: 0.6194, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 021, Loss: 0.5824, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 022, Loss: 0.5689, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 023, Loss: 0.5677, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 024, Loss: 0.5913, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 025, Loss: 0.5711, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 026, Loss: 0.5983, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 027, Loss: 0.5765, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 028, Loss: 0.5759, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 029, Loss: 0.5680, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 030, Loss: 0.5688, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 031, Loss: 0.5809, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 032, Loss: 0.5775, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 033, Loss: 0.5538, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 034, Loss: 0.5776, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 035, Loss: 0.5826, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 036, Loss: 0.5520, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 037, Loss: 0.5491, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 038, Loss: 0.5805, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 039, Loss: 0.5593, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 040, Loss: 0.5628, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 041, Loss: 0.5574, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 042, Loss: 0.5587, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 043, Loss: 0.5650, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 044, Loss: 0.5675, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 045, Loss: 0.5298, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 046, Loss: 0.5597, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 047, Loss: 0.5239, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 048, Loss: 0.5256, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 049, Loss: 0.5531, Train: 0.6941, Test: 0.3889\n",
            "Epoch: 050, Loss: 0.5356, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 051, Loss: 0.5424, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 052, Loss: 0.5568, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 053, Loss: 0.5320, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 054, Loss: 0.5386, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 055, Loss: 0.5396, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 056, Loss: 0.5191, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 057, Loss: 0.5337, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 058, Loss: 0.5319, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 059, Loss: 0.5386, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 060, Loss: 0.5279, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 061, Loss: 0.5384, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 062, Loss: 0.5181, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 063, Loss: 0.5147, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 064, Loss: 0.5309, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 065, Loss: 0.5208, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 066, Loss: 0.5214, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 067, Loss: 0.5235, Train: 0.7000, Test: 0.3889\n",
            "Epoch: 068, Loss: 0.5156, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 069, Loss: 0.5151, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 070, Loss: 0.5130, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 071, Loss: 0.5041, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 072, Loss: 0.5334, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 073, Loss: 0.5189, Train: 0.7118, Test: 0.5000\n",
            "Epoch: 074, Loss: 0.5229, Train: 0.7118, Test: 0.5000\n",
            "Epoch: 075, Loss: 0.4995, Train: 0.7118, Test: 0.5000\n",
            "Epoch: 076, Loss: 0.5081, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 077, Loss: 0.4828, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 078, Loss: 0.5033, Train: 0.7059, Test: 0.5000\n",
            "Epoch: 079, Loss: 0.4875, Train: 0.7000, Test: 0.5000\n",
            "Epoch: 080, Loss: 0.4939, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 081, Loss: 0.4824, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 082, Loss: 0.5021, Train: 0.7294, Test: 0.5556\n",
            "Epoch: 083, Loss: 0.5097, Train: 0.7353, Test: 0.6111\n",
            "Epoch: 084, Loss: 0.4799, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 085, Loss: 0.5031, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 086, Loss: 0.4811, Train: 0.7412, Test: 0.6667\n",
            "Epoch: 087, Loss: 0.4929, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 088, Loss: 0.5036, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 089, Loss: 0.4888, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 090, Loss: 0.4825, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 091, Loss: 0.5080, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 092, Loss: 0.4888, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 093, Loss: 0.4928, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 094, Loss: 0.4840, Train: 0.7294, Test: 0.6667\n",
            "Epoch: 095, Loss: 0.4974, Train: 0.7471, Test: 0.6667\n",
            "Epoch: 096, Loss: 0.4886, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 097, Loss: 0.4892, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 098, Loss: 0.4949, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 099, Loss: 0.4794, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 100, Loss: 0.4906, Train: 0.7353, Test: 0.6667\n",
            "Epoch: 101, Loss: 0.4819, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 102, Loss: 0.4851, Train: 0.7471, Test: 0.7222\n",
            "Epoch: 103, Loss: 0.4832, Train: 0.7118, Test: 0.7222\n",
            "Epoch: 104, Loss: 0.4850, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 105, Loss: 0.4902, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 106, Loss: 0.4819, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 107, Loss: 0.4708, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 108, Loss: 0.4782, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 109, Loss: 0.4770, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 110, Loss: 0.4632, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 111, Loss: 0.4680, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 112, Loss: 0.4780, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 113, Loss: 0.4797, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 114, Loss: 0.4686, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 115, Loss: 0.4687, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 116, Loss: 0.4660, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 117, Loss: 0.4854, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 118, Loss: 0.4783, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 119, Loss: 0.4783, Train: 0.7235, Test: 0.7778\n",
            "Epoch: 120, Loss: 0.4634, Train: 0.7176, Test: 0.7778\n",
            "Epoch: 121, Loss: 0.4896, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 122, Loss: 0.4571, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 123, Loss: 0.4568, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 124, Loss: 0.4718, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 125, Loss: 0.4736, Train: 0.7294, Test: 0.7778\n",
            "Epoch: 126, Loss: 0.4693, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 127, Loss: 0.4566, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 128, Loss: 0.4664, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 129, Loss: 0.4668, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 130, Loss: 0.4768, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 131, Loss: 0.4538, Train: 0.7353, Test: 0.7778\n",
            "Epoch: 132, Loss: 0.4624, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 133, Loss: 0.4588, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 134, Loss: 0.4683, Train: 0.7471, Test: 0.7778\n",
            "Epoch: 135, Loss: 0.4708, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 136, Loss: 0.4490, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 137, Loss: 0.4574, Train: 0.7412, Test: 0.7778\n",
            "Epoch: 138, Loss: 0.4695, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 139, Loss: 0.4658, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 140, Loss: 0.4651, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 141, Loss: 0.4563, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 142, Loss: 0.4648, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 143, Loss: 0.4651, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 144, Loss: 0.4617, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 145, Loss: 0.4533, Train: 0.7588, Test: 0.7778\n",
            "Epoch: 146, Loss: 0.4570, Train: 0.7529, Test: 0.7778\n",
            "Epoch: 147, Loss: 0.4478, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 148, Loss: 0.4563, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 149, Loss: 0.4548, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 150, Loss: 0.4509, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 151, Loss: 0.4587, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 152, Loss: 0.4564, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 153, Loss: 0.4572, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 154, Loss: 0.4563, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 155, Loss: 0.4551, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 156, Loss: 0.4617, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 157, Loss: 0.4607, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 158, Loss: 0.4554, Train: 0.7471, Test: 0.8333\n",
            "Epoch: 159, Loss: 0.4525, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 160, Loss: 0.4490, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 161, Loss: 0.4428, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 162, Loss: 0.4346, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 163, Loss: 0.4394, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 164, Loss: 0.4400, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 165, Loss: 0.4499, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 166, Loss: 0.4434, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 167, Loss: 0.4410, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 168, Loss: 0.4436, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 169, Loss: 0.4492, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 170, Loss: 0.4514, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 171, Loss: 0.4351, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 172, Loss: 0.4410, Train: 0.7529, Test: 0.8333\n",
            "Epoch: 173, Loss: 0.4382, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 174, Loss: 0.4302, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 175, Loss: 0.4489, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 176, Loss: 0.4381, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 177, Loss: 0.4355, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 178, Loss: 0.4400, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 179, Loss: 0.4303, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 180, Loss: 0.4479, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 181, Loss: 0.4452, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 182, Loss: 0.4354, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 183, Loss: 0.4464, Train: 0.7588, Test: 0.8333\n",
            "Epoch: 184, Loss: 0.4290, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 185, Loss: 0.4346, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 186, Loss: 0.4279, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 187, Loss: 0.4380, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 188, Loss: 0.4431, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 189, Loss: 0.4418, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 190, Loss: 0.4255, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 191, Loss: 0.4273, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 192, Loss: 0.4427, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 193, Loss: 0.4348, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 194, Loss: 0.4285, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 195, Loss: 0.4278, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 196, Loss: 0.4305, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 197, Loss: 0.4328, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 198, Loss: 0.4258, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 199, Loss: 0.4291, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 200, Loss: 0.4334, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 201, Loss: 0.4227, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 202, Loss: 0.4283, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 203, Loss: 0.4207, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 204, Loss: 0.4252, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 205, Loss: 0.4251, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 206, Loss: 0.4364, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 207, Loss: 0.4245, Train: 0.7647, Test: 0.8333\n",
            "Epoch: 208, Loss: 0.4236, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 209, Loss: 0.4146, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 210, Loss: 0.4310, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 211, Loss: 0.4145, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 212, Loss: 0.4153, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 213, Loss: 0.4095, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 214, Loss: 0.4174, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 215, Loss: 0.4182, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 216, Loss: 0.4152, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 217, Loss: 0.4215, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 218, Loss: 0.4154, Train: 0.7706, Test: 0.8333\n",
            "Epoch: 219, Loss: 0.4118, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 220, Loss: 0.4092, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 221, Loss: 0.4196, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 222, Loss: 0.4133, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 223, Loss: 0.4202, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 224, Loss: 0.4076, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 225, Loss: 0.4273, Train: 0.7765, Test: 0.8333\n",
            "Epoch: 226, Loss: 0.4089, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 227, Loss: 0.4194, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 228, Loss: 0.4068, Train: 0.7824, Test: 0.8333\n",
            "Epoch: 229, Loss: 0.4170, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 230, Loss: 0.4117, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 231, Loss: 0.4168, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 232, Loss: 0.4116, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 233, Loss: 0.4068, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 234, Loss: 0.3981, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 235, Loss: 0.4046, Train: 0.8000, Test: 0.8333\n",
            "Epoch: 236, Loss: 0.4066, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 237, Loss: 0.3905, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 238, Loss: 0.4119, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 239, Loss: 0.3952, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 240, Loss: 0.3985, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 241, Loss: 0.4097, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 242, Loss: 0.4079, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 243, Loss: 0.4057, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 244, Loss: 0.3978, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 245, Loss: 0.4005, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 246, Loss: 0.3976, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 247, Loss: 0.3939, Train: 0.8235, Test: 0.8333\n",
            "Epoch: 248, Loss: 0.3983, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 249, Loss: 0.3990, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 250, Loss: 0.4021, Train: 0.7941, Test: 0.8889\n",
            "Epoch: 251, Loss: 0.3905, Train: 0.7941, Test: 0.8333\n",
            "Epoch: 252, Loss: 0.4095, Train: 0.8059, Test: 0.8333\n",
            "Epoch: 253, Loss: 0.3834, Train: 0.8294, Test: 0.8333\n",
            "Epoch: 254, Loss: 0.3989, Train: 0.8294, Test: 0.8333\n",
            "Epoch: 255, Loss: 0.3954, Train: 0.7882, Test: 0.8333\n",
            "Epoch: 256, Loss: 0.3998, Train: 0.7941, Test: 0.8333\n",
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.37,    Train:0.81,    Test:0.78\n",
            "Median time per epoch: 0.0356s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Random Node Features"
      ],
      "metadata": {
        "id": "XQbR9QSyNdhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_rand = []\n",
        "for g in dataset:\n",
        "    g_copy = g\n",
        "    g_copy.x = torch.cat((g_copy.x,torch.rand(g_copy.x.shape[0],2)),dim = -1)\n",
        "    dataset_rand.append(g_copy)"
      ],
      "metadata": {
        "id": "oF0Pq2Itcm-c"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model1: GCN\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features+2, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kiiqmo9lhUms"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     loss_ = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         data = data.to(device)\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         loss = criterion(out, data.y)\n",
        "         loss_ += loss.item()\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset), loss_ / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset_rand)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset_rand[:start] + dataset_rand[end:]\n",
        "    test_dataset = dataset_rand[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model = GCN(params[\"hidden_features_gcn\"]).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr=params[\"learning_rate\"],\n",
        "                                weight_decay=params[\"weight_decay\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, _ = test(train_loader)\n",
        "        test_acc, _ = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        # log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR48lj8gNcx7",
        "outputId": "766d5463-a25f-4275-db5c-dcb126a10867"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.53,    Train:0.72,    Test:0.69\n",
            "Median time per epoch: 0.0394s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model2：GIN\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        total_correct += int((pred == data.y).sum())\n",
        "    return total_correct / len(loader.dataset)\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset_rand)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset_rand[:start] + dataset_rand[end:]\n",
        "    test_dataset = dataset_rand[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model = GIN(\n",
        "        in_channels=params[\"input_features\"]+2,\n",
        "        hidden_channels=params[\"hidden_features_gin\"],\n",
        "        out_channels=params[\"num_classes\"],\n",
        "        num_layers=params[\"num_gin_layers\"],\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss) * data.num_graphs\n",
        "        return total_loss / len(train_loader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "\n",
        "        total_correct = 0\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=-1)\n",
        "            total_correct += int((pred == data.y).sum())\n",
        "        return total_correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc = test(train_loader)\n",
        "        test_acc = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        # log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1doGZEmn2We",
        "outputId": "09278fa5-285d-4a52-b70c-2c79c2a1034a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.01,    Train:1.00,    Test:0.76\n",
            "Median time per epoch: 0.0363s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3: k-GNN\n",
        "class kGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(kGNN, self).__init__()\n",
        "\n",
        "        self.conv1 = GraphConv(dataset.num_node_features+2, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1vP1oWQOriap"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3：k-GNN\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        optimizer.zero_grad()\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        total_correct += int((pred == data.y).sum())\n",
        "    return total_correct / len(loader.dataset)\n",
        "\n",
        "# 10-fold cv\n",
        "each_group_samples = len(dataset_rand)//10\n",
        "\n",
        "k_scores = [0,0,0]\n",
        "k_times = []\n",
        "for group_i in range(10):\n",
        "    start = group_i * each_group_samples\n",
        "    end = (group_i + 1) * each_group_samples\n",
        "\n",
        "    train_dataset = dataset_rand[:start] + dataset_rand[end:]\n",
        "    test_dataset = dataset_rand[start:end]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model =  kGNN(hidden_channels=64).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss) * data.num_graphs\n",
        "        return total_loss / len(train_loader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "\n",
        "        total_correct = 0\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=-1)\n",
        "            total_correct += int((pred == data.y).sum())\n",
        "        return total_correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "    times = []\n",
        "    for epoch in range(1, params[\"num_epochs\"] + 1):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc = test(train_loader)\n",
        "        test_acc = test(test_loader)\n",
        "        if epoch == params[\"num_epochs\"]:\n",
        "            k_scores[0]+=loss\n",
        "            k_scores[1]+=train_acc\n",
        "            k_scores[2]+=test_acc\n",
        "        # log(Epoch=epoch, Loss=loss, Train=train_acc, Test=test_acc)\n",
        "        times.append(time.time() - start)\n",
        "    k_times.append(torch.tensor(times).median())\n",
        "\n",
        "print('')\n",
        "print(f'10-fold cross validation   Epoch:256    Loss:{k_scores[0]/10:.2f},    Train:{k_scores[1]/10:.2f},    Test:{k_scores[2]/10:.2f}')\n",
        "print(f'Median time per epoch: {sum(k_times)/10:.4f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKgZSJ9trc6t",
        "outputId": "6eccb769-265c-4d60-cb4a-aa9b49184677"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10-fold cross validation   Epoch:256    Loss:0.35,    Train:0.84,    Test:0.79\n",
            "Median time per epoch: 0.0308s\n"
          ]
        }
      ]
    }
  ]
}